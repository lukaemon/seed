## tl;dr

## Context
This is Gato's follow up. Automatic full read status. My hunch is multimodal models learn faster than monomodal models. If that's the case, native multimodal model would render GPT kind of text model as specialized, myopic model. The paradigm shift is immanent. 

That doesn't mean GPT3 is useless. No. Once a model saturates a modality, it's good. However, with ever improving inference optimization and new sparse architecture, LLM may not be relevant anymore if superior multimodal model could perform better at close inference cost. 

## Done

## Learned

## Next?

## Log
- `NERF` representation as input is interesting, which is the future real world interaction. Basically an internal 3d world model.
- Task driven feedback loop: "sequential robotic manipulation planning, visual question answering, and captioning." This means no worry about mid attention fusion. `ViT` is a proven image encoder. Just put multimodal tokens together and let the attention to do whatever fusion is necessary wrt tasks. 
- [question -> I don't understand the meaning of text pretraining here. How text only pretrain help multimodal finetuning here? What kind of learned pattern that is transferred?]
- 