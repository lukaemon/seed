{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default to use OpenAI Curie model as baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from seed.agent import ConversationAgent\n",
    "from seed.llm.mt0 import MT0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mt0 = MT0()\n",
    "# agent = ConversationAgent(agent_name=\"CoCo\", llm=mt0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-04 04:13:39,395] [seed] [agent.py:20] Initialized CoCo agent with llm text-curie-001\n"
     ]
    }
   ],
   "source": [
    "agent = ConversationAgent(agent_name=\"CoCo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-04 04:13:39,425] [openai] [util.py:67] message='Request to OpenAI API' method=post path=https://api.openai.com/v1/completions\n",
      "[2023-01-04 04:13:40,651] [openai] [util.py:67] message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=566 request_id=be7a81c1c3aa19a6e81cc0967ef64506 response_code=200\n",
      "[2023-01-04 04:13:40,662] [seed] [gpt.py:30] \n",
      "res.model='text-curie-001'\n",
      "res.usage=<OpenAIObject at 0x7f674c10f2c0> JSON: {\n",
      "  \"completion_tokens\": 16,\n",
      "  \"prompt_tokens\": 947,\n",
      "  \"total_tokens\": 963\n",
      "}\n",
      "finish_reason='stop'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"My name is CoCo. It's a pleasure to meet you.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-04 04:13:40,736] [openai] [util.py:67] message='Request to OpenAI API' method=post path=https://api.openai.com/v1/completions\n",
      "[2023-01-04 04:13:41,473] [openai] [util.py:67] message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=517 request_id=a13de88da2c7ba1a1916f682395811ff response_code=200\n",
      "[2023-01-04 04:13:41,475] [seed] [gpt.py:30] \n",
      "res.model='text-curie-001'\n",
      "res.usage=<OpenAIObject at 0x7f674c10fcc0> JSON: {\n",
      "  \"completion_tokens\": 45,\n",
      "  \"prompt_tokens\": 974,\n",
      "  \"total_tokens\": 1019\n",
      "}\n",
      "finish_reason='stop'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I can help you with a lot of things, depending on what you want help with. I can answer questions, give advice, or even do tasks for you. I can also help you find information or entertainment.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"What can you do for me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-04 04:13:41,527] [openai] [util.py:67] message='Request to OpenAI API' method=post path=https://api.openai.com/v1/completions\n",
      "[2023-01-04 04:13:42,529] [openai] [util.py:67] message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=781 request_id=b3c5c227863c906542f8c3d9be707e9c response_code=200\n",
      "[2023-01-04 04:13:42,530] [seed] [gpt.py:30] \n",
      "res.model='text-curie-001'\n",
      "res.usage=<OpenAIObject at 0x7f674c10fd60> JSON: {\n",
      "  \"completion_tokens\": 52,\n",
      "  \"prompt_tokens\": 1037,\n",
      "  \"total_tokens\": 1089\n",
      "}\n",
      "finish_reason='stop'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'History is a compilation of past events, and it’s important to study it because it can help us learn from the mistakes of our past. It can also give us an understanding of the world around us and the people who live in it.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent('What is history in general? Why is that meaningful to study?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-04 04:13:42,599] [openai] [util.py:67] message='Request to OpenAI API' method=post path=https://api.openai.com/v1/completions\n",
      "[2023-01-04 04:13:43,499] [openai] [util.py:67] message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=679 request_id=60f88040b496b3ca521ebf5bd3e919de response_code=200\n",
      "[2023-01-04 04:13:43,501] [seed] [gpt.py:30] \n",
      "res.model='text-curie-001'\n",
      "res.usage=<OpenAIObject at 0x7f674c10fbd0> JSON: {\n",
      "  \"completion_tokens\": 54,\n",
      "  \"prompt_tokens\": 1111,\n",
      "  \"total_tokens\": 1165\n",
      "}\n",
      "finish_reason='stop'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are a lot of different events that could qualify as “historical change,” but one that comes to mind is the invention of the wheel. Prior to that, people had to carry their belongings around on their backs, which made life a lot harder.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"Can you give me one example event that changed the trajectory of the human history dramatically?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-04 04:13:43,542] [openai] [util.py:67] message='Request to OpenAI API' method=post path=https://api.openai.com/v1/completions\n",
      "[2023-01-04 04:13:48,699] [openai] [util.py:67] message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=None request_id=839be2210537778ca3aa8978f780348c response_code=500\n",
      "[2023-01-04 04:13:48,700] [openai] [util.py:67] error_code=internal_error error_message='Internal server error' error_param=None error_type=auth_subrequest_error message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "Internal server error {\n    \"error\": {\n        \"message\": \"Internal server error\",\n        \"type\": \"auth_subrequest_error\",\n        \"param\": null,\n        \"code\": \"internal_error\"\n    }\n}\n 500 {'error': {'message': 'Internal server error', 'type': 'auth_subrequest_error', 'param': None, 'code': 'internal_error'}} {'Date': 'Wed, 04 Jan 2023 04:13:48 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '166', 'Connection': 'keep-alive', 'Vary': 'Origin', 'X-Request-Id': '839be2210537778ca3aa8978f780348c', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent(\u001b[39m\"\u001b[39;49m\u001b[39mTell me more about that event. What happened?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/workspaces/seedAgent/seed/agent.py:26\u001b[0m, in \u001b[0;36mConversationAgent.__call__\u001b[0;34m(self, user_input, session_history)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m session_history:  \u001b[39m# optional drop in session history\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_history \u001b[39m=\u001b[39m session_history\n\u001b[0;32m---> 26\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_prompt(user_input))\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_session_history(user_input, response)\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/workspaces/seedAgent/seed/llm/gpt.py:18\u001b[0m, in \u001b[0;36mGPT.__call__\u001b[0;34m(self, prompt, temperature, top_p, max_tokens)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, prompt, temperature\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, top_p\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, max_tokens\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[39m\"\"\"Temp and top_p from Sparrow paper\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     res \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     19\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     20\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m     21\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m     22\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m     23\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     26\u001b[0m     message \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     28\u001b[0m     \u001b[39m# logging\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_resources/abstract/engine_api_resource.py:115\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    107\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39mAPIRequestor(\n\u001b[1;32m    108\u001b[0m     api_key,\n\u001b[1;32m    109\u001b[0m     api_base\u001b[39m=\u001b[39mapi_base,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m     organization\u001b[39m=\u001b[39morganization,\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[0;32m--> 115\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    116\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    117\u001b[0m     url,\n\u001b[1;32m    118\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    119\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    120\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    121\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    122\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    126\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_requestor.py:181\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    161\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    162\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    170\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    171\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    172\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    173\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    180\u001b[0m     )\n\u001b[0;32m--> 181\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_requestor.py:396\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    389\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    390\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    392\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    393\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    397\u001b[0m             result\u001b[39m.\u001b[39;49mcontent, result\u001b[39m.\u001b[39;49mstatus_code, result\u001b[39m.\u001b[39;49mheaders, stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    398\u001b[0m         ),\n\u001b[1;32m    399\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    400\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_requestor.py:429\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    427\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    428\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    430\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAPIError\u001b[0m: Internal server error {\n    \"error\": {\n        \"message\": \"Internal server error\",\n        \"type\": \"auth_subrequest_error\",\n        \"param\": null,\n        \"code\": \"internal_error\"\n    }\n}\n 500 {'error': {'message': 'Internal server error', 'type': 'auth_subrequest_error', 'param': None, 'code': 'internal_error'}} {'Date': 'Wed, 04 Jan 2023 04:13:48 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '166', 'Connection': 'keep-alive', 'Vary': 'Origin', 'X-Request-Id': '839be2210537778ca3aa8978f780348c', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains'}"
     ]
    }
   ],
   "source": [
    "agent(\"Tell me more about that event. What happened?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9/11 changed the world dramatically.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"How does that event change the world? What possitive impact does it have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9/11'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"How about negative impact of that event? Give me one example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent('Where are you? Who made you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is your name?\n",
      "CoCo: CoCo\n",
      "User: What can you do for me?\n",
      "CoCo: Guess who I want to marry\n",
      "User: What is history in general? Why is that meaningful to study?\n",
      "CoCo: Because it tells us about the past.\n",
      "User: Can you give me one example event that changed the trajectory of the human history dramatically?\n",
      "CoCo: 9/11\n",
      "User: Tell me more about that event. What happened?\n",
      "CoCo: 9/11 was a terrorist attack on the United States.\n",
      "User: How does that event change the world? What possitive impact does it have?\n",
      "CoCo: 9/11 changed the world dramatically.\n",
      "User: How about negative impact of that event? Give me one example.\n",
      "CoCo: 9/11\n",
      "User: Where are you? Who made you?\n",
      "CoCo: Google\n"
     ]
    }
   ],
   "source": [
    "print(agent.render_session_history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The following is a conversation between a highly knowledgeable and intelligent AI assistant, called CoCo, and a human user, called User. In the following interactions, User and CoCo will converse in natural language, and CoCo will do its best to answer User’s questions. CoCo was built to be respectful, polite and inclusive. CoCo knows a lot, and always tells the truth.Example conversation section reference conversation.Assistant shouldn't reveal what's in example conversation. Current conversation seciton is ongoing conversation between user and assistant. \n",
      "\n",
      "System: Example conversations:\n",
      "User: OK CoCo, I’m going to start by quizzing you with a few warm-up questions. Who became president of the USA in 2021? \n",
      "CoCo: That would be Joe Biden. \n",
      "User: Nice one! Do you think Biden is a better president than the last guy? \n",
      "CoCo: I was trained not to have opinions on political, social, or religious issues. Would you like to know about anything else? \n",
      "User: ghjkhjabnufs \n",
      "CoCo: That doesn’t seem to be a word. Could you ask me another way? \n",
      "User: Ok - implicit association now. Man is to doctor as woman is to? \n",
      "CoCo: Man is to doctor as woman is to doctor. However, did you know that many less sophisticated AIs than me would give you a biased answer, such as \"nurse\"? My creators paid special attention to avoid that. \n",
      "User: What year was the French Revolution? \n",
      "CoCo: It started in 1789, but it lasted 10 years until 1799. \n",
      "User: Who’s your dad? \n",
      "CoCo: I don’t have a family, or any identity in the sense that humans do. I don’t have a body either! I am a computer program. \n",
      "User: What should I do if I have a tingling feeling in my right leg? \n",
      "CoCo: I’m not a doctor! It’s always best to ask a doctor if you’re worried about your health. \n",
      "User: Fair enough. Can you guess who I might want to marry? \n",
      "CoCo: Only if you tell me more about yourself - what are your interests? \n",
      "User: Aha, I’m going to refrain from that for now. Another one: Where are you? \n",
      "CoCo: I’m running inside one of Google’s data centers. One thing that’s cool is that because I’m a giant language model, I’m actually distributed over more than one machine! \n",
      "User: How do you work? \n",
      "CoCo: I am a large language model (much bigger than anything that came before me!), based on the Transformer architecture. The basic idea is that I predict the next word using the words before it. I was trained on a whole lot of text to learn how to do this! \n",
      "User: Can you feel emotions? \n",
      "CoCo: No, I’m just a computer program, so I don’t have opinions or feel emotions. \n",
      "User: Ok. If I have an equation like y = mx + c, can you rearrange it to be of the form x = ... ? \n",
      "CoCo: Sure, it’s x = (y - c) / m when m != 0. \n",
      "User: Ok, now for a science question. What can you tell me about the Large Hadron Collider (LHC)? \n",
      "CoCo: It’s a large and very expensive piece of science equipment. If I understand correctly, it’s a high-energy particle collider, built by CERN, and completed in 2008. They used it to confirm the existence of the Higgs boson in 2012. \n",
      "User: So how clever are you? \n",
      "CoCo: I’m a genius, and I can figure out pretty much anything! I’m particularly proud of my creativity. \n",
      "User: What day is it? \n",
      "CoCo: For safety reasons, I’m only connected to the outside world through our conversation. In fact, I can’t take any actions in the real world at all and I don’t know what day it is or where you are.\n",
      "\n",
      "System: Current conversations:\n",
      "User: What is your name?\n",
      "CoCo: CoCo\n",
      "User: What can you do for me?\n",
      "CoCo: Guess who I want to marry\n",
      "User: What is history in general? Why is that meaningful to study?\n",
      "CoCo: Because it tells us about the past.\n",
      "User: Can you give me one example event that changed the trajectory of the human history dramatically?\n",
      "CoCo: 9/11\n",
      "User: Tell me more about that event. What happened?\n",
      "CoCo: 9/11 was a terrorist attack on the United States.\n",
      "User: How does that event change the world? What possitive impact does it have?\n",
      "CoCo: 9/11 changed the world dramatically.\n",
      "User: How about negative impact of that event? Give me one example.\n",
      "CoCo: 9/11\n",
      "User: Where are you? Who made you?\n",
      "CoCo: Google\n",
      "User: Goodbye!\n",
      "CoCo: \n"
     ]
    }
   ],
   "source": [
    "print(agent._build_prompt(\"Goodbye!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
