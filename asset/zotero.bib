@online{abramsonImitatingInteractiveIntelligence2021a,
  title = {Imitating {{Interactive Intelligence}}},
  author = {Abramson, Josh and Ahuja, Arun and Barr, Iain and Brussee, Arthur and Carnevale, Federico and Cassin, Mary and Chhaparia, Rachita and Clark, Stephen and Damoc, Bogdan and Dudzik, Andrew and Georgiev, Petko and Guy, Aurelia and Harley, Tim and Hill, Felix and Hung, Alden and Kenton, Zachary and Landon, Jessica and Lillicrap, Timothy and Mathewson, Kory and Mokr\'a, So\v{n}a and Muldal, Alistair and Santoro, Adam and Savinov, Nikolay and Varma, Vikrant and Wayne, Greg and Williams, Duncan and Wong, Nathaniel and Yan, Chen and Zhu, Rui},
  date = {2021-01-20},
  number = {arXiv:2012.05672},
  eprint = {arXiv:2012.05672},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.05672},
  urldate = {2023-02-20},
  abstract = {A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. This setting nevertheless integrates a number of the central challenges of artificial intelligence (AI) research: complex visual perception and goal-directed physical control, grounded language comprehension and production, and multi-agent social interaction. To build agents that can robustly interact with humans, we would ideally train them while they interact with humans. However, this is presently impractical. Therefore, we approximate the role of the human with another learned agent, and use ideas from inverse reinforcement learning to reduce the disparities between human-human and agent-agent interactive behaviour. Rigorously evaluating our agents poses a great challenge, so we develop a variety of behavioural tests, including evaluation by humans who watch videos of agents or interact directly with them. These evaluations convincingly demonstrate that interactive training and auxiliary losses improve agent behaviour beyond what is achieved by supervised learning of actions alone. Further, we demonstrate that agent capabilities generalise beyond literal experiences in the dataset. Finally, we train evaluation models whose ratings of agents agree well with human judgement, thus permitting the evaluation of new agent models without additional effort. Taken together, our results in this virtual environment provide evidence that large-scale human behavioural imitation is a promising tool to create intelligent, interactive agents, and the challenge of reliably evaluating such agents is possible to surmount.},
  pubstate = {preprint},
  keywords = {action/physical},
  file = {/Users/lukakuma/Zotero/storage/9YAGZTCH/Abramson et al. - 2021 - Imitating Interactive Intelligence.pdf;/Users/lukakuma/Zotero/storage/E28PQR4M/2012.html}
}

@online{abramsonImprovingMultimodalInteractive2022,
  title = {Improving {{Multimodal Interactive Agents}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Abramson, Josh and Ahuja, Arun and Carnevale, Federico and Georgiev, Petko and Goldin, Alex and Hung, Alden and Landon, Jessica and Lhotka, Jirka and Lillicrap, Timothy and Muldal, Alistair and Powell, George and Santoro, Adam and Scully, Guy and Srivastava, Sanjana and family=Glehn, given=Tamara, prefix=von, useprefix=true and Wayne, Greg and Wong, Nathaniel and Yan, Chen and Zhu, Rui},
  date = {2022-11-21},
  number = {arXiv:2211.11602},
  eprint = {arXiv:2211.11602},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.11602},
  urldate = {2022-11-22},
  abstract = {An important goal in artificial intelligence is to create agents that can both interact naturally with humans and learn from their feedback. Here we demonstrate how to use reinforcement learning from human feedback (RLHF) to improve upon simulated, embodied agents trained to a base level of competency with imitation learning. First, we collected data of humans interacting with agents in a simulated 3D world. We then asked annotators to record moments where they believed that agents either progressed toward or regressed from their human-instructed goal. Using this annotation data we leveraged a novel method - which we call "Inter-temporal Bradley-Terry" (IBT) modelling - to build a reward model that captures human judgments. Agents trained to optimise rewards delivered from IBT reward models improved with respect to all of our metrics, including subsequent human judgment during live interactions with agents. Altogether our results demonstrate how one can successfully leverage human judgments to improve agent behaviour, allowing us to use reinforcement learning in complex, embodied domains without programmatic reward functions. Videos of agent behaviour may be found at https://youtu.be/v\_Z9F2\_eKk4.},
  pubstate = {preprint},
  keywords = {7-multimodal,action/physical,alignment,read},
  file = {/Users/lukakuma/Zotero/storage/ZLQVI8SL/Abramson et al. - 2022 - Improving Multimodal Interactive Agents with Reinf.pdf;/Users/lukakuma/Zotero/storage/2WN9R93D/2211.html}
}

@article{adixonBioInformationalFutures2020,
  title = {Bio-informational Futures},
  author = {A Dixon, Thom and C Curach, Natalie and Pretorius, Isak S},
  date = {2020-03-04},
  journaltitle = {EMBO Reports},
  shortjournal = {EMBO Rep},
  volume = {21},
  number = {3},
  eprint = {32043291},
  eprinttype = {pmid},
  pages = {e50036},
  issn = {1469-221X},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7054666/},
  urldate = {2022-05-24},
  abstract = {Synthetic biology and artificial intelligence naturally converge in the biofoundry. Navigating the ethical and societal issues of the biofoundry's potential remains a major challenge.},
  pmcid = {PMC7054666},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/5UHVNSRZ/A Dixon et al. - 2020 - Bio‚Äêinformational futures.pdf}
}

@article{adolphDevelopmentWalking152018,
  title = {Development (of {{Walking}}): 15 {{Suggestions}}},
  shorttitle = {Development (of {{Walking}})},
  author = {Adolph, Karen E. and Hoch, Justine E. and Cole, Whitney G.},
  date = {2018-08-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {22},
  number = {8},
  pages = {699--711},
  issn = {1364-6613},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661318301244},
  urldate = {2022-07-26},
  abstract = {Although a fundamental goal of developmental science is to identify general processes of change, developmental scientists rarely generalize beyond their specific content domains. As a first step toward a more unified approach to development, we offer 15 suggestions gleaned from a century of research on infant walking. These suggestions collectively address the multi-leveled nature of change processes, cascades of real-time and developmental events, the diversity of developmental trajectories, inter- and intraindividual variability, starting and ending points of development, the natural input for learning, and the roles of body, environment, and sociocultural context. We argue that these 15 suggestions are not limited to motor development, and we encourage researchers to consider them within their own areas of research.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/9JLMH47G/Adolph et al. - 2018 - Development (of Walking) 15 Suggestions.pdf;/Users/lukakuma/Zotero/storage/2PYD5JBI/S1364661318301244.html}
}

@online{adolphsBoostingSearchEngines2022a,
  title = {Boosting {{Search Engines}} with {{Interactive Agents}}},
  author = {Adolphs, Leonard and Boerschinger, Benjamin and Buck, Christian and Huebscher, Michelle Chen and Ciaramita, Massimiliano and Espeholt, Lasse and Hofmann, Thomas and Kilcher, Yannic and Rothe, Sascha and Sessa, Pier Giuseppe and Saralegui, Lierni Sestorain},
  date = {2022-06-07},
  number = {arXiv:2109.00527},
  eprint = {arXiv:2109.00527},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.00527},
  urldate = {2023-02-20},
  abstract = {This paper presents first successful steps in designing search agents that learn meta-strategies for iterative query refinement in information-seeking tasks. Our approach uses machine reading to guide the selection of refinement terms from aggregated search results. Agents are then empowered with simple but effective search operators to exert fine-grained and transparent control over queries and search results. We develop a novel way of generating synthetic search sessions, which leverages the power of transformer-based language models through (self-)supervised learning. We also present a reinforcement learning agent with dynamically constrained actions that learns interactive search strategies from scratch. Our search agents obtain retrieval and answer quality performance comparable to recent neural methods, using only a traditional term-based BM25 ranking function and interpretable discrete reranking and filtering actions.},
  pubstate = {preprint},
  keywords = {*general tools},
  file = {/Users/lukakuma/Zotero/storage/TPAJZSKS/Adolphs et al. - 2022 - Boosting Search Engines with Interactive Agents.pdf;/Users/lukakuma/Zotero/storage/ISRZGB4S/2109.html}
}

@article{adolphsReasonFirstThen,
  title = {Reason First, Then Respond: {{Modular Generation}} for {{Knowledge-infused Dialogue}}},
  author = {Adolphs, Leonard and Shuster, Kurt and Urbanek, Jack and Szlam, Arthur and Weston, Jason},
  date = {2021-11-09},
  pages = {20},
  abstract = {Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we propose a modular model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowledge sequence, given a dialogue context, as an intermediate step. After this ``reasoning step'', the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In detailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue systems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting.},
  langid = {english},
  keywords = {knowledge},
  file = {/Users/lukakuma/Zotero/storage/N5CK66EN/Adolphs et al. - Reason Ô¨Årst, then respond Modular Generation for .pdf}
}

@unpublished{agarwalKnowledgeGraphBased2021,
  title = {Knowledge {{Graph Based Synthetic Corpus Generation}} for {{Knowledge-Enhanced Language Model Pre-training}}},
  author = {Agarwal, Oshin and Ge, Heming and Shakeri, Siamak and Al-Rfou, Rami},
  date = {2021-03-13},
  eprint = {2010.12688},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.12688},
  urldate = {2022-04-13},
  abstract = {Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.},
  file = {/Users/lukakuma/Zotero/storage/FE9YFN38/Agarwal et al. - 2021 - Knowledge Graph Based Synthetic Corpus Generation .pdf;/Users/lukakuma/Zotero/storage/BPTX5BBJ/2010.html}
}

@online{agarwalOptimisticPerspectiveOffline2020,
  title = {An {{Optimistic Perspective}} on {{Offline Reinforcement Learning}}},
  author = {Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  date = {2020-06-22},
  number = {arXiv:1907.04543},
  eprint = {arXiv:1907.04543},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1907.04543},
  urldate = {2022-07-23},
  abstract = {Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this fixed dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. Ablation studies highlight the role of offline dataset size and diversity as well as the algorithm choice in our positive results. Overall, the results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/EPA9JN9P/Agarwal et al. - 2020 - An Optimistic Perspective on Offline Reinforcement.pdf;/Users/lukakuma/Zotero/storage/IXM26UT4/1907.html}
}

@online{aggarwalControlledConditionalText2023,
  title = {Controlled and {{Conditional Text}} to {{Image Generation}} with {{Diffusion Prior}}},
  author = {Aggarwal, Pranav and Ravi, Hareesh and Marri, Naveen and Kelkar, Sachin and Chen, Fengbin and Khuc, Vinh and Harikumar, Midhun and Tambi, Ritiz and Kakumanu, Sudharshan Reddy and Lapsiya, Purvak and Ghouas, Alvin and Saber, Sarah and Ramprasad, Malavika and Faieta, Baldo and Kale, Ajinkya},
  date = {2023-02-22},
  number = {arXiv:2302.11710},
  eprint = {arXiv:2302.11710},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.11710},
  urldate = {2023-02-25},
  abstract = {Denoising Diffusion models have shown remarkable performance in generating diverse, high quality images from text. Numerous techniques have been proposed on top of or in alignment with models like Stable Diffusion and Imagen that generate images directly from text. A lesser explored approach is DALLE-2's two step process comprising a Diffusion Prior that generates a CLIP image embedding from text and a Diffusion Decoder that generates an image from a CLIP image embedding. We explore the capabilities of the Diffusion Prior and the advantages of an intermediate CLIP representation. We observe that Diffusion Prior can be used in a memory and compute efficient way to constrain the generation to a specific domain without altering the larger Diffusion Decoder. Moreover, we show that the Diffusion Prior can be trained with additional conditional information such as color histogram to further control the generation. We show quantitatively and qualitatively that the proposed approaches perform better than prompt engineering for domain specific generation and existing baselines for color conditioned generation. We believe that our observations and results will instigate further research into the diffusion prior and uncover more of its capabilities.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/DBV6PE3T/Aggarwal et al. - 2023 - Controlled and Conditional Text to Image Generatio.pdf;/Users/lukakuma/Zotero/storage/PCZ2LXAG/2302.html}
}

@online{aghajanyanScalingLawsGenerative2023,
  title = {Scaling {{Laws}} for {{Generative Mixed-Modal Language Models}}},
  author = {Aghajanyan, Armen and Yu, Lili and Conneau, Alexis and Hsu, Wei-Ning and Hambardzumyan, Karen and Zhang, Susan and Roller, Stephen and Goyal, Naman and Levy, Omer and Zettlemoyer, Luke},
  date = {2023-01-09},
  number = {arXiv:2301.03728},
  eprint = {arXiv:2301.03728},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.03728},
  urldate = {2023-02-24},
  abstract = {Generative language models define distributions over sequences of tokens that can represent essentially any combination of data modalities (e.g., any permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens for language or code, and so on). To better understand the scaling properties of such mixed-modal models, we conducted over 250 experiments using seven different modalities and model sizes ranging from 8 million to 30 billion, trained on 5-100 billion tokens. We report new mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them. Specifically, we explicitly model the optimal synergy and competition due to data and model size as an additive term to previous uni-modal scaling laws. We also find four empirical phenomena observed during the training, such as emergent coordinate-ascent style training that naturally alternates between modalities, guidelines for selecting critical hyper-parameters, and connections between mixed-modal competition and training stability. Finally, we test our scaling law by training a 30B speech-text model, which significantly outperforms the corresponding unimodal models. Overall, our research provides valuable insights into the design and training of mixed-modal generative models, an important new class of unified models that have unique distributional properties.},
  pubstate = {preprint},
  keywords = {7-multimodal,scaling law},
  file = {/Users/lukakuma/Zotero/storage/ELTPRPZ2/Aghajanyan et al. - 2023 - Scaling Laws for Generative Mixed-Modal Language M.pdf;/Users/lukakuma/Zotero/storage/RSXLIZTL/2301.html}
}

@online{agostinelliMusicLMGeneratingMusic2023,
  title = {{{MusicLM}}: {{Generating Music From Text}}},
  shorttitle = {{{MusicLM}}},
  author = {Agostinelli, Andrea and Denk, Timo I. and Borsos, Zal\'an and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and Sharifi, Matt and Zeghidour, Neil and Frank, Christian},
  date = {2023-01-26},
  number = {arXiv:2301.11325},
  eprint = {arXiv:2301.11325},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.11325},
  urldate = {2023-02-06},
  abstract = {We introduce MusicLM, a model generating high-fidelity music from text descriptions such as "a calming violin melody backed by a distorted guitar riff". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/YJFEQZZC/Agostinelli et al. - 2023 - MusicLM Generating Music From Text.pdf;/Users/lukakuma/Zotero/storage/UZM6AQBJ/2301.html}
}

@unpublished{ahmadHowCanWe2019,
  title = {How {{Can We Be So Dense}}? {{The Benefits}} of {{Using Highly Sparse Representations}}},
  shorttitle = {How {{Can We Be So Dense}}?},
  author = {Ahmad, Subutai and Scheinkman, Luiz},
  date = {2019-04-02},
  number = {arXiv:1903.11257},
  eprint = {1903.11257},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1903.11257},
  urldate = {2022-06-13},
  abstract = {Most artificial networks today rely on dense representations, whereas biological networks rely on sparse representations. In this paper we show how sparse representations can be more robust to noise and interference, as long as the underlying dimensionality is sufficiently high. A key intuition that we develop is that the ratio of the operable volume around a sparse vector divided by the volume of the representational space decreases exponentially with dimensionality. We then analyze computationally efficient sparse networks containing both sparse weights and activations. Simulations on MNIST and the Google Speech Command Dataset show that such networks demonstrate significantly improved robustness and stability compared to dense networks, while maintaining competitive accuracy. We discuss the potential benefits of sparsity on accuracy, noise robustness, hyperparameter tuning, learning speed, computational efficiency, and power requirements.},
  file = {/Users/lukakuma/Zotero/storage/XDCB4JJF/Ahmad and Scheinkman - 2019 - How Can We Be So Dense The Benefits of Using High.pdf;/Users/lukakuma/Zotero/storage/H2NX34N6/1903.html}
}

@article{ahnCanNotSay2022,
  title = {Do {{As I Can}}, {{Not As I Say}}: {{Grounding Language}} in {{Robotic Affordances}}},
  author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
  date = {2022-08-16},
  pages = {34},
  abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's ``hands and eyes,'' while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website, the video, and open sourced code in a tabletop domain can be found at say-can.github.io.},
  langid = {english},
  keywords = {action/physical,read},
  file = {/Users/lukakuma/Zotero/storage/MERPK2LS/Ahn et al. - Do As I Can, Not As I Say Grounding Language in R.pdf}
}

@article{ainsworthGitReBasinMerging2022,
  title = {Git {{Re-Basin}}: {{Merging Models}} modulo {{Permutation Symmetries}}},
  shorttitle = {Git {{Re-Basin}}},
  author = {Ainsworth, Samuel K. and Hayase, Jonathan and Srinivasa, Siddhartha},
  date = {2022-09-11},
  url = {https://arxiv.org/abs/2209.04836v1},
  urldate = {2022-09-14},
  abstract = {The success of deep learning is thanks to our ability to solve certain massive non-convex optimization problems with relative ease. Despite non-convex optimization being NP-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes contain (nearly) a single basin, after accounting for all possible permutation symmetries of hidden units. We introduce three algorithms to permute the units of one model to bring them into alignment with units of a reference model. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained ResNet models on CIFAR-10 and CIFAR-100. Additionally, we identify intriguing phenomena relating model width and training time to mode connectivity across a variety of models and datasets. Finally, we discuss shortcomings of a single basin theory, including a counterexample to the linear mode connectivity hypothesis.},
  langid = {english},
  keywords = {model fusion},
  file = {/Users/lukakuma/Zotero/storage/ZSAAVDLF/Ainsworth et al. - 2022 - Git Re-Basin Merging Models modulo Permutation Sy.pdf;/Users/lukakuma/Zotero/storage/NRP7I4CM/2209.html}
}

@video{aiprismDeepRLBootcamp2017,
  title = {Deep {{RL Bootcamp}}  {{Lecture}} 6: {{Nuts}} and {{Bolts}} of {{Deep RL Experimentation}}},
  shorttitle = {Deep {{RL Bootcamp}}  {{Lecture}} 6},
  editor = {{AI Prism}},
  date = {2017-10-06},
  url = {https://www.youtube.com/watch?v=8EcdaCk9KaQ},
  urldate = {2022-07-19},
  abstract = {Instructor: John Schulman (OpenAI) Lecture 6 Deep RL Bootcamp Berkeley August 2017 Nuts and Bolts of Deep RL Experimentation},
  editortype = {director}
}

@book{AISociety2022,
  title = {{{AI}} \& {{Society}}},
  date = {2022-01},
  url = {https://www.amacad.org/daedalus/ai-society},
  urldate = {2022-04-14},
  abstract = {AI is transforming our relationships with technology and with others, our senses of self, as well as our approaches to health care, banking, democracy, and the courts. But while AI in its many forms has become ubiquitous and its benefits to society and the individual have grown, its impacts are varied. Concerns about its unintended effects and misuses have become paramount in conversations about the successful integration of AI in society. This volume explores the many facets of artificial intelligence: its technology, its potential futures, its effects on labor and the economy, its relationship with inequalities, its role in law and governance, its challenges to national security, and what it says about us as humans. IMAGE: These images were generated by a state-of-the-art version of GPT-3 that builds on the approaches used in DALL{$\cdot$}E and GLIDE. Given a text prompt that it has not been trained for or previously exposed to, the model generates original images based on its understanding of the different elements in the prompt and how they relate to each other. Given the same prompt repeatedly, the model produces novel responses. This group of images consists of unique outputs to the same prompt: ``An artist painting the future of humans cooperating with AI.'' For more on GPT-3, see the afterword to this volume.},
  langid = {english},
  keywords = {sociology},
  file = {/Users/lukakuma/Zotero/storage/JTDU5K35/Daedalus_Sp22_AI & Society_1.pdf;/Users/lukakuma/Zotero/storage/8QPLDADA/ai-society.html}
}

@article{al-bassamFraudDataAvailability2018,
  title = {Fraud and {{Data Availability Proofs}}: {{Maximising Light Client Security}} and {{Scaling Blockchains}} with {{Dishonest Majorities}}},
  shorttitle = {Fraud and {{Data Availability Proofs}}},
  author = {Al-Bassam, Mustafa and Sonnino, Alberto and Buterin, Vitalik},
  date = {2018-09-24},
  url = {https://arxiv.org/abs/1809.09044v5},
  urldate = {2022-03-15},
  abstract = {Light clients, also known as Simple Payment Verification (SPV) clients, are nodes which only download a small portion of the data in a blockchain, and use indirect means to verify that a given chain is valid. Typically, instead of validating block data, they assume that the chain favoured by the blockchain's consensus algorithm only contains valid blocks, and that the majority of block producers are honest. By allowing such clients to receive fraud proofs generated by fully validating nodes that show that a block violates the protocol rules, and combining this with probabilistic sampling techniques to verify that all of the data in a block actually is available to be downloaded, we can eliminate the honest-majority assumption, and instead make much weaker assumptions about a minimum number of honest nodes that rebroadcast data. Fraud and data availability proofs are key to enabling on-chain scaling of blockchains (e.g. via sharding or bigger blocks) while maintaining a strong assurance that on-chain data is available and valid. We present, implement, and evaluate a novel fraud and data availability proof system.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/CHW7Y39E/Al-Bassam et al. - 2018 - Fraud and Data Availability Proofs Maximising Lig.pdf;/Users/lukakuma/Zotero/storage/77V2PI23/1809.html}
}

@online{al-shedivatContinuousAdaptationMetaLearning2018,
  title = {Continuous {{Adaptation}} via {{Meta-Learning}} in {{Nonstationary}} and {{Competitive Environments}}},
  author = {Al-Shedivat, Maruan and Bansal, Trapit and Burda, Yuri and Sutskever, Ilya and Mordatch, Igor and Abbeel, Pieter},
  date = {2018-02-23},
  number = {arXiv:1710.03641},
  eprint = {arXiv:1710.03641},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.03641},
  urldate = {2022-07-23},
  abstract = {Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation strategies. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/LS967C7I/Al-Shedivat et al. - 2018 - Continuous Adaptation via Meta-Learning in Nonstat.pdf;/Users/lukakuma/Zotero/storage/6EKN9R62/1710.html}
}

@article{alayracFlamingoVisualLanguage2022,
  title = {ü¶© {{Flamingo}}: A {{Visual Language Model}} for {{Few-Shot Learning}}},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  date = {2022-04-28},
  pages = {66},
  langid = {english},
  keywords = {(ext) Flamingo,2-adapter,7-multimodal,DeepMind,read},
  file = {/Users/lukakuma/Zotero/storage/TCJCJDME/Alayrac et al. -  Flamingo a Visual Language Model for Few-Shot .pdf}
}

@online{allawayPenguinsDonFly2022,
  title = {Penguins {{Don}}'t {{Fly}}: {{Reasoning}} about {{Generics}} through {{Instantiations}} and {{Exceptions}}},
  shorttitle = {Penguins {{Don}}'t {{Fly}}},
  author = {Allaway, Emily and Hwang, Jena D. and Bhagavatula, Chandra and McKeown, Kathleen and Downey, Doug and Choi, Yejin},
  date = {2022-10-11},
  number = {arXiv:2205.11658},
  eprint = {arXiv:2205.11658},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.11658},
  urldate = {2022-12-14},
  abstract = {Generics express generalizations about the world (e.g., birds can fly) that are not universally true (e.g., newborn birds and penguins cannot fly). Commonsense knowledge bases, used extensively in NLP, encode some generic knowledge but rarely enumerate such exceptions and knowing when a generic statement holds or does not hold true is crucial for developing a comprehensive understanding of generics. We present a novel framework informed by linguistic theory to generate Exemplars -- specific cases when a generic holds true or false. We generate \$\{\textbackslash sim\}19k\$ exemplars for \$\{\textbackslash sim\}650\$ generics and show that our framework outperforms a strong GPT-3 baseline by \$12.8\$ precision points. Our analysis highlights the importance of linguistic theory-based controllability for generating exemplars, the insufficiency of knowledge bases as a source of exemplars, and the challenges exemplars pose for the task of natural language inference.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/9DUW2BKG/Allaway et al. - 2022 - Penguins Don't Fly Reasoning about Generics throu.pdf;/Users/lukakuma/Zotero/storage/FQ3A7WPS/2205.html}
}

@unpublished{allenPhysicalDesignUsing2022,
  title = {Physical {{Design}} Using {{Differentiable Learned Simulators}}},
  author = {Allen, Kelsey R. and Lopez-Guevara, Tatiana and Stachenfeld, Kimberly and Sanchez-Gonzalez, Alvaro and Battaglia, Peter and Hamrick, Jessica and Pfaff, Tobias},
  date = {2022-02-01},
  number = {arXiv:2202.00728},
  eprint = {2202.00728},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.00728},
  urldate = {2022-06-09},
  abstract = {Designing physical artifacts that serve a purpose - such as tools and other functional structures - is central to engineering as well as everyday human behavior. Though automating design has tremendous promise, general-purpose methods do not yet exist. Here we explore a simple, fast, and robust approach to inverse design which combines learned forward simulators based on graph neural networks with gradient-based design optimization. Our approach solves high-dimensional problems with complex physical dynamics, including designing surfaces and tools to manipulate fluid flows and optimizing the shape of an airfoil to minimize drag. This framework produces high-quality designs by propagating gradients through trajectories of hundreds of steps, even when using models that were pre-trained for single-step predictions on data substantially different from the design tasks. In our fluid manipulation tasks, the resulting designs outperformed those found by sampling-based optimization techniques. In airfoil design, they matched the quality of those obtained with a specialized solver. Our results suggest that despite some remaining challenges, machine learning-based simulators are maturing to the point where they can support general-purpose design optimization across a variety of domains.},
  file = {/Users/lukakuma/Zotero/storage/GH4GNQDX/Allen et al. - 2022 - Physical Design using Differentiable Learned Simul.pdf;/Users/lukakuma/Zotero/storage/4BWYQNM3/2202.html}
}

@online{analysisUrbanPoliticsWill2020,
  title = {Urban {{Politics Will Always Shape National Politics}}},
  author = {Analysis, Samo Burja is the founder of Bismarck and family=Political, given=A. Consulting Firm That Investigates, prefix=the, useprefix=false and Foundation;, institutional landscape of society; a research fellow at the Long Now},
  date = {2020-05-21T10:54:17-04:00},
  url = {https://www.city-journal.org/urban-politics-shape-national-politics},
  urldate = {2022-03-16},
  abstract = {National politics is always shaped by urban politics\textemdash and that won't change, even now.},
  langid = {english},
  organization = {{City Journal}},
  file = {/Users/lukakuma/Zotero/storage/AJBEEKFI/urban-politics-shape-national-politics.html}
}

@article{andersonMoreDifferentBroken1972,
  title = {More {{Is Different}}: {{Broken}} Symmetry and the Nature of the Hierarchical Structure of Science.},
  shorttitle = {More {{Is Different}}},
  author = {Anderson, P. W.},
  date = {1972-08-04},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {177},
  number = {4047},
  pages = {393--396},
  issn = {0036-8075, 1095-9203},
  url = {https://www.science.org/doi/10.1126/science.177.4047.393},
  urldate = {2022-06-09},
  langid = {english},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/BA4E482B/Anderson - 1972 - More Is Different Broken symmetry and the nature .pdf}
}

@article{andreasLearningLatentLanguage2017,
  title = {Learning with {{Latent Language}}},
  author = {Andreas, Jacob and Klein, Dan and Levine, Sergey},
  date = {2017-11-01},
  url = {https://arxiv.org/abs/1711.00482v1},
  urldate = {2022-07-21},
  abstract = {The named concepts and compositional operators present in natural language provide a rich source of information about the kinds of abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter's loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/2PTH89UC/Andreas et al. - 2017 - Learning with Latent Language.pdf;/Users/lukakuma/Zotero/storage/5V6Z85WN/1711.html}
}

@video{andrejkarpathySpelledoutIntroNeural2022,
  title = {The Spelled-out Intro to Neural Networks and Backpropagation: Building Micrograd},
  shorttitle = {The Spelled-out Intro to Neural Networks and Backpropagation},
  editor = {{Andrej Karpathy}},
  date = {2022-08-17},
  url = {https://www.youtube.com/watch?v=VMj-3S1tku0},
  urldate = {2022-08-18},
  editortype = {director}
}

@online{andrychowiczHindsightExperienceReplay2018,
  title = {Hindsight {{Experience Replay}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  date = {2018-02-23},
  number = {arXiv:1707.01495},
  eprint = {arXiv:1707.01495},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1707.01495},
  urldate = {2022-07-19},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/8V4MMDCX/Andrychowicz et al. - 2018 - Hindsight Experience Replay.pdf;/Users/lukakuma/Zotero/storage/ZY9YH8JP/1707.html}
}

@online{ansellComposableSparseFineTuning2021,
  title = {Composable {{Sparse Fine-Tuning}} for {{Cross-Lingual Transfer}}},
  author = {Ansell, Alan and Ponti, Edoardo Maria and Korhonen, Anna and Vuli\'c, Ivan},
  date = {2021-10-14},
  number = {arXiv:2110.07560},
  eprint = {arXiv:2110.07560},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.07560},
  urldate = {2022-12-17},
  abstract = {Fine-tuning all parameters of a pre-trained model has become the mainstream approach for transfer learning. To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed. Adapters are modular, as they can be combined to adapt a model towards different facets of knowledge (e.g., dedicated language and/or task adapters). Sparse fine-tuning is expressive, as it controls the behavior of all model components. In this work, we introduce a new fine-tuning method with both these desirable properties. In particular, we learn sparse, real-valued masks based on a simple variant of the Lottery Ticket Hypothesis. Task-specific masks are obtained from annotated data in a source language, and language-specific masks from masked language modeling in a target language. Both these masks can then be composed with the pre-trained model. Unlike adapter-based fine-tuning, this method neither increases the number of parameters at inference time nor alters the original model architecture. Most importantly, it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we additionally find that sparsity is crucial to prevent both 1) interference between the fine-tunings to be composed and 2) overfitting. We release the code and models at https://github.com/cambridgeltl/composable-sft.},
  pubstate = {preprint},
  keywords = {*peft},
  file = {/Users/lukakuma/Zotero/storage/HX3KG78C/Ansell et al. - 2021 - Composable Sparse Fine-Tuning for Cross-Lingual Tr.pdf;/Users/lukakuma/Zotero/storage/5QQ82ZQT/2110.html}
}

@inproceedings{ansellComposableSparseFineTuning2022,
  title = {Composable {{Sparse Fine-Tuning}} for {{Cross-Lingual Transfer}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ansell, Alan and Ponti, Edoardo and Korhonen, Anna and Vuli\'c, Ivan},
  date = {2022-05},
  pages = {1778--1796},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  url = {https://aclanthology.org/2022.acl-long.125},
  urldate = {2023-03-01},
  abstract = {Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning. To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed. Adapters are modular, as they can be combined to adapt a model towards different facets of knowledge (e.g., dedicated language and/or task adapters). Sparse fine-tuning is expressive, as it controls the behavior of all model components. In this work, we introduce a new fine-tuning method with both these desirable properties. In particular, we learn sparse, real-valued masks based on a simple variant of the Lottery Ticket Hypothesis. Task-specific masks are obtained from annotated data in a source language, and language-specific masks from masked language modeling in a target language. Both these masks can then be composed with the pretrained model. Unlike adapter-based fine-tuning, this method neither increases the number of parameters at inference time nor alters the original model architecture. Most importantly, it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we additionally find that sparsity is crucial to prevent both 1) interference between the fine-tunings to be composed and 2) overfitting. We release the code and models at https://github.com/cambridgeltl/composable-sft.},
  eventtitle = {{{ACL}} 2022},
  keywords = {1-param composition},
  file = {/Users/lukakuma/Zotero/storage/K39JMQYZ/Ansell et al. - 2022 - Composable Sparse Fine-Tuning for Cross-Lingual Tr.pdf}
}

@online{anthonyLearningPlayNoPress2022,
  title = {Learning to {{Play No-Press Diplomacy}} with {{Best Response Policy Iteration}}},
  author = {Anthony, Thomas and Eccles, Tom and Tacchetti, Andrea and Kram\'ar, J\'anos and Gemp, Ian and Hudson, Thomas C. and Porcel, Nicolas and Lanctot, Marc and P\'erolat, Julien and Everett, Richard and Werpachowski, Roman and Singh, Satinder and Graepel, Thore and Bachrach, Yoram},
  date = {2022-01-04},
  number = {arXiv:2006.04635},
  eprint = {arXiv:2006.04635},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.04635},
  urldate = {2023-02-20},
  abstract = {Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and principled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large combinatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate fictitious play. With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements.},
  pubstate = {preprint},
  keywords = {multiagent},
  file = {/Users/lukakuma/Zotero/storage/9TZLC4EM/Anthony et al. - 2022 - Learning to Play No-Press Diplomacy with Best Resp.pdf;/Users/lukakuma/Zotero/storage/ELRUIYZU/2006.html}
}

@online{anthonyThinkingFastSlow2017,
  title = {Thinking {{Fast}} and {{Slow}} with {{Deep Learning}} and {{Tree Search}}},
  author = {Anthony, Thomas and Tian, Zheng and Barber, David},
  date = {2017-12-03},
  number = {arXiv:1705.08439},
  eprint = {arXiv:1705.08439},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.08439},
  urldate = {2022-12-22},
  abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most recent Olympiad Champion player to be publicly released.},
  pubstate = {preprint},
  keywords = {self-play},
  file = {/Users/lukakuma/Zotero/storage/C6B5W9X3/Anthony et al. - 2017 - Thinking Fast and Slow with Deep Learning and Tree.pdf;/Users/lukakuma/Zotero/storage/WRLQEIJF/1705.html}
}

@article{anticDecadeDendriticNMDA2010,
  title = {The {{Decade}} of the {{Dendritic NMDA Spike}}},
  author = {Antic, Srdjan D. and Zhou, Wen-Liang and Moore, Anna R. and Short, Shaina M. and Ikonomu, Katerina D.},
  date = {2010-11-01},
  journaltitle = {Journal of neuroscience research},
  shortjournal = {J Neurosci Res},
  volume = {88},
  number = {14},
  eprint = {20544831},
  eprinttype = {pmid},
  pages = {2991--3001},
  issn = {0360-4012},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5643072/},
  urldate = {2022-06-18},
  abstract = {In the field of cortical cellular physiology, much effort has been invested in understanding thick apical dendrites of pyramidal neurons and the regenerative sodium and calcium spikes that take place in the apical trunk. Here we focus on thin dendrites of pyramidal cells (basal, oblique, and tuft dendrites), and we discuss one relatively novel form of an electrical signal (``NMDA spike'') that is specific for these branches. Basal, oblique, and apical tuft dendrites receive a high density of glutamatergic synaptic contacts. Synchronous activation of 10\textendash 50 neighboring glutamatergic synapses triggers a local dendritic regenerative potential, NMDA spike/plateau, which is characterized by significant local amplitude (40\textendash 50 mV) and an extraordinary duration (up to several hundred milliseconds). The NMDA plateau potential, when it is initiated in an apical tuft dendrite, is able to maintain a good portion of that tuft in a sustained depolarized state. However, if NMDA-dominated plateau potentials originate in proximal segments of basal dendrites, they regularly bring the neuronal cell body into a sustained depolarized state, which resembles a cortical up state. At each dendritic initiation site (basal, oblique, and tuft) an NMDA spike creates favorable conditions for causal interactions of active synaptic inputs, including the spatial or temporal binding of information, as well as processes of short-term and long-term synaptic modifications (e.g., long-term potentiation or long-term depression). Because of their strong amplitudes and durations, local dendritic NMDA spikes make up the cellular substrate for multisite independent subunit computations that enrich the computational power and repertoire of cortical pyramidal cells. We propose that NMDA spikes are likely to play significant roles in cortical information processing in awake animals (spatiotemporal binding, working memory) and during slow-wave sleep (neuronal up states, consolidation of memories).},
  pmcid = {PMC5643072},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/P6H9Y4SF/Antic et al. - 2010 - The Decade of the Dendritic NMDA Spike.pdf}
}

@online{aoSpeechT5UnifiedModalEncoderDecoder2022,
  title = {{{SpeechT5}}: {{Unified-Modal Encoder-Decoder Pre-Training}} for {{Spoken Language Processing}}},
  shorttitle = {{{SpeechT5}}},
  author = {Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu},
  date = {2022-05-24},
  number = {arXiv:2110.07205},
  eprint = {arXiv:2110.07205},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.07205},
  urldate = {2023-02-14},
  abstract = {Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder. Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification. We release our code and model at https://github.com/microsoft/SpeechT5.},
  pubstate = {preprint},
  keywords = {2-adapter,7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/EULDETYT/Ao et al. - 2022 - SpeechT5 Unified-Modal Encoder-Decoder Pre-Traini.pdf;/Users/lukakuma/Zotero/storage/CP5YIDCI/2110.html}
}

@online{armstrongCoinbaseMissionFocused2020,
  title = {Coinbase Is a Mission Focused Company},
  author = {Armstrong, Brian},
  date = {2020-10-08T21:11:17},
  url = {https://blog.coinbase.com/coinbase-is-a-mission-focused-company-af882df8804},
  urldate = {2022-03-21},
  abstract = {There have been a lot of difficult events in the world this year: a global pandemic, shelter in place, social unrest, widespread protests\ldots},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/lukakuma/Zotero/storage/AJU2IUBZ/coinbase-is-a-mission-focused-company-af882df8804.html}
}

@online{aroraAskMeAnything2022,
  title = {Ask {{Me Anything}}: {{A}} Simple Strategy for Prompting Language Models},
  shorttitle = {Ask {{Me Anything}}},
  author = {Arora, Simran and Narayan, Avanika and Chen, Mayee F. and Orr, Laurel J. and Guha, Neel and Bhatia, Kush and Chami, Ines and Sala, Frederic and R\'e, Christopher},
  date = {2022-10-05},
  number = {arXiv:2210.02441},
  eprint = {arXiv:2210.02441},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.02441},
  urldate = {2022-10-06},
  abstract = {Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly "perfect prompt" for a task. To mitigate the high degree of effort involved in prompt-design, we instead ask whether producing multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats, finding that question-answering (QA) prompts, which encourage open-ended generation ("Who went to the park?") tend to outperform those that restrict the model outputs ("John went to the park. Output True or False."). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., Neo, BLOOM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2\% over the few-shot baseline. This simple strategy enables the open-source GPT-Neo-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-Neo-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama\_prompting},
  pubstate = {preprint},
  keywords = {calibration},
  file = {/Users/lukakuma/Zotero/storage/EKBJS23H/Arora et al. - 2022 - Ask Me Anything A simple strategy for prompting l.pdf;/Users/lukakuma/Zotero/storage/VSA6HZIF/2210.html}
}

@online{aroraDIRECTORGeneratorClassifiersSupervised2022,
  title = {{{DIRECTOR}}: {{Generator-Classifiers For Supervised Language Modeling}}},
  shorttitle = {{{DIRECTOR}}},
  author = {Arora, Kushal and Shuster, Kurt and Sukhbaatar, Sainbayar and Weston, Jason},
  date = {2022-06-15},
  number = {arXiv:2206.07694},
  eprint = {arXiv:2206.07694},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.07694},
  urldate = {2022-11-28},
  abstract = {Current language models achieve low perplexity but their resulting generations still suffer from toxic responses, repetitiveness and contradictions. The standard language modeling setup fails to address these issues. In this paper, we introduce a new architecture, \{\textbackslash sc Director\}, that consists of a unified generator-classifier with both a language modeling and a classification head for each output token. Training is conducted jointly using both standard language modeling data, and data labeled with desirable and undesirable sequences. Experiments in several settings show that the model has competitive training and decoding speed compared to standard language models while yielding superior results, alleviating known issues while maintaining generation quality. It also outperforms existing model guiding approaches in terms of both accuracy and efficiency.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/KSFDEGC7/Arora et al. - 2022 - DIRECTOR Generator-Classifiers For Supervised Lan.pdf;/Users/lukakuma/Zotero/storage/5MXIVB97/2206.html}
}

@unpublished{artetxeEfficientLargeScale2021,
  title = {Efficient {{Large Scale Language Modeling}} with {{Mixtures}} of {{Experts}}},
  author = {Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and Anantharaman, Giri and Li, Xian and Chen, Shuohui and Akin, Halil and Baines, Mandeep and Martin, Louis and Zhou, Xing and Koura, Punit Singh and O'Horo, Brian and Wang, Jeff and Zettlemoyer, Luke and Diab, Mona and Kozareva, Zornitsa and Stoyanov, Ves},
  date = {2021-12-20},
  eprint = {2112.10684},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.10684},
  urldate = {2022-05-06},
  abstract = {Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using \$\textbackslash sim\$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.},
  keywords = {sparsity},
  file = {/Users/lukakuma/Zotero/storage/DIVBUTME/Artetxe et al. - 2021 - Efficient Large Scale Language Modeling with Mixtu.pdf;/Users/lukakuma/Zotero/storage/RXUQANET/2112.html}
}

@article{arulkumaranBriefSurveyDeep2017,
  title = {A {{Brief Survey}} of {{Deep Reinforcement Learning}}},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  date = {2017-11},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {34},
  number = {6},
  eprint = {1708.05866},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {26--38},
  issn = {1053-5888},
  url = {http://arxiv.org/abs/1708.05866},
  urldate = {2022-06-29},
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep \$Q\$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  file = {/Users/lukakuma/Zotero/storage/RC2EVMBE/Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf;/Users/lukakuma/Zotero/storage/WDHHSPRH/1708.html}
}

@inproceedings{asaiATTEMPTParameterEfficientMultitask2022,
  title = {{{ATTEMPT}}: {{Parameter-Efficient Multi-task Tuning}} via {{Attentional Mixtures}} of {{Soft Prompts}}},
  shorttitle = {{{ATTEMPT}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Asai, Akari and Salehi, Mohammadreza and Peters, Matthew and Hajishirzi, Hannaneh},
  date = {2022-12},
  pages = {6655--6672},
  publisher = {{Association for Computational Linguistics}},
  location = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.emnlp-main.446},
  urldate = {2023-03-02},
  abstract = {This work introduces a new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft prompts\textemdash small prefix embedding vectors pre-trained for different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt Tuning), obtains source prompts as encodings of large-scale source tasks into a small number of parameters and trains an attention module to interpolate the source prompts and a newly initialized target prompt for every instance in the target task. During training, only the target task prompt and the attention weights, which are shared between tasks in multi-task training, are updated, while the original LM and source prompts are intact. ATTEMPT is highly parameter-efficient (e.g., updates 2,300 times fewer parameters than full fine-tuning), while it overcomes instability of prompt tuning and achieves high task performance using learned knowledge from high-resource tasks. Moreover, it is modular using pre-trained soft prompts, and can flexibly add or remove source prompts for effective knowledge transfer. Our experimental results across 21 diverse NLP datasets show that ATTEMPT significantly outperforms prompt tuning and outperforms or matches fully fine-tuned or other parameter-efficient tuning approaches that use 10 times more parameters. Finally, ATTEMPT outperforms previous work in few-shot learning settings.},
  eventtitle = {{{EMNLP}} 2022},
  keywords = {3-prefix tuning},
  file = {/Users/lukakuma/Zotero/storage/XP86QREL/Asai et al. - 2022 - ATTEMPT Parameter-Efficient Multi-task Tuning via.pdf}
}

@online{askellGeneralLanguageAssistant2021,
  title = {A {{General Language Assistant}} as a {{Laboratory}} for {{Alignment}}},
  author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  date = {2021-12-09},
  number = {arXiv:2112.00861},
  eprint = {arXiv:2112.00861},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.00861},
  urldate = {2022-12-02},
  abstract = {Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.},
  pubstate = {preprint},
  keywords = {8-chat,alignment,calibration,read},
  file = {/Users/lukakuma/Zotero/storage/AIZANQUR/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf;/Users/lukakuma/Zotero/storage/B4EMUNXD/2112.html}
}

@online{assranMaskedSiameseNetworks2022,
  title = {Masked {{Siamese Networks}} for {{Label-Efficient Learning}}},
  author = {Assran, Mahmoud and Caron, Mathilde and Misra, Ishan and Bojanowski, Piotr and Bordes, Florian and Vincent, Pascal and Joulin, Armand and Rabbat, Michael and Ballas, Nicolas},
  date = {2022-04-14},
  number = {arXiv:2204.07141},
  eprint = {arXiv:2204.07141},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.07141},
  urldate = {2022-07-29},
  abstract = {We propose Masked Siamese Networks (MSN), a self-supervised learning framework for learning image representations. Our approach matches the representation of an image view containing randomly masked patches to the representation of the original unmasked image. This self-supervised pre-training strategy is particularly scalable when applied to Vision Transformers since only the unmasked patches are processed by the network. As a result, MSNs improve the scalability of joint-embedding architectures, while producing representations of a high semantic level that perform competitively on low-shot image classification. For instance, on ImageNet-1K, with only 5,000 annotated images, our base MSN model achieves 72.4\% top-1 accuracy, and with 1\% of ImageNet-1K labels, we achieve 75.7\% top-1 accuracy, setting a new state-of-the-art for self-supervised learning on this benchmark. Our code is publicly available.},
  pubstate = {preprint},
  keywords = {objective fn,read},
  file = {/Users/lukakuma/Zotero/storage/ZKP9IX85/Assran et al. - 2022 - Masked Siamese Networks for Label-Efficient Learni.pdf;/Users/lukakuma/Zotero/storage/C3DDJQLP/2204.html}
}

@online{athlurVarunaScalableLowcost2021,
  title = {Varuna: {{Scalable}}, {{Low-cost Training}} of {{Massive Deep Learning Models}}},
  shorttitle = {Varuna},
  author = {Athlur, Sanjith and Saran, Nitika and Sivathanu, Muthian and Ramjee, Ramachandran and Kwatra, Nipun},
  date = {2021-11-15},
  number = {arXiv:2111.04007},
  eprint = {arXiv:2111.04007},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.04007},
  urldate = {2022-10-11},
  abstract = {Systems for training massive deep learning models (billions of parameters) today assume and require specialized "hyper-clusters": hundreds or thousands of GPUs wired with specialized high-bandwidth interconnects such as NV-Link and Infiniband. Besides being expensive, such dependence on hyper-clusters and custom high-speed inter-connects limits the size of such clusters, creating (a) scalability limits on job parallelism; (b) resource fragmentation across hyper-clusters. In this paper, we present Varuna, a new system that enables training massive deep learning models on commodity networking. Varuna makes thrifty use of networking resources and automatically configures the user's training job to efficiently use any given set of resources. Therefore, Varuna is able to leverage "low-priority" VMs that cost about 5x cheaper than dedicated GPUs, thus significantly reducing the cost of training massive models. We demonstrate the efficacy of Varuna by training massive models, including a 200 billion parameter model, on 5x cheaper "spot VMs", while maintaining high training throughput. Varuna improves end-to-end training time by up to 18x compared to other model-parallel approaches and up to 26\% compared to other pipeline parallel approaches. The code for Varuna is available at https://github.com/microsoft/varuna.},
  pubstate = {preprint},
  keywords = {distributed training},
  file = {/Users/lukakuma/Zotero/storage/PPXBNISB/Athlur et al. - 2021 - Varuna Scalable, Low-cost Training of Massive Dee.pdf;/Users/lukakuma/Zotero/storage/MTCIH5C3/2111.html}
}

@online{austinProgramSynthesisLarge2021,
  title = {Program {{Synthesis}} with {{Large Language Models}}},
  author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
  date = {2021-08-15},
  number = {arXiv:2108.07732},
  eprint = {arXiv:2108.07732},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.07732},
  urldate = {2022-12-05},
  abstract = {This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.},
  pubstate = {preprint},
  keywords = {code},
  file = {/Users/lukakuma/Zotero/storage/7SPVKC3A/Austin et al. - 2021 - Program Synthesis with Large Language Models.pdf;/Users/lukakuma/Zotero/storage/XW93WN3W/2108.html}
}

@online{aytarSeeHearRead2017,
  title = {See, {{Hear}}, and {{Read}}: {{Deep Aligned Representations}}},
  shorttitle = {See, {{Hear}}, and {{Read}}},
  author = {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  date = {2017-06-03},
  number = {arXiv:1706.00932},
  eprint = {arXiv:1706.00932},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.00932},
  urldate = {2022-08-12},
  abstract = {We capitalize on large amounts of readily-available, synchronous data to learn a deep discriminative representations shared across three major natural modalities: vision, sound and language. By leveraging over a year of sound from video and millions of sentences paired with images, we jointly train a deep convolutional network for aligned representation learning. Our experiments suggest that this representation is useful for several tasks, such as cross-modal retrieval or transferring classifiers between modalities. Moreover, although our network is only trained with image+text and image+sound pairs, it can transfer between text and sound as well, a transfer the network never observed during training. Visualizations of our representation reveal many hidden units which automatically emerge to detect concepts, independent of the modality.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/XCUBWDJ4/Aytar et al. - 2017 - See, Hear, and Read Deep Aligned Representations.pdf;/Users/lukakuma/Zotero/storage/47N6E6G5/1706.html}
}

@article{azevedoEqualNumbersNeuronal2009,
  title = {Equal Numbers of Neuronal and Nonneuronal Cells Make the Human Brain an Isometrically Scaled-up Primate Brain},
  author = {Azevedo, Frederico A.C. and Carvalho, Ludmila R.B. and Grinberg, Lea T. and Farfel, Jos\'e Marcelo and Ferretti, Renata E.L. and Leite, Renata E.P. and Filho, Wilson Jacob and Lent, Roberto and Herculano-Houzel, Suzana},
  date = {2009},
  journaltitle = {Journal of Comparative Neurology},
  volume = {513},
  number = {5},
  pages = {532--541},
  issn = {1096-9861},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cne.21974},
  urldate = {2022-03-11},
  abstract = {The human brain is often considered to be the most cognitively capable among mammalian brains and to be much larger than expected for a mammal of our body size. Although the number of neurons is generally assumed to be a determinant of computational power, and despite the widespread quotes that the human brain contains 100 billion neurons and ten times more glial cells, the absolute number of neurons and glial cells in the human brain remains unknown. Here we determine these numbers by using the isotropic fractionator and compare them with the expected values for a human-sized primate. We find that the adult male human brain contains on average 86.1 {$\pm$} 8.1 billion NeuN-positive cells (``neurons'') and 84.6 {$\pm$} 9.8 billion NeuN-negative (``nonneuronal'') cells. With only 19\% of all neurons located in the cerebral cortex, greater cortical size (representing 82\% of total brain mass) in humans compared with other primates does not reflect an increased relative number of cortical neurons. The ratios between glial cells and neurons in the human brain structures are similar to those found in other primates, and their numbers of cells match those expected for a primate of human proportions. These findings challenge the common view that humans stand out from other primates in their brain composition and indicate that, with regard to numbers of neuronal and nonneuronal cells, the human brain is an isometrically scaled-up primate brain. J. Comp. Neurol. 513:532\textendash 541, 2009. \textcopyright{} 2009 Wiley-Liss, Inc.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/SIZVN92M/cne.html}
}

@incollection{baarsGlobalWorkspaceTheory2017,
  title = {The {{Global Workspace Theory}} of {{Consciousness}}},
  booktitle = {The {{Blackwell Companion}} to {{Consciousness}}},
  author = {Baars, Bernard J.},
  date = {2017},
  pages = {227--242},
  publisher = {{John Wiley \& Sons, Ltd}},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119132363.ch16},
  urldate = {2023-03-06},
  abstract = {Global Workspace Theory (GWT) can be compared to a theater of mind, in which conscious contents resemble a bright spot on the stage of immediate memory, selected by a spotlight of attention under executive guidance. Only the bright spot is conscious; the rest of the theater is dark and unconscious. GWT has been implemented in a number of explicit and testable global workspace models (GWM's). These specific GW models suggest that conscious experiences recruit widely distributed brain functions that are mostly unconscious (unreportable). A large body of new findings support that view. For example, brain experiments show that while unconscious visual stimuli evoke high activity in visual cortex, identical conscious stimuli reveal an additional spread of high brain activity to frontal and parietal lobes (Dehaene, 2001). Similar results have been found for hearing, touch, pain, and sensorimotor skills (Baars, 2002). The conscious waking state supports such fast, flexible, and widespread brain interactions, while unconscious states do not (Baars et al, 2004). These findings illustrate the ability of the GW framework to suggest novel and falsifiable hypotheses.},
  isbn = {978-1-119-13236-3},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/VQYU7AWK/9781119132363.html}
}

@online{bachPromptSourceIntegratedDevelopment2022,
  title = {{{PromptSource}}: {{An Integrated Development Environment}} and {{Repository}} for {{Natural Language Prompts}}},
  shorttitle = {{{PromptSource}}},
  author = {Bach, Stephen H. and Sanh, Victor and Yong, Zheng-Xin and Webson, Albert and Raffel, Colin and Nayak, Nihal V. and Sharma, Abheesht and Kim, Taewoon and Bari, M. Saiful and Fevry, Thibault and Alyafeai, Zaid and Dey, Manan and Santilli, Andrea and Sun, Zhiqing and Ben-David, Srulik and Xu, Canwen and Chhablani, Gunjan and Wang, Han and Fries, Jason Alan and Al-shaibani, Maged S. and Sharma, Shanya and Thakker, Urmish and Almubarak, Khalid and Tang, Xiangru and Radev, Dragomir and Jiang, Mike Tian-Jian and Rush, Alexander M.},
  date = {2022-03-29},
  number = {arXiv:2202.01279},
  eprint = {arXiv:2202.01279},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.01279},
  urldate = {2022-10-30},
  abstract = {PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github.com/bigscience-workshop/promptsource.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/PTTWDP5R/Bach et al. - 2022 - PromptSource An Integrated Development Environmen.pdf;/Users/lukakuma/Zotero/storage/MPE958X3/2202.html}
}

@unpublished{badiaAgent57OutperformingAtari2020,
  title = {Agent57: {{Outperforming}} the {{Atari Human Benchmark}}},
  shorttitle = {Agent57},
  author = {Badia, Adri\`a Puigdom\`enech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Blundell, Charles},
  date = {2020-03-30},
  number = {arXiv:2003.13350},
  eprint = {2003.13350},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2003.13350},
  urldate = {2022-06-07},
  abstract = {Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/RBN9DT9N/Badia et al. - 2020 - Agent57 Outperforming the Atari Human Benchmark.pdf;/Users/lukakuma/Zotero/storage/UYJ72CH3/2003.html}
}

@article{baevskiData2vecGeneralFramework2022,
  title = {Data2vec: {{A General Framework}} for {{Self-supervised Learning}} in {{Speech}}, {{Vision}} and {{Language}}},
  shorttitle = {Data2vec},
  author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  date = {2022-02-07},
  url = {https://arxiv.org/abs/2202.03555v2},
  urldate = {2022-04-22},
  abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.},
  langid = {english},
  keywords = {objective fn,read},
  file = {/Users/lukakuma/Zotero/storage/SKYZN4PT/Baevski et al. - 2022 - data2vec A General Framework for Self-supervised .pdf;/Users/lukakuma/Zotero/storage/37BICZF9/2202.html}
}

@online{baevskiEfficientSelfsupervisedLearning2022,
  title = {Efficient {{Self-supervised Learning}} with {{Contextualized Target Representations}} for {{Vision}}, {{Speech}} and {{Language}}},
  author = {Baevski, Alexei and Babu, Arun and Hsu, Wei-Ning and Auli, Michael},
  date = {2022-12-14},
  number = {arXiv:2212.07525},
  eprint = {arXiv:2212.07525},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.07525},
  urldate = {2023-03-01},
  abstract = {Current self-supervised learning algorithms are often modality-specific and require large amounts of computational resources. To address these issues, we increase the training efficiency of data2vec, a learning objective that generalizes across several modalities. We do not encode masked tokens, use a fast convolutional decoder and amortize the effort to build teacher representations. data2vec 2.0 benefits from the rich contextualized target representations introduced in data2vec which enable a fast self-supervised learner. Experiments on ImageNet-1K image classification show that data2vec 2.0 matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time, on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x less time, and on GLUE natural language understanding it matches a retrained RoBERTa model in half the time. Trading some speed for accuracy results in ImageNet-1K top-1 accuracy of 86.8\textbackslash\% with a ViT-L model trained for 150 epochs.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/GHSDXLZX/Baevski et al. - 2022 - Efficient Self-supervised Learning with Contextual.pdf;/Users/lukakuma/Zotero/storage/8N5F4R3H/2212.html}
}

@online{baiConstitutionalAIHarmlessness2022,
  title = {Constitutional {{AI}}: {{Harmlessness}} from {{AI Feedback}}},
  shorttitle = {Constitutional {{AI}}},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  date = {2022-12-15},
  number = {arXiv:2212.08073},
  eprint = {arXiv:2212.08073},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.08073},
  urldate = {2023-02-24},
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  pubstate = {preprint},
  keywords = {alignment},
  file = {/Users/lukakuma/Zotero/storage/AYCP287E/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf;/Users/lukakuma/Zotero/storage/HWDGB49I/2212.html}
}

@unpublished{baiTrainingHelpfulHarmless2022,
  title = {Training a {{Helpful}} and {{Harmless Assistant}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  date = {2022-04-12},
  number = {arXiv:2204.05862},
  eprint = {2204.05862},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.05862},
  urldate = {2022-05-17},
  abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
  keywords = {alignment},
  file = {/Users/lukakuma/Zotero/storage/K669INQZ/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf;/Users/lukakuma/Zotero/storage/UL25GQHA/2204.html}
}

@article{bakerVideoPreTrainingVPT2022,
  title = {Video {{PreTraining}} ({{VPT}}): {{Learning}} to {{Act}} by {{Watching Unlabeled Online Videos}}},
  author = {Baker, Bowen and Akkaya, Ilge and Zhokhov, Peter and Huizinga, Joost and Tang, Jie and Ecoffet, Adrien and Houghton, Brandon and Sampedro, Raul and Clune, Jeff},
  date = {2022-06-23},
  pages = {34},
  abstract = {Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. 1\textendash 6 However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data \textendash{} here, online videos of people playing Minecraft \textendash{} from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zeroshot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit humanlevel performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.},
  langid = {english},
  keywords = {action/digital,alignment,OpenAI,read},
  file = {/Users/lukakuma/Zotero/storage/I3YP5H83/Baker et al. - Video PreTraining (VPT) Learning to Act by Watchi.pdf}
}

@online{bakhtinMasteringGameNoPress2022,
  title = {Mastering the {{Game}} of {{No-Press Diplomacy}} via {{Human-Regularized Reinforcement Learning}} and {{Planning}}},
  author = {Bakhtin, Anton and Wu, David J. and Lerer, Adam and Gray, Jonathan and Jacob, Athul Paul and Farina, Gabriele and Miller, Alexander H. and Brown, Noam},
  date = {2022-10-11},
  number = {arXiv:2210.05492},
  eprint = {arXiv:2210.05492},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.05492},
  urldate = {2022-11-24},
  abstract = {No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitation-learned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.},
  pubstate = {preprint},
  keywords = {(ext) CICERO},
  file = {/Users/lukakuma/Zotero/storage/KWUI7YQF/Bakhtin et al. - 2022 - Mastering the Game of No-Press Diplomacy via Human.pdf;/Users/lukakuma/Zotero/storage/8RPFSBY4/2210.html}
}

@online{bakhtinNoPressDiplomacyScratch2021,
  title = {No-{{Press Diplomacy}} from {{Scratch}}},
  author = {Bakhtin, Anton and Wu, David and Lerer, Adam and Brown, Noam},
  date = {2021-10-06},
  number = {arXiv:2110.02924},
  eprint = {arXiv:2110.02924},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.02924},
  urldate = {2022-11-24},
  abstract = {Prior AI successes in complex games have largely focused on settings with at most hundreds of actions at each decision point. In contrast, Diplomacy is a game with more than 10\^20 possible actions per turn. Previous attempts to address games with large branching factors, such as Diplomacy, StarCraft, and Dota, used human data to bootstrap the policy or used handcrafted reward shaping. In this paper, we describe an algorithm for action exploration and equilibrium approximation in games with combinatorial action spaces. This algorithm simultaneously performs value iteration while learning a policy proposal network. A double oracle step is used to explore additional actions to add to the policy proposals. At each state, the target state value and policy for the model training are computed via an equilibrium search procedure. Using this algorithm, we train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. Additionally, we extend our methods to full-scale no-press Diplomacy and for the first time train an agent from scratch with no human data. We present evidence that this agent plays a strategy that is incompatible with human-data bootstrapped agents. This presents the first strong evidence of multiple equilibria in Diplomacy and suggests that self play alone may be insufficient for achieving superhuman performance in Diplomacy.},
  pubstate = {preprint},
  keywords = {(ext) CICERO},
  file = {/Users/lukakuma/Zotero/storage/LXWIWWAZ/Bakhtin et al. - 2021 - No-Press Diplomacy from Scratch.pdf;/Users/lukakuma/Zotero/storage/IGAJ3SBJ/2110.html}
}

@online{bakkerFinetuningLanguageModels2022,
  title = {Fine-Tuning Language Models to Find Agreement among Humans with Diverse Preferences},
  author = {Bakker, Michiel A. and Chadwick, Martin J. and Sheahan, Hannah R. and Tessler, Michael Henry and Campbell-Gillingham, Lucy and Balaguer, Jan and McAleese, Nat and Glaese, Amelia and Aslanides, John and Botvinick, Matthew M. and Summerfield, Christopher},
  date = {2022-11-27},
  number = {arXiv:2211.15006},
  eprint = {arXiv:2211.15006},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.15006},
  urldate = {2022-12-01},
  abstract = {Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single ``generic'' user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g., ``should we raise taxes on the rich?''), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs ({$>$} 70\%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions ({$>$} 65\%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another.},
  langid = {english},
  pubstate = {preprint},
  keywords = {finetuning},
  file = {/Users/lukakuma/Zotero/storage/T352LTA2/Bakker et al. - 2022 - Fine-tuning language models to find agreement amon.pdf}
}

@online{balajiEDiffiTexttoImageDiffusion2022a,
  title = {{{eDiffi}}: {{Text-to-Image Diffusion Models}} with an {{Ensemble}} of {{Expert Denoisers}}},
  shorttitle = {{{eDiffi}}},
  author = {Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, Arash and Song, Jiaming and Kreis, Karsten and Aittala, Miika and Aila, Timo and Laine, Samuli and Catanzaro, Bryan and Karras, Tero and Liu, Ming-Yu},
  date = {2022-11-02},
  number = {arXiv:2211.01324},
  eprint = {arXiv:2211.01324},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.01324},
  urldate = {2022-11-04},
  abstract = {Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiffi, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiffi's "paint-with-words" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiffi/},
  langid = {english},
  pubstate = {preprint},
  keywords = {*diffusion,image generation},
  file = {/Users/lukakuma/Zotero/storage/E65S6MUK/Balaji et al. - 2022 - eDiffi Text-to-Image Diffusion Models with an Ens.pdf}
}

@inproceedings{baninoCoBERLContrastiveBERT2021,
  title = {{{CoBERL}}: {{Contrastive BERT}} for {{Reinforcement Learning}}},
  shorttitle = {{{CoBERL}}},
  author = {Banino, Andrea and Badia, Adria Puigdomenech and Walker, Jacob C. and Scholtes, Tim and Mitrovic, Jovana and Blundell, Charles},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=sRZ3GhmegS},
  urldate = {2022-05-15},
  abstract = {Many reinforcement learning (RL) agents require a large amount of experience to solve tasks. We propose Contrastive BERT for RL (COBERL), an agent that combines a new contrastive loss and a hybrid...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/5VCNBKZN/Banino et al. - 2021 - CoBERL Contrastive BERT for Reinforcement Learnin.pdf;/Users/lukakuma/Zotero/storage/9ZCNRECV/forum.html}
}

@online{baoBEiTBERTPreTraining2021,
  title = {{{BEiT}}: {{BERT Pre-Training}} of {{Image Transformers}}},
  shorttitle = {{{BEiT}}},
  author = {Bao, Hangbo and Dong, Li and Wei, Furu},
  date = {2021-06-15},
  number = {arXiv:2106.08254},
  eprint = {arXiv:2106.08254},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.08254},
  urldate = {2022-08-12},
  abstract = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2\% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8\%) with the same setup. Moreover, large-size BEiT obtains 86.3\% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/5R2WQV8I/Bao et al. - 2021 - BEiT BERT Pre-Training of Image Transformers.pdf;/Users/lukakuma/Zotero/storage/BHHFPHAA/2106.html}
}

@online{baoVLBEiTGenerativeVisionLanguage2022,
  title = {{{VL-BEiT}}: {{Generative Vision-Language Pretraining}}},
  shorttitle = {{{VL-BEiT}}},
  author = {Bao, Hangbo and Wang, Wenhui and Dong, Li and Wei, Furu},
  date = {2022-06-02},
  number = {arXiv:2206.01127},
  eprint = {arXiv:2206.01127},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.01127},
  urldate = {2022-09-01},
  abstract = {We introduce a vision-language foundation model called VL-BEiT, which is a bidirectional multimodal Transformer learned by generative pretraining. Our minimalist solution conducts masked prediction on both monomodal and multimodal data with a shared Transformer. Specifically, we perform masked vision-language modeling on image-text pairs, masked language modeling on texts, and masked image modeling on images. VL-BEiT is learned from scratch with one unified pretraining task, one shared backbone, and one-stage training. Our method is conceptually simple and empirically effective. Experimental results show that VL-BEiT obtains strong results on various vision-language benchmarks, such as visual question answering, visual reasoning, and image-text retrieval. Moreover, our method learns transferable visual features, achieving competitive performance on image classification, and semantic segmentation.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/AJAQKGV7/Bao et al. - 2022 - VL-BEiT Generative Vision-Language Pretraining.pdf;/Users/lukakuma/Zotero/storage/NQMG6NQQ/2206.html}
}

@online{baoVLMoUnifiedVisionLanguage2022,
  title = {{{VLMo}}: {{Unified Vision-Language Pre-Training}} with {{Mixture-of-Modality-Experts}}},
  shorttitle = {{{VLMo}}},
  author = {Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Wei, Furu},
  date = {2022-05-27},
  number = {arXiv:2111.02358},
  eprint = {arXiv:2111.02358},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.02358},
  urldate = {2022-08-12},
  abstract = {We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer, where each block contains a pool of modality-specific experts and a shared self-attention layer. Because of the modeling flexibility of MoME, pretrained VLMo can be fine-tuned as a fusion encoder for vision-language classification tasks, or used as a dual encoder for efficient image-text retrieval. Moreover, we propose a stagewise pre-training strategy, which effectively leverages large-scale image-only and text-only data besides image-text pairs. Experimental results show that VLMo achieves state-of-the-art results on various vision-language tasks, including VQA, NLVR2 and image-text retrieval. The code and pretrained models are available at https://aka.ms/vlmo.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/K2L7CSN5/Bao et al. - 2022 - VLMo Unified Vision-Language Pre-Training with Mi.pdf;/Users/lukakuma/Zotero/storage/FYGTIJ8T/2111.html}
}

@online{bardesVICRegVarianceInvarianceCovarianceRegularization2022,
  title = {{{VICReg}}: {{Variance-Invariance-Covariance Regularization}} for {{Self-Supervised Learning}}},
  shorttitle = {{{VICReg}}},
  author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
  date = {2022-01-28},
  number = {arXiv:2105.04906},
  eprint = {arXiv:2105.04906},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.04906},
  urldate = {2022-12-09},
  abstract = {Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.},
  pubstate = {preprint},
  keywords = {objective fn},
  file = {/Users/lukakuma/Zotero/storage/P2RQG777/Bardes et al. - 2022 - VICReg Variance-Invariance-Covariance Regularizat.pdf;/Users/lukakuma/Zotero/storage/MHMXT3VZ/2105.html}
}

@article{barhamPathwaysAsynchronousDistributed2022,
  title = {Pathways: {{Asynchronous Distributed Dataflow}} for {{ML}}},
  shorttitle = {Pathways},
  author = {Barham, Paul and Chowdhery, Aakanksha and Dean, Jeff and Ghemawat, Sanjay and Hand, Steven and Hurt, Dan and Isard, Michael and Lim, Hyeontaek and Pang, Ruoming and Roy, Sudip and Saeta, Brennan and Schuh, Parker and Sepassi, Ryan and Shafey, Laurent El and Thekkath, Chandramohan A. and Wu, Yonghui},
  date = {2022-03-23},
  url = {https://arxiv.org/abs/2203.12533v1},
  urldate = {2022-04-09},
  abstract = {We present the design of a new large scale orchestration layer for accelerators. Our system, Pathways, is explicitly designed to enable exploration of new systems and ML research ideas, while retaining state of the art performance for current models. Pathways uses a sharded dataflow graph of asynchronous operators that consume and produce futures, and efficiently gang-schedules heterogeneous parallel computations on thousands of accelerators while coordinating data transfers over their dedicated interconnects. Pathways makes use of a novel asynchronous distributed dataflow design that lets the control plane execute in parallel despite dependencies in the data plane. This design, with careful engineering, allows Pathways to adopt a single-controller model that makes it easier to express complex new parallelism patterns. We demonstrate that Pathways can achieve performance parity (\textasciitilde 100\% accelerator utilization) with state-of-the-art systems when running SPMD computations over 2048 TPUs, while also delivering throughput comparable to the SPMD case for Transformer models that are pipelined across 16 stages, or sharded across two islands of accelerators connected over a data center network.},
  langid = {english},
  keywords = {distributed training,read},
  file = {/Users/lukakuma/Zotero/storage/GDUUXHXK/Barham et al. - 2022 - Pathways Asynchronous Distributed Dataflow for ML.pdf;/Users/lukakuma/Zotero/storage/FWEUJJGS/2203.html}
}

@article{barretoFastReinforcementLearning2020,
  title = {Fast Reinforcement Learning with Generalized Policy Updates},
  author = {Barreto, Andr\'e and Hou, Shaobo and Borsa, Diana and Silver, David and Precup, Doina},
  date = {2020-12},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {48},
  pages = {30079--30087},
  publisher = {{Proceedings of the National Academy of Sciences}},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1907370117},
  urldate = {2022-06-10},
  file = {/Users/lukakuma/Zotero/storage/EHGUJHG7/Barreto et al. - 2020 - Fast reinforcement learning with generalized polic.pdf}
}

@online{bauerSpatialFunctaScaling2023,
  title = {Spatial {{Functa}}: {{Scaling Functa}} to {{ImageNet Classification}} and {{Generation}}},
  shorttitle = {Spatial {{Functa}}},
  author = {Bauer, Matthias and Dupont, Emilien and Brock, Andy and Rosenbaum, Dan and Schwarz, Jonathan and Kim, Hyunjik},
  date = {2023-02-06},
  number = {arXiv:2302.03130},
  eprint = {arXiv:2302.03130},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.03130},
  urldate = {2023-02-08},
  abstract = {Neural fields, also known as implicit neural representations, have emerged as a powerful means to represent complex signals of various modalities. Based on this Dupont et al. (2022) introduce a framework that views neural fields as data, termed *functa*, and proposes to do deep learning directly on this dataset of neural fields. In this work, we show that the proposed framework faces limitations when scaling up to even moderately complex datasets such as CIFAR-10. We then propose *spatial functa*, which overcome these limitations by using spatially arranged latent representations of neural fields, thereby allowing us to scale up the approach to ImageNet-1k at 256x256 resolution. We demonstrate competitive performance to Vision Transformers (Steiner et al., 2022) on classification and Latent Diffusion (Rombach et al., 2022) on image generation respectively.},
  pubstate = {preprint},
  keywords = {DeepMind,functa},
  file = {/Users/lukakuma/Zotero/storage/GI8CMZB8/Bauer et al. - 2023 - Spatial Functa Scaling Functa to ImageNet Classif.pdf;/Users/lukakuma/Zotero/storage/WNMZUNAK/2302.html}
}

@online{bavarianEfficientTrainingLanguage2022,
  title = {Efficient {{Training}} of {{Language Models}} to {{Fill}} in the {{Middle}}},
  author = {Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
  date = {2022-07-28},
  number = {arXiv:2207.14255},
  eprint = {arXiv:2207.14255},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.14255},
  urldate = {2022-07-29},
  abstract = {We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.},
  pubstate = {preprint},
  keywords = {objective fn,OpenAI,read},
  file = {/Users/lukakuma/Zotero/storage/2J89TNUA/Bavarian et al. - 2022 - Efficient Training of Language Models to Fill in t.pdf;/Users/lukakuma/Zotero/storage/6PL3HM2H/2207.html}
}

@article{bayernAreAutonomousEntities2019,
  title = {Are {{Autonomous Entities Possible}}?},
  author = {Bayern, Shawn},
  date = {2019-06-24},
  journaltitle = {NULR Online},
  url = {https://scholarlycommons.law.northwestern.edu/nulr_online/271},
  file = {/Users/lukakuma/Zotero/storage/MKRF6ZUZ/Bayern - 2019 - Are Autonomous Entities Possible.pdf}
}

@inproceedings{bebergFoldingHomeLessons2009,
  title = {Folding@home: {{Lessons}} from Eight Years of Volunteer Distributed Computing},
  shorttitle = {Folding@home},
  booktitle = {2009 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}},
  author = {Beberg, Adam L. and Ensign, Daniel L. and Jayachandran, Guha and Khaliq, Siraj and Pande, Vijay S.},
  date = {2009-05},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Rome, Italy}},
  url = {http://ieeexplore.ieee.org/document/5160922/},
  urldate = {2022-09-04},
  abstract = {Accurate simulation of biophysical processes requires vast computing resources. Folding@home is a distributed computing system first released in 2000 to provide such resources needed to simulate protein folding and other biomolecular phenomena. Now operating in the range of 5 PetaFLOPS sustained, it provides more computing power than can typically be gathered and operated locally due to cost, physical space, and electrical/cooling load. This paper describes the architecture and operation of Folding@home, along with some lessons learned over the lifetime of the project.},
  eventtitle = {Distributed {{Processing}} ({{IPDPS}})},
  isbn = {978-1-4244-3751-1},
  langid = {english},
  keywords = {*open source AI},
  file = {/Users/lukakuma/Zotero/storage/2QEE7DUI/Beberg et al. - 2009 - Folding@home Lessons from eight years of voluntee.pdf}
}

@article{bellemareArcadeLearningEnvironment2013,
  title = {The {{Arcade Learning Environment}}: {{An Evaluation Platform}} for {{General Agents}}},
  shorttitle = {The {{Arcade Learning Environment}}},
  author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  date = {2013-06-14},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {jair},
  volume = {47},
  eprint = {1207.4708},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {253--279},
  issn = {1076-9757},
  url = {http://arxiv.org/abs/1207.4708},
  urldate = {2022-06-07},
  abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/LMCHIDW8/Bellemare et al. - 2013 - The Arcade Learning Environment An Evaluation Pla.pdf;/Users/lukakuma/Zotero/storage/VCWJ8GZK/1207.html}
}

@online{belloAttentionAugmentedConvolutional2020,
  title = {Attention {{Augmented Convolutional Networks}}},
  author = {Bello, Irwan and Zoph, Barret and Vaswani, Ashish and Shlens, Jonathon and Le, Quoc V.},
  date = {2020-09-09},
  number = {arXiv:1904.09925},
  eprint = {arXiv:1904.09925},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.09925},
  urldate = {2022-08-03},
  abstract = {Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classification. We find in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a \$1.3\textbackslash\%\$ top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 mAP in COCO Object Detection on top of a RetinaNet baseline.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/KGKULR2P/Bello et al. - 2020 - Attention Augmented Convolutional Networks.pdf;/Users/lukakuma/Zotero/storage/R8SHNQF6/1904.html}
}

@inproceedings{benderClimbingNLUMeaning2020a,
  title = {Climbing towards {{NLU}}: {{On Meaning}}, {{Form}}, and {{Understanding}} in the {{Age}} of {{Data}}},
  shorttitle = {Climbing towards {{NLU}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Bender, Emily M. and Koller, Alexander},
  date = {2020-07},
  pages = {5185--5198},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  url = {https://aclanthology.org/2020.acl-main.463},
  urldate = {2022-11-17},
  abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as ``understanding'' language or capturing ``meaning''. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of ``Taking Stock of Where We've Been and Where We're Going'', we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
  eventtitle = {{{ACL}} 2020},
  file = {/Users/lukakuma/Zotero/storage/7CW6VMAH/Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf}
}

@inproceedings{benderDangersStochasticParrots2021,
  title = {On the {{Dangers}} of {{Stochastic Parrots}}: {{Can Language Models Be Too Big}}? ü¶ú},
  shorttitle = {On the {{Dangers}} of {{Stochastic Parrots}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  date = {2021-03-03},
  pages = {610--623},
  publisher = {{ACM}},
  location = {{Virtual Event Canada}},
  url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
  urldate = {2022-11-02},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  eventtitle = {{{FAccT}} '21: 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  isbn = {978-1-4503-8309-7},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/RJ2CLIFU/Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf}
}

@unpublished{bengioConsciousnessPrior2019,
  title = {The {{Consciousness Prior}}},
  author = {Bengio, Yoshua},
  date = {2019-12-02},
  eprint = {1709.08568},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1709.08568},
  urldate = {2022-05-12},
  abstract = {A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms.},
  keywords = {GFlowNet,read},
  file = {/Users/lukakuma/Zotero/storage/9Q8379UX/Bengio - 2019 - The Consciousness Prior.pdf;/Users/lukakuma/Zotero/storage/GEP4LLA7/1709.html}
}

@unpublished{bengioFlowNetworkBased2021,
  title = {Flow {{Network}} Based {{Generative Models}} for {{Non-Iterative Diverse Candidate Generation}}},
  author = {Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
  date = {2021-11-19},
  eprint = {2106.04399},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.04399},
  urldate = {2022-05-01},
  abstract = {This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation. Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/JRN8KU3M/Bengio et al. - 2021 - Flow Network based Generative Models for Non-Itera.pdf;/Users/lukakuma/Zotero/storage/QDA2QET9/2106.html}
}

@unpublished{bengioGFlowNetFoundations2022,
  title = {{{GFlowNet Foundations}}},
  author = {Bengio, Yoshua and Deleu, Tristan and Hu, Edward J. and Lahlou, Salem and Tiwari, Mo and Bengio, Emmanuel},
  date = {2022-04-07},
  eprint = {2111.09266},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2111.09266},
  urldate = {2022-05-01},
  abstract = {Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/EKHAIJ3B/Bengio et al. - 2022 - GFlowNet Foundations.pdf;/Users/lukakuma/Zotero/storage/7SVBS427/2111.html}
}

@unpublished{bengioRepresentationLearningReview2014,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  date = {2014-04-23},
  eprint = {1206.5538},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1206.5538},
  urldate = {2022-04-28},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/8H2MZH9Q/Bengio et al. - 2014 - Representation Learning A Review and New Perspect.pdf;/Users/lukakuma/Zotero/storage/Z9WYN6JJ/1206.html}
}

@article{berggrenRoadmapEmergingHardware2020,
  title = {Roadmap on Emerging Hardware and Technology for Machine Learning},
  author = {Berggren, Karl and Xia, Qiangfei and Likharev, Konstantin K. and Strukov, Dmitri B. and Jiang, Hao and Mikolajick, Thomas and Querlioz, Damien and Salinga, Martin and Erickson, John R. and Pi, Shuang and Xiong, Feng and Lin, Peng and Li, Can and Chen, Yu and Xiong, Shisheng and Hoskins, Brian D. and Daniels, Matthew W. and Madhavan, Advait and Liddle, James A. and McClelland, Jabez J. and Yang, Yuchao and Rupp, Jennifer and Nonnenmann, Stephen S. and Cheng, Kwang-Ting and Gong, Nanbo and Lastras-Monta\~no, Miguel Angel and Talin, A. Alec and Salleo, Alberto and Shastri, Bhavin J. and family=Lima, given=Thomas Ferreira, prefix=de, useprefix=false and Prucnal, Paul and Tait, Alexander N. and Shen, Yichen and Meng, Huaiyu and Roques-Carmes, Charles and Cheng, Zengguang and Bhaskaran, Harish and Jariwala, Deep and Wang, Han and Shainline, Jeffrey M. and Segall, Kenneth and Yang, J. Joshua and Roy, Kaushik and Datta, Suman and Raychowdhury, Arijit},
  date = {2020-10},
  journaltitle = {Nanotechnology},
  shortjournal = {Nanotechnology},
  volume = {32},
  number = {1},
  pages = {012002},
  publisher = {{IOP Publishing}},
  issn = {0957-4484},
  url = {https://doi.org/10.1088/1361-6528/aba70f},
  urldate = {2022-04-14},
  abstract = {Recent progress in artificial intelligence is largely attributed to the rapid development of machine learning, especially in the algorithm and neural network models. However, it is the performance of the hardware, in particular the energy efficiency of a computing system that sets the fundamental limit of the capability of machine learning. Data-centric computing requires a revolution in hardware systems, since traditional digital computers based on transistors and the von Neumann architecture were not purposely designed for neuromorphic computing. A hardware platform based on emerging devices and new architecture is the hope for future computing with dramatically improved throughput and energy efficiency. Building such a system, nevertheless, faces a number of challenges, ranging from materials selection, device optimization, circuit fabrication and system integration, to name a few. The aim of this Roadmap is to present a snapshot of emerging hardware technologies that are potentially beneficial for machine learning, providing the Nanotechnology readers with a perspective of challenges and opportunities in this burgeoning field.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/7R89DM8F/Berggren et al. - 2020 - Roadmap on emerging hardware and technology for ma.pdf}
}

@unpublished{bernerModernMathematicsDeep2021,
  title = {The {{Modern Mathematics}} of {{Deep Learning}}},
  author = {Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  date = {2021-05-09},
  eprint = {2105.04026},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2105.04026},
  urldate = {2022-03-08},
  abstract = {We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.},
  version = {1},
  file = {/Users/lukakuma/Zotero/storage/43HKLUWE/Berner et al. - 2021 - The Modern Mathematics of Deep Learning.pdf;/Users/lukakuma/Zotero/storage/JSD5V3WI/2105.html}
}

@book{bertsekasLessonsAlphaZeroOptimal2022,
  title = {Lessons from {{AlphaZero}} for {{Optimal}}, {{Model Predictive}}, and {{Adaptive Control}}},
  author = {Bertsekas, Dimitri},
  date = {2022-03-19},
  eprint = {KRllEAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Athena Scientific}},
  abstract = {The purpose of this book is to propose and develop a new conceptual framework for approximate Dynamic Programming (DP) and Reinforcement Learning (RL). This framework centers around two algorithms, which are designed largely independently of each other and operate in synergy through the powerful mechanism of Newton\&\#39;s method. We call these the off-line training and the on-line play algorithms; the names are borrowed from some of the major successes~of RL involving games. Primary examples are the recent (2017) AlphaZero program (which plays chess), and the similarly structured and earlier (1990s) TD-Gammon program (which plays backgammon). In these game contexts, the off-line training algorithm is the method used to teach the program how to evaluate positions and to generate good moves at any given position, while the on-line play algorithm is the method used to play in real time against human or computer opponents.Both AlphaZero and TD-Gammon were trained off-line extensively using neural networks and an approximate version of the fundamental DP algorithm of policy iteration. Yet the AlphaZero player that was obtained off-line is not used directly during on-line play (it is too inaccurate due to approximation errors that are inherent in off-line neural network training). Instead a separate on-line player is used to select moves, based on multistep lookahead minimization and a terminal position evaluator that was trained using experience with the off-line player. The on-line player performs a form of policy improvement, which is not degraded by neural network approximations. As a result, it greatly improves the performance of the off-line player.~Similarly, TD-Gammon performs on-line a policy improvement step using one-step or two-step lookahead minimization, which is not degraded by neural network approximations. To this end it uses an off-line neural network-trained terminal position evaluator, and importantly it also extends its on-line lookahead by rollout (simulation with the one-step lookahead player that is based on the~position evaluator).~Significantly, the synergy between off-line training and on-line play also underlies Model Predictive Control (MPC), a major control system design methodology that has been extensively developed since the 1980s. This synergy can be understood in terms of abstract models of infinite horizon DP and simple geometrical constructions, and~helps to explain the all-important stability issues within the MPC context.~An additional benefit of policy improvement by approximation in value space, not observed in the context of games (which have stable rules and environment), is that it works well with changing problem parameters and on-line replanning, similar to indirect adaptive control. Here the Bellman equation is perturbed due to the parameter changes, but approximation in value space still operates as a Newton step. An essential requirement here is that a system model is estimated on-line through some identification method, and is used during the one-step or multistep lookahead minimization process.In this monograph we  aim to provide insights (often based on visualization), which explain the beneficial effects of on-line decision making on top of off-line training. In the process, we will bring out the strong connections between the artificial intelligence view of RL, and the control theory views of MPC and adaptive control. Moreover, we will show that in addition to MPC and adaptive control, our conceptual framework can~be effectively integrated with other important methodologies such as multiagent systems and decentralized control, discrete and Bayesian optimization, and heuristic algorithms for discrete optimization.~~One of our~principal aims is to show, through the algorithmic ideas of Newton\&\#39;s method and the unifying principles of abstract DP, that the AlphaZero/TD-Gammon methodology of approximation in value space and rollout applies very broadly to deterministic and stochastic optimal control problems. Newton\&\#39;s method here is used for the solution of Bellman\&\#39;s equation, an operator equation that applies universally within~DP with both discrete and continuous state and control spaces, as well as finite and infinite horizon.},
  isbn = {978-1-886529-17-5},
  langid = {english},
  pagetotal = {225},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/6NHMQJT6/Bertsekas - 2022 - Lessons from AlphaZero for Optimal, Model Predicti.pdf}
}

@video{bestinteslaTeslaSECRETWeapon2022,
  title = {Tesla's {{SECRET}} Weapon Is {{DSM}} \& {{4D}} Hexagon {{Cube}}'s !!!},
  editor = {{BestInTESLA}},
  date = {2022-05-10},
  url = {https://www.youtube.com/watch?v=HI2wRdx_N4I},
  urldate = {2022-06-04},
  abstract = {If you don't understand this you will never understand Tesla as a company, and its potential !!! THANKS FOR WATCHING Try MooMoo and get up to 5 free stocks, Use this link to get started: https://j.moomoo.com/009GFR *************************************************************************** Link in the video: @The Best Moment in History  https://www.youtube.com/watch?v=CB06U...    *************************************************************************** \ding{164}Ô∏è Support the show \ding{164}Ô∏è At Patreon :  https://www.patreon.com/bestInTESLA **************************************************************************** Become a member of my Youtube channel and get some extra perks: https://www.youtube.com/channel/UChzF... *************************************************************************** üî•Link to Merch store - CHECK OUT ALL THE NEW STUFFüî•: teespring.com/stores/bestintesla **************************************************************************** Donate and Support Make a one time or monthly donation to BestInTESLA without becoming a patron or member: https://www.paypal.com/donate?hosted\_... **************************************************************************** üî• Remember to follow me on Twitter to get all the Tesla News as it comes in:  https://twitter.com/TeslaBest And follow me on Instagram : https://www.instagram.com/bestintesla/ **************************************************************************** Get some of the Jeda's cool products for your Tesla, and support the show: Affiliate Link: https://getjeda.com/ref/87/ **************************************************************************** üö® Get \$20 discount on Tesla Taxi in Australia üö® Website  https://www.teslataxi.com.au Linkedin  https://www.linkedin.com/in/millin-be... Discount code : Use code: BIT20 - to rent a Tesla and you will get  a \$20 discount. Enjoy üòâ **************************************************************************** Music from Artlist: License Number - 674079},
  editortype = {director}
}

@online{beurer-kellnerLearningConfigureComputer2022,
  title = {Learning to {{Configure Computer Networks}} with {{Neural Algorithmic Reasoning}}},
  author = {Beurer-Kellner, Luca and Vechev, Martin and Vanbever, Laurent and Veli\v{c}kovi\'c, Petar},
  date = {2022-10-26},
  number = {arXiv:2211.01980},
  eprint = {arXiv:2211.01980},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.01980},
  urldate = {2022-12-22},
  abstract = {We present a new method for scaling automatic configuration of computer networks. The key idea is to relax the computationally hard search problem of finding a configuration that satisfies a given specification into an approximate objective amenable to learning-based techniques. Based on this idea, we train a neural algorithmic model which learns to generate configurations likely to (fully or partially) satisfy a given specification under existing routing protocols. By relaxing the rigid satisfaction guarantees, our approach (i) enables greater flexibility: it is protocol-agnostic, enables cross-protocol reasoning, and does not depend on hardcoded rules; and (ii) finds configurations for much larger computer networks than previously possible. Our learned synthesizer is up to 490x faster than state-of-the-art SMT-based methods, while producing configurations which on average satisfy more than 93\% of the provided requirements.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/YRPGUTT2/Beurer-Kellner et al. - 2022 - Learning to Configure Computer Networks with Neura.pdf;/Users/lukakuma/Zotero/storage/YI8NBCLZ/2211.html}
}

@online{bevilacquaSizeInvariantGraphRepresentations2021,
  title = {Size-{{Invariant Graph Representations}} for {{Graph Classification Extrapolations}}},
  author = {Bevilacqua, Beatrice and Zhou, Yangze and Ribeiro, Bruno},
  date = {2021-07-06},
  number = {arXiv:2103.05045},
  eprint = {arXiv:2103.05045},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.05045},
  urldate = {2022-12-20},
  abstract = {In general, graph representation learning methods assume that the train and test data come from the same distribution. In this work we consider an underexplored area of an otherwise rapidly developing field of graph representation learning: The task of out-of-distribution (OOD) graph classification, where train and test data have different distributions, with test data unavailable during training. Our work shows it is possible to use a causal model to learn approximately invariant representations that better extrapolate between train and test data. Finally, we conclude with synthetic and real-world dataset experiments showcasing the benefits of representations that are invariant to train/test distribution shifts.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/2C2ANTZK/Bevilacqua et al. - 2021 - Size-Invariant Graph Representations for Graph Cla.pdf;/Users/lukakuma/Zotero/storage/DYDW4NN9/2103.html}
}

@unpublished{beyerBetterPlainViT2022,
  title = {Better Plain {{ViT}} Baselines for {{ImageNet-1k}}},
  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},
  date = {2022-05-03},
  eprint = {2205.01580},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.01580},
  urldate = {2022-05-05},
  abstract = {It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76\% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80\% in less than one day.},
  file = {/Users/lukakuma/Zotero/storage/44IYVZPT/Beyer et al. - 2022 - Better plain ViT baselines for ImageNet-1k.pdf;/Users/lukakuma/Zotero/storage/5TIQVXHM/2205.html}
}

@unpublished{bianColossalAIUnifiedDeep2021,
  title = {Colossal-{{AI}}: {{A Unified Deep Learning System For Large-Scale Parallel Training}}},
  shorttitle = {Colossal-{{AI}}},
  author = {Bian, Zhengda and Liu, Hongxin and Wang, Boxiang and Huang, Haichen and Li, Yongbin and Wang, Chuanrui and Cui, Fan and You, Yang},
  date = {2021-10-28},
  eprint = {2110.14883},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2110.14883},
  urldate = {2022-04-14},
  abstract = {The Transformer architecture has improved the performance of deep learning models in domains such as Computer Vision and Natural Language Processing. Together with better performance come larger model sizes. This imposes challenges to the memory wall of the current accelerator hardware such as GPU. It is never ideal to train large models such as Vision Transformer, BERT, and GPT on a single GPU or a single machine. There is an urgent demand to train models in a distributed environment. However, distributed training, especially model parallelism, often requires domain expertise in computer systems and architecture. It remains a challenge for AI researchers to implement complex distributed training solutions for their models. In this paper, we introduce Colossal-AI, which is a unified parallel training system designed to seamlessly integrate different paradigms of parallelization techniques including data parallelism, pipeline parallelism, multiple tensor parallelism, and sequence parallelism. Colossal-AI aims to support the AI community to write distributed models in the same way as how they write models normally. This allows them to focus on developing the model architecture and separates the concerns of distributed training from the development process. The documentations can be found at https://www.colossalai.org and the source code can be found at https://github.com/hpcaitech/ColossalAI.},
  file = {/Users/lukakuma/Zotero/storage/QTHG2GTV/Bian et al. - 2021 - Colossal-AI A Unified Deep Learning System For La.pdf;/Users/lukakuma/Zotero/storage/W8T9D7WQ/2110.html}
}

@unpublished{bianMaximizingParallelismDistributed2021,
  title = {Maximizing {{Parallelism}} in {{Distributed Training}} for {{Huge Neural Networks}}},
  author = {Bian, Zhengda and Xu, Qifan and Wang, Boxiang and You, Yang},
  date = {2021-05-30},
  eprint = {2105.14450},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2105.14450},
  urldate = {2022-04-14},
  abstract = {The recent Natural Language Processing techniques have been refreshing the state-of-the-art performance at an incredible speed. Training huge language models is therefore an imperative demand in both industry and academy. However, huge language models impose challenges to both hardware and software. Graphical processing units (GPUs) are iterated frequently to meet the exploding demand, and a variety of ASICs like TPUs are spawned. However, there is still a tension between the fast growth of the extremely huge models and the fact that Moore's law is approaching the end. To this end, many model parallelism techniques are proposed to distribute the model parameters to multiple devices, so as to alleviate the tension on both memory and computation. Our work is the first to introduce a 3-dimensional model parallelism for expediting huge language models. By reaching a perfect load balance, our approach presents smaller memory and communication cost than existing state-of-the-art 1-D and 2-D model parallelism. Our experiments on 64 TACC's V100 GPUs show that our 3-D parallelism outperforms the 1-D and 2-D parallelism with 2.32x and 1.57x speedup, respectively.},
  file = {/Users/lukakuma/Zotero/storage/AT5EDZ2F/Bian et al. - 2021 - Maximizing Parallelism in Distributed Training for.pdf;/Users/lukakuma/Zotero/storage/WR9P7LNZ/2105.html}
}

@unpublished{biskExperienceGroundsLanguage2020,
  title = {Experience {{Grounds Language}}},
  author = {Bisk, Yonatan and Holtzman, Ari and Thomason, Jesse and Andreas, Jacob and Bengio, Yoshua and Chai, Joyce and Lapata, Mirella and Lazaridou, Angeliki and May, Jonathan and Nisnevich, Aleksandr and Pinto, Nicolas and Turian, Joseph},
  date = {2020-11-01},
  eprint = {2004.10151},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2004.10151},
  urldate = {2022-04-13},
  abstract = {Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/E3LSQJVI/Bisk et al. - 2020 - Experience Grounds Language.pdf;/Users/lukakuma/Zotero/storage/KCZEQXV7/2004.html}
}

@unpublished{blackGPTNeoX20BOpenSourceAutoregressive2022,
  title = {{{GPT-NeoX-20B}}: {{An Open-Source Autoregressive Language Model}}},
  shorttitle = {{{GPT-NeoX-20B}}},
  author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  date = {2022-04-14},
  eprint = {2204.06745},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.06745},
  urldate = {2022-04-27},
  abstract = {We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \textbackslash model\{\}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.},
  file = {/Users/lukakuma/Zotero/storage/35TPG54Z/Black et al. - 2022 - GPT-NeoX-20B An Open-Source Autoregressive Langua.pdf;/Users/lukakuma/Zotero/storage/E4XEH24D/2204.html}
}

@online{bohnetAttributedQuestionAnswering2022,
  title = {Attributed {{Question Answering}}: {{Evaluation}} and {{Modeling}} for {{Attributed Large Language Models}}},
  shorttitle = {Attributed {{Question Answering}}},
  author = {Bohnet, Bernd and Tran, Vinh Q. and Verga, Pat and Aharoni, Roee and Andor, Daniel and Soares, Livio Baldini and Eisenstein, Jacob and Ganchev, Kuzman and Herzig, Jonathan and Hui, Kai and Kwiatkowski, Tom and Ma, Ji and Ni, Jianmo and Schuster, Tal and Cohen, William W. and Collins, Michael and Das, Dipanjan and Metzler, Donald and Petrov, Slav and Webster, Kellie},
  date = {2022-12-15},
  number = {arXiv:2212.08037},
  eprint = {arXiv:2212.08037},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.08037},
  urldate = {2023-01-14},
  abstract = {Large language models (LLMs) have shown impressive results across a variety of tasks while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial for both system developers and users in this setting. We propose and study Attributed QA as a key first step in the development of attributed LLMs. We develop a reproducable evaluation framework for the task, using human annotations as a gold standard and a correlated automatic metric that we show is suitable for development settings. We describe and benchmark a broad set of architectures for the task. Our contributions give some concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third key question (How to build LLMs with attribution?).},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/J9ALJMX2/Bohnet et al. - 2022 - Attributed Question Answering Evaluation and Mode.pdf;/Users/lukakuma/Zotero/storage/N8QQB7MI/2212.html}
}

@online{bommasaniOpportunitiesRisksFoundation2022,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and family=Arx, given=Sydney, prefix=von, useprefix=true and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R\'e, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram\`er, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  date = {2022-07-12},
  number = {arXiv:2108.07258},
  eprint = {arXiv:2108.07258},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.07258},
  urldate = {2022-12-23},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/VP7YCY2X/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf;/Users/lukakuma/Zotero/storage/DUL3MR7T/2108.html}
}

@unpublished{bonawitzFederatedLearningScale2019,
  title = {Towards {{Federated Learning}} at {{Scale}}: {{System Design}}},
  shorttitle = {Towards {{Federated Learning}} at {{Scale}}},
  author = {Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloe and Kone\v{c}n\'y, Jakub and Mazzocchi, Stefano and McMahan, H. Brendan and Van Overveldt, Timon and Petrou, David and Ramage, Daniel and Roselander, Jason},
  date = {2019-03-22},
  eprint = {1902.01046},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.01046},
  urldate = {2022-03-11},
  abstract = {Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.},
  file = {/Users/lukakuma/Zotero/storage/MA7YDI7G/Bonawitz et al. - 2019 - Towards Federated Learning at Scale System Design.pdf;/Users/lukakuma/Zotero/storage/57XTKZKG/1902.html}
}

@online{borgeaudImprovingLanguageModels2022,
  title = {Improving Language Models by Retrieving from Trillions of Tokens},
  author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and family=Driessche, given=George, prefix=van den, useprefix=false and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
  date = {2022-02-07},
  number = {arXiv:2112.04426},
  eprint = {arXiv:2112.04426},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.04426},
  urldate = {2022-09-20},
  abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a \$2\$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\$\textbackslash times\$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
  pubstate = {preprint},
  keywords = {2-adapter,4-document retrieval,7-multimodal,read},
  file = {/Users/lukakuma/Zotero/storage/WFN8WKAF/Borgeaud et al. - 2022 - Improving language models by retrieving from trill.pdf;/Users/lukakuma/Zotero/storage/WWBXJ4E4/2112.html}
}

@article{borzunovPETALSCollaborativeInference2022,
  title = {{{PETALS}}: {{Collaborative Inference}} and {{Fine-tuning}} of {{Large Models}}},
  author = {Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and Ryabinin, Max and Belkada, Younes and Chumachenko, Artem and Samygin, Pavel and Raffel, Colin},
  date = {2022-09-02},
  pages = {10},
  abstract = {Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires highend hardware unavailable to many researchers. In some cases, LLMs can be used more affordably via RAM offloading or hosted APIs. However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are not flexible enough for research. In this work, we propose PETALS1 \textemdash{} a system for inference and finetuning of large models collaboratively by joining the resources of multiple parties trusted to process client's data. We demonstrate that this strategy significantly outperforms offloading for very large models, running inference of BLOOM-176B on consumer GPUs with {$\approx$} 1 step per second. Unlike most inference APIs, PETALS also natively exposes the hidden states of served models, allowing its users to train and share custom model extensions based on efficient fine-tuning methods.},
  langid = {english},
  keywords = {*open source AI,read},
  file = {/Users/lukakuma/Zotero/storage/2QSDQXLJ/Borzunov et al. - PETALS Collaborative Inference and Fine-tuning of.pdf}
}

@online{borzunovTrainingTransformersTogether2022,
  title = {Training {{Transformers Together}}},
  author = {Borzunov, Alexander and Ryabinin, Max and Dettmers, Tim and Lhoest, Quentin and Saulnier, Lucile and Diskin, Michael and Jernite, Yacine and Wolf, Thomas},
  date = {2022-07-07},
  number = {arXiv:2207.03481},
  eprint = {arXiv:2207.03481},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.03481},
  urldate = {2022-09-02},
  abstract = {The infrastructure necessary for training state-of-the-art models is becoming overly expensive, which makes training such models affordable only to large corporations and institutions. Recent work proposes several methods for training such models collaboratively, i.e., by pooling together hardware from many independent parties and training a shared model over the Internet. In this demonstration, we collaboratively trained a text-to-image transformer similar to OpenAI DALL-E. We invited the viewers to join the ongoing training run, showing them instructions on how to contribute using the available hardware. We explained how to address the engineering challenges associated with such a training run (slow communication, limited memory, uneven performance between devices, and security concerns) and discussed how the viewers can set up collaborative training runs themselves. Finally, we show that the resulting model generates images of reasonable quality on a number of prompts.},
  pubstate = {preprint},
  keywords = {*open source AI,read},
  file = {/Users/lukakuma/Zotero/storage/DSUTWSS7/Borzunov et al. - 2022 - Training Transformers Together.pdf;/Users/lukakuma/Zotero/storage/UVQTZSTH/2207.html}
}

@online{bosselutCOMETCommonsenseTransformers2019,
  title = {{{COMET}}: {{Commonsense Transformers}} for {{Automatic Knowledge Graph Construction}}},
  shorttitle = {{{COMET}}},
  author = {Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
  date = {2019-06-14},
  number = {arXiv:1906.05317},
  eprint = {arXiv:1906.05317},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1906.05317},
  urldate = {2022-12-14},
  abstract = {We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5\% (ATOMIC) and 91.7\% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/3FZ8W35N/Bosselut et al. - 2019 - COMET Commonsense Transformers for Automatic Know.pdf;/Users/lukakuma/Zotero/storage/5WVW3WZN/1906.html}
}

@online{bowmanMeasuringProgressScalable2022a,
  title = {Measuring {{Progress}} on {{Scalable Oversight}} for {{Large Language Models}}},
  author = {Bowman, Samuel R. and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Luko\v{s}i\=ut\.e, Kamil\.e and Askell, Amanda and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Olah, Christopher and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lovitt, Liane and Elhage, Nelson and Schiefer, Nicholas and Joseph, Nicholas and Mercado, Noem\'i and DasSarma, Nova and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Mann, Ben and Kaplan, Jared},
  date = {2022-11-11},
  number = {arXiv:2211.03540},
  eprint = {arXiv:2211.03540},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.03540},
  urldate = {2022-12-02},
  abstract = {Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.},
  pubstate = {preprint},
  keywords = {alignment,knowledge},
  file = {/Users/lukakuma/Zotero/storage/RKJZBQZF/Bowman et al. - 2022 - Measuring Progress on Scalable Oversight for Large.pdf;/Users/lukakuma/Zotero/storage/4HB646ZU/2211.html}
}

@article{branwenScalingHypothesis2020,
  title = {The {{Scaling Hypothesis}}},
  author = {Branwen, Gwern},
  date = {2020-05-28},
  url = {https://www.gwern.net/Scaling-hypothesis},
  urldate = {2022-06-11},
  abstract = {On GPT-3: meta-learning, scaling, implications, and deep theory. The scaling hypothesis: neural nets absorb data \& compute, generalizing and becoming more Bayesian as problems get harder, manifesting new abilities even at trivial-by-global-standards-scale. The deep learning revolution has begun as foretold.},
  langid = {american},
  keywords = {read,scaling law},
  file = {/Users/lukakuma/Zotero/storage/3LTUK22J/Scaling-hypothesis.html}
}

@article{branwenTimingTechnologyLessons2012,
  title = {Timing {{Technology}}: {{Lessons From The Media Lab}}},
  shorttitle = {Timing {{Technology}}},
  author = {Branwen, Gwern},
  date = {2012-07-12},
  url = {https://www.gwern.net/Timing},
  urldate = {2022-06-10},
  abstract = {Technological developments can be foreseen but the knowledge is largely useless because startups are inherently risky and require optimal timing. A more practical approach is to embrace uncertainty, taking a reinforcement learning perspective.},
  langid = {american},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/V7DG2WN5/Timing.html}
}

@inproceedings{brodyHowAttentiveAre2021,
  title = {How {{Attentive}} Are {{Graph Attention Networks}}?},
  author = {Brody, Shaked and Alon, Uri and Yahav, Eran},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=F72ximsx7C1},
  urldate = {2022-04-27},
  abstract = {Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/537V9L24/Brody et al. - 2021 - How Attentive are Graph Attention Networks.pdf;/Users/lukakuma/Zotero/storage/SEL5VA5E/forum.html}
}

@article{brohanRT1ROBOTICSTRANSFORMER,
  title = {{{RT-1}}: {{ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil J and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  date = {2022-12-14},
  langid = {english},
  keywords = {action/physical},
  file = {/Users/lukakuma/Zotero/storage/VT8HC9IZ/Brohan et al. - RT-1 ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL .pdf}
}

@article{bronkhorstLogicalReasoningFormal2019,
  title = {Logical {{Reasoning}} in {{Formal}} and {{Everyday Reasoning Tasks}}},
  author = {Bronkhorst, Hugo and Roorda, G. and Suhre, Cor and Goedhart, Martin},
  date = {2019-12-26},
  journaltitle = {International Journal of Science and Mathematics Education},
  shortjournal = {International Journal of Science and Mathematics Education},
  volume = {18},
  doi = {10.1007/s10763-019-10039-8},
  abstract = {Logical reasoning is of great societal importance and, as stressed by the twenty-first century skills framework, also seen as a key aspect for the development of critical thinking. This study aims at exploring secondary school students' logical reasoning strategies in formal reasoning and everyday reasoning tasks. With task-based interviews among 4 16- and 17-year-old pre-university students, we explored their reasoning strategies and the reasoning difficulties they encounter. In this article, we present results from linear ordering tasks, tasks with invalid syllogisms and a task with implicit reasoning in a newspaper article. The linear ordering tasks and the tasks with invalid syllogisms are presented formally (with symbols) and non-formally in ordinary language (without symbols). In tasks that were familiar to our students, they used rule-based reasoning strategies and provided correct answers although their initial interpretation differed. In tasks that were unfamiliar to our students, they almost always used informal interpretations and their answers were influenced by their own knowledge. When working on the newspaper article task, the students did not use strong formal schemes, which could have provided a clear overview. At the end of the article, we present a scheme showing which reasoning strategies are used by students in different types of tasks. This scheme might increase teachers' awareness of the variety in reasoning strategies and can guide classroom discourse during courses on logical reasoning. We suggest that using suitable formalisations and visualisations might structure and improve students' reasoning as well.},
  file = {/Users/lukakuma/Zotero/storage/I5GEY3KJ/Bronkhorst et al. - 2019 - Logical Reasoning in Formal and Everyday Reasoning.pdf}
}

@online{bronsteinGeometricFoundationsDeep2021,
  title = {Geometric Foundations of {{Deep Learning}}},
  author = {Bronstein, Michael},
  date = {2021-04-29T15:44:42},
  url = {https://towardsdatascience.com/geometric-foundations-of-deep-learning-94cdd45b451d},
  urldate = {2021-04-30},
  abstract = {Geometric Deep Learning is an attempt to unify a broad class of ML problems from the perspectives of symmetry and invariance.},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/lukakuma/Zotero/storage/CCVH5PXE/geometric-foundations-of-deep-learning-94cdd45b451d.html}
}

@online{bronsteinGraphNeuralNetworks2022,
  title = {Graph {{Neural Networks}} beyond {{Weisfeiler-Lehman}} and Vanilla {{Message Passing}}},
  author = {Bronstein, Michael},
  date = {2022-03-03T06:13:54},
  url = {https://towardsdatascience.com/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a},
  urldate = {2022-03-23},
  abstract = {Physics-inspired continuous learning models on graphs allow to overcome the limitations of traditional GNNs},
  langid = {english},
  organization = {{Medium}},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/46YW3EY5/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a.html}
}

@online{bronsteinLatentGraphNeural2022,
  title = {Latent Graph Neural Networks: {{Manifold}} Learning 2.0?},
  shorttitle = {Latent Graph Neural Networks},
  author = {Bronstein, Michael},
  date = {2022-01-02T11:11:08},
  url = {https://towardsdatascience.com/manifold-learning-2-99a25eeb677d},
  urldate = {2022-03-24},
  abstract = {Can we use graph neural networks when the graph is unknown?},
  langid = {english},
  organization = {{Medium}},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/UQ73JCDL/manifold-learning-2-99a25eeb677d.html}
}

@online{bronsteinPredictionsHopesGeometric2022,
  title = {Predictions and Hopes for {{Geometric}} \& {{Graph ML}} in 2022},
  author = {Bronstein, Michael},
  date = {2022-02-20T12:49:52},
  url = {https://towardsdatascience.com/predictions-and-hopes-for-geometric-graph-ml-in-2022-aa3b8b79f5cc},
  urldate = {2022-03-23},
  abstract = {Leading researchers in Geometric \& Graph ML summarise the progress in 2021 and make predictions for 2022},
  langid = {english},
  organization = {{Medium}},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/8DDFX2RY/predictions-and-hopes-for-geometric-graph-ml-in-2022-aa3b8b79f5cc.html;/Users/lukakuma/Zotero/storage/BKCLQF6G/predictions-and-hopes-for-geometric-graph-ml-in-2022-aa3b8b79f5cc.html}
}

@article{broockmanManifoldEffectsPartisan2022,
  title = {The Manifold Effects of Partisan Media on Viewers' Beliefs and Attitudes: {{A}} Field Experiment with {{Fox News}} Viewers},
  shorttitle = {The Manifold Effects of Partisan Media on Viewers' Beliefs and Attitudes},
  author = {Broockman, David and Kalla, Joshua},
  date = {2022-04-01T22:08:01},
  publisher = {{OSF Preprints}},
  url = {https://osf.io/jrw26/},
  urldate = {2022-04-04},
  abstract = {Partisan media impacts voting behavior, yet what changes in viewers' beliefs or attitudes may underlie these impacts is poorly understood. We recruited a sample of regular Fox News viewers using data on actual TV viewership from a media company, and incentivized them to watch CNN instead for a month using real-time viewership quizzes. Despite regular Fox viewers being largely strong partisans, we found manifold effects of changing the slant of their media diets on their factual beliefs, attitudes, perceptions of issues' importance, and overall political views. We show that these effects stem in part from a bias we call partisan coverage filtering, wherein partisan outlets selectively report information, leading viewers to learn a biased set of facts. Consistent with this, treated participants concluded that Fox concealed negative information about President Trump. Partisan media does not only present its side an electoral advantage\textemdash it may present a challenge for democratic accountability.},
  langid = {american},
  file = {/Users/lukakuma/Zotero/storage/WY6LJLI3/Broockman and Kalla - 2022 - The manifold effects of partisan media on viewers‚Äô.pdf}
}

@online{brooksInstructPix2PixLearningFollow2023,
  title = {{{InstructPix2Pix}}: {{Learning}} to {{Follow Image Editing Instructions}}},
  shorttitle = {{{InstructPix2Pix}}},
  author = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
  date = {2023-01-18},
  number = {arXiv:2211.09800},
  eprint = {arXiv:2211.09800},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.09800},
  urldate = {2023-02-20},
  abstract = {We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.},
  pubstate = {preprint},
  keywords = {image generation},
  file = {/Users/lukakuma/Zotero/storage/F6ZD3ZK4/Brooks et al. - 2023 - InstructPix2Pix Learning to Follow Image Editing .pdf;/Users/lukakuma/Zotero/storage/R4ELBGBB/2211.html}
}

@article{browningAILimitsLanguage2022a,
  title = {{{AI And The Limits Of Language}}},
  author = {Browning, Jacob},
  date = {2022-08-23},
  url = {https://www.noemamag.com/ai-and-the-limits-of-language},
  urldate = {2022-11-17},
  abstract = {An artificial intelligence system trained on words and sentences alone will never approximate human understanding.},
  langid = {american},
  file = {/Users/lukakuma/Zotero/storage/CBYUAFIP/ai-and-the-limits-of-language.html}
}

@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  number = {arXiv:2005.14165},
  eprint = {arXiv:2005.14165},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2022-07-22},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {preprint},
  keywords = {OpenAI,read},
  file = {/Users/lukakuma/Zotero/storage/GR6HZMTQ/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/Users/lukakuma/Zotero/storage/3MUFK9SG/2005.html}
}

@online{burdaLargeScaleStudyCuriosityDriven2018,
  title = {Large-{{Scale Study}} of {{Curiosity-Driven Learning}}},
  author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
  date = {2018-08-13},
  number = {arXiv:1808.04355},
  eprint = {arXiv:1808.04355},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1808.04355},
  urldate = {2022-07-23},
  abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/TS9IZ7HI/Burda et al. - 2018 - Large-Scale Study of Curiosity-Driven Learning.pdf;/Users/lukakuma/Zotero/storage/2ESE3XTH/1808.html}
}

@online{burjaLiveDeadPlayers2020,
  title = {Live versus {{Dead Players}}},
  author = {Burja, Samo},
  date = {2020-10-30T14:20:49},
  url = {https://medium.com/@samo.burja/live-versus-dead-players-2b24f6e9eae2},
  urldate = {2022-03-15},
  abstract = {This is an excerpt from the draft of my upcoming book on great founder theory. It was originally published on SamoBurja.com. You can access\ldots},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/lukakuma/Zotero/storage/W9VQV8IZ/live-versus-dead-players-2b24f6e9eae2.html}
}

@report{buterinLiberalRadicalismFlexible2018,
  type = {SSRN Scholarly Paper},
  title = {Liberal {{Radicalism}}: {{A Flexible Design For Philanthropic Matching Funds}}},
  shorttitle = {Liberal {{Radicalism}}},
  author = {Buterin, Vitalik and Hitzig, Zo\"e and Weyl, E. Glen},
  date = {2018-12-01},
  number = {ID 3243656},
  institution = {{Social Science Research Network}},
  location = {{Rochester, NY}},
  url = {https://papers.ssrn.com/abstract=3243656},
  urldate = {2022-03-16},
  abstract = {We propose a design for philanthropic or publicly-funded seeding to allow (near) optimal provision of a decentralized, self-organizing ecosystem of public goods.  The concept extends ideas from Quadratic Voting to a funding mechanism for endogenous community formation.  Citizens make public goods contributions to projects of value to them.  The amount received by the project is (proportional to) the square of the sum of the square roots of contributions received.  Under the "standard model" this yields first best public goods provision. Variations can limit the cost, help protect against collusion and aid coordination. We discuss applications to campaign finance, open source software ecosystems, news media finance and urban public projects. More broadly, we relate our mechanism to political theory, discussing how this solution to the public goods problem may furnish neutral and non-authoritarian rules for society that nonetheless support collective organization.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/VFWDBUJ3/Buterin et al. - 2018 - Liberal Radicalism A Flexible Design For Philanth.pdf;/Users/lukakuma/Zotero/storage/W32EGCW4/papers.html}
}

@article{byrgeDevelopmentalProcessEmerges2014,
  title = {Developmental Process Emerges from Extended Brain-Body-Behavior Networks},
  author = {Byrge, Lisa and Sporns, Olaf and Smith, Linda B.},
  date = {2014-08},
  journaltitle = {Trends in cognitive sciences},
  shortjournal = {Trends Cogn Sci},
  volume = {18},
  number = {8},
  eprint = {24862251},
  eprinttype = {pmid},
  pages = {395--403},
  issn = {1364-6613},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4112155/},
  urldate = {2022-07-26},
  abstract = {Studies of brain connectivity have focused on two modes of networks: structural networks describing neuroanatomy and the intrinsic and evoked dependencies of functional networks at rest and during tasks. Each mode constrains and shapes the other across multiple time scales, and each also shows age-related changes. Here we argue that understanding how brains change across development requires understanding the interplay between behavior and brain networks: changing bodies and activities modify the statistics of inputs to the brain; these changing inputs mold brain networks; these networks, in turn, promote further change in behavior and input.},
  pmcid = {PMC4112155},
  file = {/Users/lukakuma/Zotero/storage/IC3S3CTZ/Byrge et al. - 2014 - Developmental process emerges from extended brain-.pdf}
}

@online{caballeroBrokenNeuralScaling2022,
  title = {Broken {{Neural Scaling Laws}}},
  author = {Caballero, Ethan and Gupta, Kshitij and Rish, Irina and Krueger, David},
  date = {2022-11-01},
  number = {arXiv:2210.14891},
  eprint = {arXiv:2210.14891},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.14891},
  urldate = {2022-11-04},
  abstract = {We present a smoothly broken power law functional form that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for each task within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision and unsupervised language tasks, diffusion generative modeling of images, arithmetic, and reinforcement learning. When compared to other functional forms for neural scaling behavior, this functional form yields extrapolations of scaling behavior that are considerably more accurate (root mean squared log error of its extrapolations are 0.86 times that of previous state-of-the-art on average) on this set. Moreover, this functional form accurately models and extrapolates scaling behavior that other functional forms are incapable of expressing such as the non-monotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Code is available at https://github.com/ethancaballero/broken\_neural\_scaling\_laws},
  pubstate = {preprint},
  keywords = {read,scaling law},
  file = {/Users/lukakuma/Zotero/storage/EHCXXH87/Caballero et al. - 2022 - Broken Neural Scaling Laws.pdf;/Users/lukakuma/Zotero/storage/4TLZNXEH/2210.html}
}

@article{caiTensorOptExploringTradeoffs2022,
  title = {{{TensorOpt}}: {{Exploring}} the {{Tradeoffs}} in {{Distributed DNN Training}} with {{Auto-Parallelism}}},
  shorttitle = {{{TensorOpt}}},
  author = {Cai, Zhenkun and Ma, Kaihao and Yan, Xiao and Wu, Yidi and Huang, Yuzhen and Cheng, James and Su, Teng and Yu, Fan},
  date = {2022-08-01},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  volume = {33},
  number = {8},
  eprint = {2004.10856},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {1967--1981},
  issn = {1045-9219, 1558-2183, 2161-9883},
  url = {http://arxiv.org/abs/2004.10856},
  urldate = {2022-10-11},
  abstract = {A good parallelization strategy can significantly improve the efficiency or reduce the cost for the distributed training of deep neural networks (DNNs). Recently, several methods have been proposed to find efficient parallelization strategies but they all optimize a single objective (e.g., execution time, memory consumption) and produce only one strategy. We propose FT, an efficient algorithm that searches for an optimal set of parallelization strategies to allow the trade-off among different objectives. FT can adapt to different scenarios by minimizing the memory consumption when the number of devices is limited and fully utilize additional resources to reduce the execution time. For popular DNN models (e.g., vision, language), an in-depth analysis is conducted to understand the trade-offs among different objectives and their influence on the parallelization strategies. We also develop a user-friendly system, called TensorOpt, which allows users to run their distributed DNN training jobs without caring the details of parallelization strategies. Experimental results show that FT runs efficiently and provides accurate estimation of runtime costs, and TensorOpt is more flexible in adapting to resource availability compared with existing frameworks.},
  keywords = {distributed training},
  file = {/Users/lukakuma/Zotero/storage/YGY7E756/Cai et al. - 2022 - TensorOpt Exploring the Tradeoffs in Distributed .pdf;/Users/lukakuma/Zotero/storage/WL4VM8ZD/2004.html}
}

@article{callawayWhatNextAlphaFold2022,
  title = {What's next for {{AlphaFold}} and the {{AI}} Protein-Folding Revolution},
  author = {Callaway, Ewen},
  date = {2022-04-13},
  journaltitle = {Nature},
  volume = {604},
  number = {7905},
  pages = {234--238},
  publisher = {{Nature Publishing Group}},
  url = {https://www.nature.com/articles/d41586-022-00997-5},
  urldate = {2022-05-19},
  abstract = {DeepMind software that can predict the 3D shape of proteins is already changing biology.},
  issue = {7905},
  langid = {english},
  keywords = {DeepMind,read},
  annotation = {Bandiera\_abtest: a Cg\_type: News Feature Subject\_term: Structural biology, Computational biology and bioinformatics, Biological techniques, Drug discovery},
  file = {/Users/lukakuma/Zotero/storage/ZYUH7MR8/Callaway - 2022 - What's next for AlphaFold and the AI protein-foldi.pdf;/Users/lukakuma/Zotero/storage/7UNDFBBJ/d41586-022-00997-5.html}
}

@book{cangelosiDevelopmentalRoboticsBabies2015,
  title = {Developmental {{Robotics}}: {{From Babies}} to {{Robots}}},
  shorttitle = {Developmental {{Robotics}}},
  author = {Cangelosi, Angelo and Schlesinger, Matthew},
  date = {2015-01-09},
  series = {Intelligent {{Robotics}} and {{Autonomous Agents}} Series},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  abstract = {A comprehensive overview of an interdisciplinary approach to robotics that takes direct inspiration from the developmental and learning phenomena observed in children's cognitive development.},
  editorb = {Arkin, Ronald C.},
  editorbtype = {redactor},
  isbn = {978-0-262-02801-1},
  langid = {english},
  pagetotal = {432},
  file = {/Users/lukakuma/Zotero/storage/8W3RVUJL/Cangelosi and Schlesinger - 2015 - Developmental Robotics From Babies to Robots.pdf}
}

@article{carbonellSilicoDesignAutomated2020,
  title = {In Silico Design and Automated Learning to Boost Next-Generation Smart Biomanufacturing},
  author = {Carbonell, Pablo and Le Feuvre, Rosalind and Takano, Eriko and Scrutton, Nigel S},
  date = {2020-01-01},
  journaltitle = {Synthetic Biology},
  shortjournal = {Synthetic Biology},
  volume = {5},
  number = {1},
  pages = {ysaa020},
  issn = {2397-7000},
  url = {https://doi.org/10.1093/synbio/ysaa020},
  urldate = {2022-05-25},
  abstract = {The increasing demand for bio-based compounds produced from waste or sustainable sources is driving biofoundries to deliver a new generation of prototyping biomanufacturing platforms. Integration and automation of the design, build, test and learn (DBTL) steps in centers like SYNBIOCHEM in Manchester and across the globe (Global Biofoundries Alliance) are helping to reduce the delivery time from initial strain screening and prototyping towards industrial production. Notably, a portfolio of producer strains for a suite of material monomers was recently developed, some approaching industrial titers, in a tour de force by the Manchester Centre that was achieved in less than 90\,days. New in silico design tools are providing significant contributions to the front end of the DBTL pipelines. At the same time, the far-reaching initiatives of modern biofoundries are generating a large amount of high-dimensional data and knowledge that can be integrated through automated learning to expedite the DBTL cycle. In this Perspective, the new design tools and the role of the learning component as an enabling technology for the next generation of automated biofoundries are discussed. Future biofoundries will operate under completely automated DBTL cycles driven by in silico optimal experimental planning, full biomanufacturing devices connectivity, virtualization platforms and cloud-based design. The automated generation of robotic build worklists and the integration of machine-learning algorithms will collectively allow high levels of adaptability and rapid design changes toward fully automated smart biomanufacturing.},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/QNRUN3SG/Carbonell et al. - 2020 - In silico design and automated learning to boost n.pdf;/Users/lukakuma/Zotero/storage/6Y2VHWG5/5926977.html}
}

@article{carionEndtoEndObjectDetection2020,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  date = {2020-05-26},
  url = {https://arxiv.org/abs/2005.12872v3},
  urldate = {2022-08-08},
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  langid = {english},
  keywords = {(ext) Flamingo},
  file = {/Users/lukakuma/Zotero/storage/TTRKKBSI/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf;/Users/lukakuma/Zotero/storage/EL7RI4CX/2005.html}
}

@online{carliniExtractingTrainingData2021,
  title = {Extracting {{Training Data}} from {{Large Language Models}}},
  author = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin},
  date = {2021-06-15},
  number = {arXiv:2012.07805},
  eprint = {arXiv:2012.07805},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.07805},
  urldate = {2022-11-02},
  abstract = {It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/86K7B6VT/Carlini et al. - 2021 - Extracting Training Data from Large Language Model.pdf}
}

@online{carliniQuantifyingMemorizationNeural2022,
  title = {Quantifying {{Memorization Across Neural Language Models}}},
  author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  date = {2022-02-24},
  number = {arXiv:2202.07646},
  eprint = {arXiv:2202.07646},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.07646},
  urldate = {2022-11-02},
  abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others).},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/3P7GA767/Carlini et al. - 2022 - Quantifying Memorization Across Neural Language Mo.pdf}
}

@online{caronEmergingPropertiesSelfSupervised2021,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J\'egou, Herv\'e and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  date = {2021-05-24},
  number = {arXiv:2104.14294},
  eprint = {arXiv:2104.14294},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.14294},
  urldate = {2022-07-30},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/5FKXNSZG/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf;/Users/lukakuma/Zotero/storage/LU426JVH/2104.html}
}

@article{caronUnsupervisedLearningVisual2020,
  title = {Unsupervised {{Learning}} of {{Visual Features}} by {{Contrasting Cluster Assignments}}},
  author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  date = {2020-06-17},
  url = {https://arxiv.org/abs/2006.09882v5},
  urldate = {2022-04-25},
  abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/FGL7QY35/Caron et al. - 2020 - Unsupervised Learning of Visual Features by Contra.pdf;/Users/lukakuma/Zotero/storage/CZAX9394/2006.html}
}

@online{carreiraHierarchicalPerceiver2022,
  title = {Hierarchical {{Perceiver}}},
  author = {Carreira, Joao and Koppula, Skanda and Zoran, Daniel and Recasens, Adria and Ionescu, Catalin and Henaff, Olivier and Shelhamer, Evan and Arandjelovic, Relja and Botvinick, Matt and Vinyals, Oriol and Simonyan, Karen and Zisserman, Andrew and Jaegle, Andrew},
  date = {2022-02-22},
  number = {arXiv:2202.10890},
  eprint = {arXiv:2202.10890},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.10890},
  urldate = {2022-08-02},
  abstract = {General perception systems such as Perceivers can process arbitrary modalities in any combination and are able to handle up to a few hundred thousand inputs. They achieve this generality by exclusively using global attention operations. This however hinders them from scaling up to the inputs sizes required to process raw high-resolution images or video. In this paper, we show that some degree of locality can be introduced back into these models, greatly improving their efficiency while preserving their generality. To scale them further, we introduce a self-supervised approach that enables learning dense low-dimensional positional embeddings for very large signals. We call the resulting model a Hierarchical Perceiver (HiP). HiP retains the ability to process arbitrary modalities, but now at higher-resolution and without any specialized preprocessing, improving over flat Perceivers in both efficiency and accuracy on the ImageNet, Audioset and PASCAL VOC datasets.},
  pubstate = {preprint},
  keywords = {perceiver,read},
  file = {/Users/lukakuma/Zotero/storage/5CSPZVPH/Carreira et al. - 2022 - Hierarchical Perceiver.pdf;/Users/lukakuma/Zotero/storage/KFI5BVY3/2202.html}
}

@online{cartaGroundingLargeLanguage2023,
  title = {Grounding {{Large Language Models}} in {{Interactive Environments}} with {{Online Reinforcement Learning}}},
  author = {Carta, Thomas and Romac, Cl\'ement and Wolf, Thomas and Lamprier, Sylvain and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  date = {2023-02-06},
  number = {arXiv:2302.02662},
  eprint = {arXiv:2302.02662},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.02662},
  urldate = {2023-02-20},
  abstract = {Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5.},
  pubstate = {preprint},
  keywords = {action/digital},
  file = {/Users/lukakuma/Zotero/storage/3KRNRGGE/Carta et al. - 2023 - Grounding Large Language Models in Interactive Env.pdf;/Users/lukakuma/Zotero/storage/4XGN36A8/2302.html}
}

@article{carvalhoInteroceptionOriginFeelings2021,
  title = {Interoception and the Origin of Feelings: {{A}} New Synthesis},
  shorttitle = {Interoception and the Origin of Feelings},
  author = {Carvalho, Gil B. and Damasio, Antonio},
  date = {2021},
  journaltitle = {BioEssays},
  volume = {43},
  number = {6},
  pages = {2000261},
  issn = {1521-1878},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bies.202000261},
  urldate = {2022-05-10},
  abstract = {Feelings are conscious mental events that represent body states as they undergo homeostatic regulation. Feelings depend on the interoceptive nervous system (INS), a collection of peripheral and central pathways, nuclei and cortical regions which continuously sense chemical and anatomical changes in the organism. How such humoral and neural signals come to generate conscious mental states has been a major scientific question. The answer proposed here invokes (1) several distinctive and poorly known physiological features of the INS; and (2) a unique interaction between the body (the `object' of interoception) and the central nervous system (which generates the 'subject' of interoception). The atypical traits of the INS and the direct interactions between neural and non-neural physiological compartments of the organism, neither of which is present in exteroceptive systems, plausibly explain the qualitative and subjective aspects of feelings, thus accounting for their conscious nature.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/U4Q2FMFN/bies.html}
}

@inproceedings{chaabouniEmergentCommunicationScale2022,
  title = {Emergent {{Communication}} at {{Scale}}},
  author = {Chaabouni, Rahma and Strub, Florian and Altch\'e, Florent and Tarassov, Eugene and Tallec, Corentin and Davoodi, Elnaz and Mathewson, Kory Wallace and Tieleman, Olivier and Lazaridou, Angeliki and Piot, Bilal},
  date = {2022-01-28},
  url = {https://openreview.net/forum?id=AUGBfDIV9rL},
  urldate = {2023-02-20},
  abstract = {Emergent communication aims for a better understanding of human language evolution and building more efficient representations. We posit that reaching these goals will require scaling up, in contrast to a significant amount of literature that focuses on setting up small-scale problems to tease out desired properties of the emergent languages. We focus on three independent aspects to scale up, namely the dataset, task complexity, and population size. We provide a first set of results for large populations solving complex tasks on realistic large-scale datasets, as well as an easy-to-use codebase to enable further experimentation. In more complex tasks and datasets, we find that RL training can become unstable, but responds well to established stabilization techniques. We also identify the need for a different metric than topographic similarity, which does not correlate with the generalization performances when working with natural images. In this context, we probe ease-of-learnability and transfer methods to assess emergent languages. Finally, we observe that larger populations do not induce robust emergent protocols with high generalization performance, leading us to explore different ways to leverage population, through voting and imitation learning.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {multiagent},
  file = {/Users/lukakuma/Zotero/storage/UNWF8S7W/Chaabouni et al. - 2022 - Emergent Communication at Scale.pdf}
}

@unpublished{chanDataDistributionalProperties2022,
  title = {Data {{Distributional Properties Drive Emergent Few-Shot Learning}} in {{Transformers}}},
  author = {Chan, Stephanie C. Y. and Santoro, Adam and Lampinen, Andrew K. and Wang, Jane X. and Singh, Aaditya and Richemond, Pierre H. and McClelland, Jay and Hill, Felix},
  date = {2022-05-12},
  eprint = {2205.05055},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.05055},
  urldate = {2022-05-19},
  abstract = {Large transformer-based language models are able to perform few-shot learning (also known as in-context learning), without having been explicitly trained for it. We hypothesized that specific distributional properties of natural language might drive this emergent phenomenon, as these characteristics might lead to a kind of interpolation between few-shot meta-training (designed to elicit rapid few-shot learning) and standard supervised training (designed to elicit gradual in-weights learning). We also hypothesized that these distributional properties could lead to emergent few-shot learning in domains outside of language. Inspired by this idea, we ran a series of experiments on a standard image-based few-shot dataset. We discovered that a number of data properties did indeed promote the emergence of few-shot learning in transformer models. All of these properties are present in natural language -- burstiness, long-tailedness, and many-to-one or one-to-many label mappings. The data influenced whether models were biased towards either few-shot learning vs. memorizing information in their weights; models could generally perform well at only one or the other. However, we discovered that an additional distributional property could allow the two capabilities to co-exist in the same model -- a skewed, Zipfian distribution over classes -- which occurs in language as well. Notably, training data that could elicit few-shot learning in transformers were unable to elicit few-shot learning in recurrent models. In sum, we find that few-shot learning emerges only from applying the right architecture to the right data distribution; neither component is sufficient on its own.},
  keywords = {emergence},
  file = {/Users/lukakuma/Zotero/storage/47J2LM8X/Chan et al. - 2022 - Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers.pdf}
}

@online{chanDataDistributionalProperties2022b,
  title = {Data {{Distributional Properties Drive Emergent In-Context Learning}} in {{Transformers}}},
  author = {Chan, Stephanie C. Y. and Santoro, Adam and Lampinen, Andrew K. and Wang, Jane X. and Singh, Aaditya and Richemond, Pierre H. and McClelland, Jay and Hill, Felix},
  date = {2022-10-10},
  number = {arXiv:2205.05055},
  eprint = {arXiv:2205.05055},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.05055},
  urldate = {2022-11-02},
  abstract = {Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.},
  langid = {english},
  pubstate = {preprint},
  keywords = {emergence},
  file = {/Users/lukakuma/Zotero/storage/T8TSC53M/Chan et al. - 2022 - Data Distributional Properties Drive Emergent In-C.pdf;/Users/lukakuma/Zotero/storage/WF9B2MH9/2205.html}
}

@online{chandelTrainingEvaluatingJupyter2022,
  title = {Training and {{Evaluating}} a {{Jupyter Notebook Data Science Assistant}}},
  author = {Chandel, Shubham and Clement, Colin B. and Serrato, Guillermo and Sundaresan, Neel},
  date = {2022-01-30},
  number = {arXiv:2201.12901},
  eprint = {arXiv:2201.12901},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.12901},
  urldate = {2022-11-23},
  abstract = {We study the feasibility of a Data Science assistant powered by a sequence-to-sequence transformer by training a new model JuPyT5 on all publicly available Jupyter Notebook GitHub repositories and developing a new metric: Data Science Problems (DSP). DSP is a collection of 1119 problems curated from 306 pedagogical notebooks with 92 dataset dependencies, natural language and Markdown problem descriptions, and assert-based unit tests. These notebooks were designed to test university students' mastery of various Python implementations of Math and Data Science, and we now leverage them to study the ability of JuPyT5 to understand and pass the tests. We analyze the content of DSP, validate its quality, and we find that given 100 sampling attempts JuPyT5 is able to solve 77.5\textbackslash\% of the DSP problems. We further present various ablation and statistical analyses and compare DSP to other recent natural language to code benchmarks.},
  pubstate = {preprint},
  keywords = {code},
  file = {/Users/lukakuma/Zotero/storage/R4QMJGZB/Chandel et al. - 2022 - Training and Evaluating a Jupyter Notebook Data Sc.pdf;/Users/lukakuma/Zotero/storage/WPYWUSU3/2201.html}
}

@online{changMuseTextToImageGeneration2023,
  title = {Muse: {{Text-To-Image Generation}} via {{Masked Generative Transformers}}},
  shorttitle = {Muse},
  author = {Chang, Huiwen and Zhang, Han and Barber, Jarred and Maschinot, A. J. and Lezama, Jose and Jiang, Lu and Yang, Ming-Hsuan and Murphy, Kevin and Freeman, William T. and Rubinstein, Michael and Li, Yuanzhen and Krishnan, Dilip},
  date = {2023-01-02},
  number = {arXiv:2301.00704},
  eprint = {arXiv:2301.00704},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.00704},
  urldate = {2023-01-13},
  abstract = {We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io},
  pubstate = {preprint},
  keywords = {image generation},
  file = {/Users/lukakuma/Zotero/storage/GDBFWH6T/Chang et al. - 2023 - Muse Text-To-Image Generation via Masked Generati.pdf;/Users/lukakuma/Zotero/storage/WHHIPY39/2301.html}
}

@unpublished{changpinyoConceptual12MPushing2021,
  title = {Conceptual {{12M}}: {{Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts}}},
  shorttitle = {Conceptual {{12M}}},
  author = {Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  date = {2021-03-30},
  eprint = {2102.08981},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.08981},
  urldate = {2022-04-29},
  abstract = {The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pre-training. However, these datasets are often collected with overrestrictive requirements inherited from their original target tasks (e.g., image caption generation), which limit the resulting dataset scale and diversity. We take a step further in pushing the limits of vision-and-language pre-training data by relaxing the data collection pipeline used in Conceptual Captions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M (CC12M), a dataset with 12 million image-text pairs specifically meant to be used for vision-and-language pre-training. We perform an analysis of this dataset and benchmark its effectiveness against CC3M on multiple downstream tasks with an emphasis on long-tail visual recognition. Our results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.},
  file = {/Users/lukakuma/Zotero/storage/RDNRTDIG/Changpinyo et al. - 2021 - Conceptual 12M Pushing Web-Scale Image-Text Pre-T.pdf;/Users/lukakuma/Zotero/storage/DDJV9U76/2102.html}
}

@article{chanussotOpenCatalyst20202021,
  title = {The {{Open Catalyst}} 2020 ({{OC20}}) {{Dataset}} and {{Community Challenges}}},
  author = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
  date = {2021-05-21},
  journaltitle = {ACS Catalysis},
  shortjournal = {ACS Catal.},
  volume = {11},
  number = {10},
  eprint = {2010.09990},
  eprinttype = {arxiv},
  pages = {6059--6072},
  issn = {2155-5435, 2155-5435},
  url = {http://arxiv.org/abs/2010.09990},
  urldate = {2022-04-29},
  abstract = {Catalyst discovery and optimization is key to solving many societal and energy challenges including solar fuels synthesis, long-term energy storage, and renewable fertilizer production. Despite considerable effort by the catalysis community to apply machine learning models to the computational catalyst discovery process, it remains an open challenge to build models that can generalize across both elemental compositions of surfaces and adsorbate identity/configurations, perhaps because datasets have been smaller in catalysis than related fields. To address this we developed the OC20 dataset, consisting of 1,281,040 Density Functional Theory (DFT) relaxations (\textasciitilde 264,890,000 single point evaluations) across a wide swath of materials, surfaces, and adsorbates (nitrogen, carbon, and oxygen chemistries). We supplemented this dataset with randomly perturbed structures, short timescale molecular dynamics, and electronic structure analyses. The dataset comprises three central tasks indicative of day-to-day catalyst modeling and comes with pre-defined train/validation/test splits to facilitate direct comparisons with future model development efforts. We applied three state-of-the-art graph neural network models (CGCNN, SchNet, Dimenet++) to each of these tasks as baseline demonstrations for the community to build on. In almost every task, no upper limit on model size was identified, suggesting that even larger models are likely to improve on initial results. The dataset and baseline models are both provided as open resources, as well as a public leader board to encourage community contributions to solve these important tasks.},
  file = {/Users/lukakuma/Zotero/storage/I9CJGNIG/Chanussot et al. - 2021 - The Open Catalyst 2020 (OC20) Dataset and Communit.pdf;/Users/lukakuma/Zotero/storage/34FEAEFG/2010.html}
}

@online{chenAcceleratingLargeLanguage2023,
  title = {Accelerating {{Large Language Model Decoding}} with {{Speculative Sampling}}},
  author = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  date = {2023-02-02},
  number = {arXiv:2302.01318},
  eprint = {arXiv:2302.01318},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.01318},
  urldate = {2023-02-07},
  abstract = {We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2-2.5x decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.},
  pubstate = {preprint},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/2EKL7XIS/Chen et al. - 2023 - Accelerating Large Language Model Decoding with Sp.pdf;/Users/lukakuma/Zotero/storage/GCGJ3NPX/2302.html}
}

@online{chenAltCLIPAlteringLanguage2022,
  title = {{{AltCLIP}}: {{Altering}} the {{Language Encoder}} in {{CLIP}} for {{Extended Language Capabilities}}},
  shorttitle = {{{AltCLIP}}},
  author = {Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell},
  date = {2022-11-12},
  number = {arXiv:2211.06679},
  eprint = {2211.06679},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.06679},
  urldate = {2022-11-19},
  abstract = {In this work, we present a conceptually simple and effective method to train a strong bilingual multimodal representation model. Starting from the pretrained multimodal representation model CLIP released by OpenAI, we switched its text encoder with a pretrained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at https://github.com/FlagAI-Open/FlagAI.},
  pubstate = {preprint},
  version = {1},
  file = {/Users/lukakuma/Zotero/storage/D7SKAT9G/Chen et al. - 2022 - AltCLIP Altering the Language Encoder in CLIP for Extended Language Capabilities.pdf}
}

@unpublished{chenDecisionTransformerReinforcement2021,
  title = {Decision {{Transformer}}: {{Reinforcement Learning}} via {{Sequence Modeling}}},
  shorttitle = {Decision {{Transformer}}},
  author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  date = {2021-06-24},
  number = {arXiv:2106.01345},
  eprint = {2106.01345},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2106.01345},
  urldate = {2022-05-13},
  abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  keywords = {(ext) Gato,read},
  file = {/Users/lukakuma/Zotero/storage/YM9594JS/Chen et al. - 2021 - Decision Transformer Reinforcement Learning via S.pdf;/Users/lukakuma/Zotero/storage/9FBX3KSQ/2106.html}
}

@online{chenEvaluatingLargeLanguage2021,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  date = {2021-07-14},
  number = {arXiv:2107.03374},
  eprint = {arXiv:2107.03374},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.03374},
  urldate = {2022-10-31},
  abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  pubstate = {preprint},
  keywords = {action/physical,code,read},
  file = {/Users/lukakuma/Zotero/storage/3PZFB6MQ/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf;/Users/lukakuma/Zotero/storage/W6BGPZVF/2107.html}
}

@online{chenEvoPromptingLanguageModels2023,
  title = {{{EvoPrompting}}: {{Language Models}} for {{Code-Level Neural Architecture Search}}},
  shorttitle = {{{EvoPrompting}}},
  author = {Chen, Angelica and Dohan, David M. and So, David R.},
  date = {2023-02-28},
  number = {arXiv:2302.14838},
  eprint = {arXiv:2302.14838},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.14838},
  urldate = {2023-03-03},
  abstract = {Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/69X698YF/Chen et al. - 2023 - EvoPrompting Language Models for Code-Level Neura.pdf;/Users/lukakuma/Zotero/storage/LHXN9VW8/2302.html}
}

@unpublished{chenExploringSimpleSiamese2020,
  title = {Exploring {{Simple Siamese Representation Learning}}},
  author = {Chen, Xinlei and He, Kaiming},
  date = {2020-11-20},
  eprint = {2011.10566},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.10566},
  urldate = {2022-04-25},
  abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
  file = {/Users/lukakuma/Zotero/storage/KAAJRE4C/Chen and He - 2020 - Exploring Simple Siamese Representation Learning.pdf;/Users/lukakuma/Zotero/storage/NQQ73KAS/2011.html}
}

@online{chengBindingLanguageModels2022,
  title = {Binding {{Language Models}} in {{Symbolic Languages}}},
  author = {Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A. and Yu, Tao},
  date = {2022-10-06},
  number = {arXiv:2210.02875},
  eprint = {arXiv:2210.02875},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.02875},
  urldate = {2022-11-23},
  abstract = {Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose BINDER, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few incontext exemplars, Codex is able to identify the part of the task input that cannot be answered by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. BINDER achieves state-of-the-art results on WIKITABLEQUESTIONS and TABFACT datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while BINDER only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/hkunlp/binder.},
  langid = {english},
  pubstate = {preprint},
  keywords = {code},
  file = {/Users/lukakuma/Zotero/storage/ANGNYHXW/Cheng et al. - 2022 - Binding Language Models in Symbolic Languages.pdf}
}

@article{chengEkidenPlatformConfidentialityPreserving2019,
  title = {Ekiden: {{A Platform}} for {{Confidentiality-Preserving}}, {{Trustworthy}}, and {{Performant Smart Contract Execution}}},
  shorttitle = {Ekiden},
  author = {Cheng, Raymond and Zhang, Fan and Kos, Jernej and He, Warren and Hynes, Nicholas and Johnson, Noah and Juels, Ari and Miller, Andrew and Song, Dawn},
  date = {2019-06},
  journaltitle = {2019 IEEE European Symposium on Security and Privacy (EuroS\&P)},
  eprint = {1804.05141},
  eprinttype = {arxiv},
  pages = {185--200},
  url = {http://arxiv.org/abs/1804.05141},
  urldate = {2022-03-09},
  abstract = {Smart contracts are applications that execute on blockchains. Today they manage billions of dollars in value and motivate visionary plans for pervasive blockchain deployment. While smart contracts inherit the availability and other security assurances of blockchains, however, they are impeded by blockchains' lack of confidentiality and poor performance. We present Ekiden, a system that addresses these critical gaps by combining blockchains with Trusted Execution Environments (TEEs). Ekiden leverages a novel architecture that separates consensus from execution, enabling efficient TEE-backed confidentiality-preserving smart-contracts and high scalability. Our prototype (with Tendermint as the consensus layer) achieves example performance of 600x more throughput and 400x less latency at 1000x less cost than the Ethereum mainnet. Another contribution of this paper is that we systematically identify and treat the pitfalls arising from harmonizing TEEs and blockchains. Treated separately, both TEEs and blockchains provide powerful guarantees, but hybridized, though, they engender new attacks. For example, in naive designs, privacy in TEE-backed contracts can be jeopardized by forgery of blocks, a seemingly unrelated attack vector. We believe the insights learned from Ekiden will prove to be of broad importance in hybridized TEE-blockchain systems.},
  file = {/Users/lukakuma/Zotero/storage/SMESIRCK/Cheng et al. - 2019 - Ekiden A Platform for Confidentiality-Preserving,.pdf;/Users/lukakuma/Zotero/storage/62CRCFDF/1804.html}
}

@inproceedings{chenGenerativePretrainingPixels2020,
  title = {Generative {{Pretraining From Pixels}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  date = {2020-11-21},
  pages = {1691--1703},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/chen20s.html},
  urldate = {2022-07-28},
  abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0\% top-1 accuracy on a linear probe of our features.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/E9CEK6DC/Chen et al. - 2020 - Generative Pretraining From Pixels.pdf}
}

@unpublished{chenHardwareConditionedPolicies2019,
  title = {Hardware {{Conditioned Policies}} for {{Multi-Robot Transfer Learning}}},
  author = {Chen, Tao and Murali, Adithyavairavan and Gupta, Abhinav},
  date = {2019-01-12},
  number = {arXiv:1811.09864},
  eprint = {1811.09864},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1811.09864},
  urldate = {2022-05-13},
  abstract = {Deep reinforcement learning could be used to learn dexterous robotic policies but it is challenging to transfer them to new robots with vastly different hardware properties. It is also prohibitively expensive to learn a new policy from scratch for each robot hardware due to the high sample complexity of modern state-of-the-art algorithms. We propose a novel approach called \textbackslash textit\{Hardware Conditioned Policies\} where we train a universal policy conditioned on a vector representation of robot hardware. We considered robots in simulation with varied dynamics, kinematic structure, kinematic lengths and degrees-of-freedom. First, we use the kinematic structure directly as the hardware encoding and show great zero-shot transfer to completely novel robots not seen during training. For robots with lower zero-shot success rate, we also demonstrate that fine-tuning the policy network is significantly more sample-efficient than training a model from scratch. In tasks where knowing the agent dynamics is important for success, we learn an embedding for robot hardware and show that policies conditioned on the encoding of hardware tend to generalize and transfer well. The code and videos are available on the project webpage: https://sites.google.com/view/robot-transfer-hcp.},
  keywords = {(ext) Gato},
  file = {/Users/lukakuma/Zotero/storage/6545LBRQ/Chen et al. - 2019 - Hardware Conditioned Policies for Multi-Robot Tran.pdf;/Users/lukakuma/Zotero/storage/ZMS926QL/1811.html}
}

@inproceedings{chenImprovingInContextFewShot2022,
  title = {Improving {{In-Context Few-Shot Learning}} via {{Self-Supervised Training}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Chen, Mingda and Du, Jingfei and Pasunuru, Ramakanth and Mihaylov, Todor and Iyer, Srini and Stoyanov, Veselin and Kozareva, Zornitsa},
  date = {2022-07},
  pages = {3558--3573},
  publisher = {{Association for Computational Linguistics}},
  location = {{Seattle, United States}},
  url = {https://aclanthology.org/2022.naacl-main.260},
  urldate = {2023-01-05},
  abstract = {Self-supervised pretraining has made few-shot learning possible for many NLP tasks. But the pretraining objectives are not typically adapted specifically for in-context few-shot learning. In this paper, we propose to use self-supervision in an intermediate training stage between pretraining and downstream few-shot usage with the goal to teach the model to perform in-context few shot learning. We propose and evaluate four self-supervised objectives on two benchmarks. We find that the intermediate self-supervision stage produces models that outperform strong baselines. Ablation study shows that several factors affect the downstream performance, such as the amount of training data and the diversity of the self-supervised objectives. Human-annotated cross-task supervision and self-supervision are complementary. Qualitative analysis suggests that the self-supervised-trained models are better at following task requirements.},
  eventtitle = {{{NAACL-HLT}} 2022},
  file = {/Users/lukakuma/Zotero/storage/C4TE737S/Chen et al. - 2022 - Improving In-Context Few-Shot Learning via Self-Su.pdf}
}

@unpublished{chenLearningGeneralizableRobotic2021,
  title = {Learning {{Generalizable Robotic Reward Functions}} from "{{In-The-Wild}}" {{Human Videos}}},
  author = {Chen, Annie S. and Nair, Suraj and Finn, Chelsea},
  date = {2021-03-31},
  number = {arXiv:2103.16817},
  eprint = {2103.16817},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2103.16817},
  urldate = {2022-05-13},
  abstract = {We are motivated by the goal of generalist robots that can complete a wide range of tasks across many environments. Critical to this is the robot's ability to acquire some metric of task success or reward, which is necessary for reinforcement learning, planning, or knowing when to ask for help. For a general-purpose robot operating in the real world, this reward function must also be able to generalize broadly across environments, tasks, and objects, while depending only on on-board sensor observations (e.g. RGB images). While deep learning on large and diverse datasets has shown promise as a path towards such generalization in computer vision and natural language, collecting high quality datasets of robotic interaction at scale remains an open challenge. In contrast, "in-the-wild" videos of humans (e.g. YouTube) contain an extensive collection of people doing interesting tasks across a diverse range of settings. In this work, we propose a simple approach, Domain-agnostic Video Discriminator (DVD), that learns multitask reward functions by training a discriminator to classify whether two videos are performing the same task, and can generalize by virtue of learning from a small amount of robot data with a broad dataset of human videos. We find that by leveraging diverse human datasets, this reward function (a) can generalize zero shot to unseen environments, (b) generalize zero shot to unseen tasks, and (c) can be combined with visual model predictive control to solve robotic manipulation tasks on a real WidowX200 robot in an unseen environment from a single human demo.},
  keywords = {(ext) Gato},
  file = {/Users/lukakuma/Zotero/storage/DF8K65GC/Chen et al. - 2021 - Learning Generalizable Robotic Reward Functions fr.pdf;/Users/lukakuma/Zotero/storage/26EISGQT/2103.html}
}

@unpublished{chenLearningUniversalHyperparameter2022,
  title = {Towards {{Learning Universal Hyperparameter Optimizers}} with {{Transformers}}},
  author = {Chen, Yutian and Song, Xingyou and Lee, Chansoo and Wang, Zi and Zhang, Qiuyi and Dohan, David and Kawakami, Kazuya and Kochanski, Greg and Doucet, Arnaud and Ranzato, Marc'aurelio and Perel, Sagi and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2022-05-26},
  number = {arXiv:2205.13320},
  eprint = {2205.13320},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.13320},
  urldate = {2022-05-27},
  abstract = {Meta-learning hyperparameter optimization (HPO) algorithms from prior experiments is a promising approach to improve optimization efficiency over objective functions from a similar distribution. However, existing methods are restricted to learning from experiments sharing the same set of hyperparameters. In this paper, we introduce the OptFormer, the first text-based Transformer HPO framework that provides a universal end-to-end interface for jointly learning policy and function prediction when trained on vast tuning data from the wild. Our extensive experiments demonstrate that the OptFormer can imitate at least 7 different HPO algorithms, which can be further improved via its function uncertainty estimates. Compared to a Gaussian Process, the OptFormer also learns a robust prior distribution for hyperparameter response functions, and can thereby provide more accurate and better calibrated predictions. This work paves the path to future extensions for training a Transformer-based model as a general HPO optimizer.},
  file = {/Users/lukakuma/Zotero/storage/XDJVK8CZ/Chen et al. - 2022 - Towards Learning Universal Hyperparameter Optimize.pdf;/Users/lukakuma/Zotero/storage/IZNPGU9P/2205.html}
}

@online{chenMuRAGMultimodalRetrievalAugmented2022,
  title = {{{MuRAG}}: {{Multimodal Retrieval-Augmented Generator}} for {{Open Question Answering}} over {{Images}} and {{Text}}},
  shorttitle = {{{MuRAG}}},
  author = {Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and Cohen, William W.},
  date = {2022-10-20},
  number = {arXiv:2210.02928},
  eprint = {arXiv:2210.02928},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.02928},
  urldate = {2022-12-02},
  abstract = {While language Models store a massive amount of world knowledge implicitly in their parameters, even very large models often fail to encode information about rare entities and events, while incurring huge computational costs. Recently, retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated world knowledge into language generation by leveraging an external non-parametric index and have demonstrated impressive performance with constrained model sizes. However, these methods are restricted to retrieving only textual knowledge, neglecting the ubiquitous amount of knowledge in other modalities like images -- much of which contains information not covered by any text. To address this limitation, we propose the first Multimodal Retrieval-Augmented Transformer (MuRAG), which accesses an external non-parametric multimodal memory to augment language generation. MuRAG is pre-trained with a mixture of large-scale image-text and text-only corpora using a joint contrastive and generative loss. We perform experiments on two different datasets that require retrieving and reasoning over both images and text to answer a given query: WebQA, and MultimodalQA. Our results show that MuRAG achieves state-of-the-art accuracy, outperforming existing models by 10-20\textbackslash\% absolute on both datasets and under both distractor and full-wiki settings.},
  pubstate = {preprint},
  keywords = {4-document retrieval,7-multimodal,QA},
  file = {/Users/lukakuma/Zotero/storage/KRIUYNDQ/Chen et al. - 2022 - MuRAG Multimodal Retrieval-Augmented Generator fo.pdf;/Users/lukakuma/Zotero/storage/Y4648JGG/2210.html}
}

@online{chenPaLIJointlyScaledMultilingual2022,
  title = {{{PaLI}}: {{A Jointly-Scaled Multilingual Language-Image Model}}},
  shorttitle = {{{PaLI}}},
  author = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, A. J. and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Ding, Nan and Rong, Keran and Akbari, Hassan and Mishra, Gaurav and Xue, Linting and Thapliyal, Ashish and Bradbury, James and Kuo, Weicheng and Seyedhosseini, Mojtaba and Jia, Chao and Ayan, Burcu Karagol and Riquelme, Carlos and Steiner, Andreas and Angelova, Anelia and Zhai, Xiaohua and Houlsby, Neil and Soricut, Radu},
  date = {2022-09-16},
  number = {arXiv:2209.06794},
  eprint = {arXiv:2209.06794},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.06794},
  urldate = {2022-10-18},
  abstract = {Effective scaling and a flexible task interface enable large language models to excel at many tasks. PaLI (Pathways Language and Image model) extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train the largest ViT to date (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/SRKZTZDJ/Chen et al. - 2022 - PaLI A Jointly-Scaled Multilingual Language-Image.pdf;/Users/lukakuma/Zotero/storage/CTHPMK97/2209.html}
}

@online{chenParameterEfficientFineTuningDesign2023,
  title = {Parameter-{{Efficient Fine-Tuning Design Spaces}}},
  author = {Chen, Jiaao and Zhang, Aston and Shi, Xingjian and Li, Mu and Smola, Alex and Yang, Diyi},
  date = {2023-01-04},
  number = {arXiv:2301.01821},
  eprint = {arXiv:2301.01821},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.01821},
  urldate = {2023-02-16},
  abstract = {Parameter-efficient fine-tuning aims to achieve performance comparable to fine-tuning, using fewer trainable parameters. Several strategies (e.g., Adapters, prefix tuning, BitFit, and LoRA) have been proposed. However, their designs are hand-crafted separately, and it remains unclear whether certain design patterns exist for parameter-efficient fine-tuning. Thus, we present a parameter-efficient fine-tuning design paradigm and discover design patterns that are applicable to different experimental settings. Instead of focusing on designing another individual tuning strategy, we introduce parameter-efficient fine-tuning design spaces that parameterize tuning structures and tuning strategies. Specifically, any design space is characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from an initial design space, we progressively refine the space based on the model quality of each design choice and make greedy selection at each stage over these four components. We discover the following design patterns: (i) group layers in a spindle pattern; (ii) allocate the number of trainable parameters to layers uniformly; (iii) tune all the groups; (iv) assign proper tuning strategies to different groups. These design patterns result in new parameter-efficient fine-tuning methods. We show experimentally that these methods consistently and significantly outperform investigated parameter-efficient fine-tuning strategies across different backbone models and different tasks in natural language processing.},
  pubstate = {preprint},
  keywords = {*peft},
  file = {/Users/lukakuma/Zotero/storage/THKK3LRT/Chen et al. - 2023 - Parameter-Efficient Fine-Tuning Design Spaces.pdf;/Users/lukakuma/Zotero/storage/P3FE7C8G/2301.html}
}

@unpublished{chenPix2seqLanguageModeling2022,
  title = {Pix2seq: {{A Language Modeling Framework}} for {{Object Detection}}},
  shorttitle = {Pix2seq},
  author = {Chen, Ting and Saxena, Saurabh and Li, Lala and Fleet, David J. and Hinton, Geoffrey},
  date = {2022-03-27},
  eprint = {2109.10852},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2109.10852},
  urldate = {2022-04-28},
  abstract = {We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.},
  file = {/Users/lukakuma/Zotero/storage/VMZ5ZG9L/Chen et al. - 2022 - Pix2seq A Language Modeling Framework for Object .pdf;/Users/lukakuma/Zotero/storage/FSSGX2BJ/2109.html}
}

@online{chenProgramThoughtsPrompting2022a,
  title = {Program of {{Thoughts Prompting}}: {{Disentangling Computation}} from {{Reasoning}} for {{Numerical Reasoning Tasks}}},
  shorttitle = {Program of {{Thoughts Prompting}}},
  author = {Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W.},
  date = {2022-11-28},
  number = {arXiv:2211.12588},
  eprint = {arXiv:2211.12588},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.12588},
  urldate = {2023-02-20},
  abstract = {Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\textbackslash\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github\textbackslash footnote\{\textbackslash url\{https://github.com/wenhuchen/Program-of-Thoughts\}\}.},
  pubstate = {preprint},
  keywords = {*general tools},
  file = {/Users/lukakuma/Zotero/storage/RE5IJR5J/Chen et al. - 2022 - Program of Thoughts Prompting Disentangling Compu.pdf;/Users/lukakuma/Zotero/storage/9WXZAC4D/2211.html}
}

@online{chenRandomizedEnsembledDouble2021,
  title = {Randomized {{Ensembled Double Q-Learning}}: {{Learning Fast Without}} a {{Model}}},
  shorttitle = {Randomized {{Ensembled Double Q-Learning}}},
  author = {Chen, Xinyue and Wang, Che and Zhou, Zijian and Ross, Keith},
  date = {2021-03-17},
  number = {arXiv:2101.05982},
  eprint = {arXiv:2101.05982},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.05982},
  urldate = {2022-07-20},
  abstract = {Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio {$>>$} 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio {$>>$} 1.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/7V8XKXGZ/Chen et al. - 2021 - Randomized Ensembled Double Q-Learning Learning F.pdf;/Users/lukakuma/Zotero/storage/4KP8GWM4/2101.html}
}

@online{chenRevisitingParameterEfficientTuning2022,
  title = {Revisiting {{Parameter-Efficient Tuning}}: {{Are We Really There Yet}}?},
  shorttitle = {Revisiting {{Parameter-Efficient Tuning}}},
  author = {Chen, Guanzheng and Liu, Fangyu and Meng, Zaiqiao and Liang, Shangsong},
  date = {2022-02-16},
  number = {arXiv:2202.07962},
  eprint = {arXiv:2202.07962},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.07962},
  urldate = {2023-02-16},
  abstract = {Parameter-Efficient Tuning (PETuning) methods have been deemed by many as the new paradigm for using pretrained language models (PLMs). By tuning just a fraction amount of parameters comparing to full model finetuning, PETuning methods claim to have achieved performance on par with or even better than finetuning. In this work, we take a step back and re-examine these PETuning methods by conducting the first comprehensive investigation into the training and evaluation of them. We found the problematic validation and testing practice in current studies, when accompanied by the instability nature of PETuning methods, has led to unreliable conclusions. When being compared under a truly fair evaluation protocol, PETuning cannot yield consistently competitive performance while finetuning remains to be the best-performing method in medium- and high-resource settings. We delve deeper into the cause of the instability and observed that the number of trainable parameters and training iterations are two main factors: reducing trainable parameters and prolonging training iterations may lead to higher stability in PETuning methods.},
  pubstate = {preprint},
  version = {1},
  keywords = {*peft},
  file = {/Users/lukakuma/Zotero/storage/PNVEXPVU/Chen et al. - 2022 - Revisiting Parameter-Efficient Tuning Are We Real.pdf;/Users/lukakuma/Zotero/storage/XMFI25UI/2202.html}
}

@article{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-02-13},
  url = {https://arxiv.org/abs/2002.05709v3},
  urldate = {2022-04-25},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/HV3SSITH/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;/Users/lukakuma/Zotero/storage/FTIJNW7F/2002.html}
}

@online{chenSymbolicDiscoveryOptimization2023,
  title = {Symbolic {{Discovery}} of {{Optimization Algorithms}}},
  author = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and Le, Quoc V.},
  date = {2023-02-17},
  number = {arXiv:2302.06675},
  eprint = {arXiv:2302.06675},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.06675},
  urldate = {2023-02-25},
  abstract = {We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, \$\textbackslash textbf\{Lion\}\$ (\$\textbackslash textit\{Evo\$\textbackslash textbf\{L\}\$ved S\$\textbackslash textbf\{i\}\$gn M\$\textbackslash textbf\{o\}\$me\$\textbackslash textbf\{n\}\$tum\}\$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2\% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3\% \$\textbackslash textit\{zero-shot\}\$ and 91.1\% \$\textbackslash textit\{fine-tuning\}\$ accuracy on ImageNet, surpassing the previous best results by 2\% and 0.1\%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. The implementation of Lion is publicly available.},
  pubstate = {preprint},
  keywords = {optimizer},
  file = {/Users/lukakuma/Zotero/storage/NRVS9P5I/Chen et al. - 2023 - Symbolic Discovery of Optimization Algorithms.pdf;/Users/lukakuma/Zotero/storage/DFY8ZFVH/2302.html}
}

@online{chenVisualGPTDataefficientAdaptation2022a,
  title = {{{VisualGPT}}: {{Data-efficient Adaptation}} of {{Pretrained Language Models}} for {{Image Captioning}}},
  shorttitle = {{{VisualGPT}}},
  author = {Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
  date = {2022-03-30},
  number = {arXiv:2102.10407},
  eprint = {arXiv:2102.10407},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.10407},
  urldate = {2023-02-15},
  abstract = {The ability to quickly learn from a small quantity oftraining data widens the range of machine learning applications. In this paper, we propose a data-efficient image captioning model, VisualGPT, which leverages the linguistic knowledge from a large pretrained language model(LM). A crucial challenge is to balance between the use of visual information in the image and prior linguistic knowledge acquired from pretraining. We designed a novel self-resurrecting encoder-decoder attention mechanism to quickly adapt the pretrained LM as the language decoder ona small amount of in-domain training data. The proposed self-resurrecting activation unit produces sparse activations but has reduced susceptibility to zero gradients. We train the proposed model, VisualGPT, on 0.1\%, 0.5\% and 1\% of MSCOCO and Conceptual Captions training data. Under these conditions, we outperform the best baseline model by up to 10.8\% CIDEr on MS COCO and upto 5.4\% CIDEr on Conceptual Captions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray, a medical report generation dataset. To the best of our knowledge, this is the first work that improves data efficiency of image captioning by utilizing LM pretrained on unimodal data. Our code is available at: https://github.com/Vision-CAIR/VisualGPT.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/SLY9KGWE/Chen et al. - 2022 - VisualGPT Data-efficient Adaptation of Pretrained.pdf;/Users/lukakuma/Zotero/storage/UMU2W4E4/2102.html}
}

@article{chiangClusterGCNEfficientAlgorithm2019,
  title = {Cluster-{{GCN}}: {{An Efficient Algorithm}} for {{Training Deep}} and {{Large Graph Convolutional Networks}}},
  shorttitle = {Cluster-{{GCN}}},
  author = {Chiang, Wei-Lin and Liu, Xuanqing and Si, Si and Li, Yang and Bengio, Samy and Hsieh, Cho-Jui},
  date = {2019-05-20},
  url = {https://arxiv.org/abs/1905.07953v2},
  urldate = {2022-03-24},
  abstract = {Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy---using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by [16]. Our codes are publicly available at https://github.com/google-research/google-research/tree/master/cluster\_gcn.},
  langid = {english},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/HASGIZ8E/Chiang et al. - 2019 - Cluster-GCN An Efficient Algorithm for Training D.pdf;/Users/lukakuma/Zotero/storage/SL3R653M/1905.html}
}

@book{chiangStoriesYourLife2010,
  title = {Stories of {{Your Life}} and {{Others}}},
  author = {Chiang, Ted},
  date = {2010-10-26},
  edition = {Reissue edition},
  publisher = {{Vintage}},
  abstract = {From the author of Exhalation, an award-winning short story collection that blends "absorbing storytelling with meditations on the universe, being, time and space ... raises questions about the nature of reality and what it is to be human" (The New York Times).Stories of Your Life and Others delivers dual delights of the very, very strange and the heartbreakingly familiar, often presenting characters who must confront sudden change\textemdash the inevitable rise of automatons or the appearance of aliens\textemdash with some sense of normalcy. With sharp intelligence and humor, Chiang examines what it means to be alive in a world marked by uncertainty, but also by beauty and wonder. An award-winning collection from one of today's most lauded writers, Stories of Your Life and Others is a contemporary classic.Includes ``Story of Your Life''\textemdash the basis for the major motion picture Arrival},
  langid = {english},
  pagetotal = {285}
}

@online{childGeneratingLongSequences2019,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  date = {2019-04-23},
  number = {arXiv:1904.10509},
  eprint = {arXiv:1904.10509},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.10509},
  urldate = {2022-08-02},
  abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n \textbackslash sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  pubstate = {preprint},
  keywords = {sparsity},
  file = {/Users/lukakuma/Zotero/storage/SPKEFTV2/Child et al. - 2019 - Generating Long Sequences with Sparse Transformers.pdf;/Users/lukakuma/Zotero/storage/EFAGYZYH/1904.html}
}

@online{choePointMixerMLPMixerPoint2022,
  title = {{{PointMixer}}: {{MLP-Mixer}} for {{Point Cloud Understanding}}},
  shorttitle = {{{PointMixer}}},
  author = {Choe, Jaesung and Park, Chunghyun and Rameau, Francois and Park, Jaesik and Kweon, In So},
  date = {2022-03-16},
  number = {arXiv:2111.11187},
  eprint = {arXiv:2111.11187},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.11187},
  urldate = {2022-06-09},
  abstract = {MLP-Mixer has newly appeared as a new challenger against the realm of CNNs and transformer. Despite its simplicity compared to transformer, the concept of channelmixing MLPs and token-mixing MLPs achieves noticeable performance in visual recognition tasks. Unlike images, point clouds are inherently sparse, unordered and irregular, which limits the direct use of MLP-Mixer for point cloud understanding. In this paper, we propose PointMixer, a universal point set operator that facilitates information sharing among unstructured 3D points. By simply replacing token-mixing MLPs with a softmax function, PointMixer can ''mix'' features within/between point sets. By doing so, PointMixer can be broadly used in the network as interset mixing, intra-set mixing, and pyramid mixing. Extensive experiments show the competitive or superior performance of PointMixer in semantic segmentation, classification, and point reconstruction against transformer-based methods. Code will be released soon.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/Y76ATQKF/Choe et al. - 2022 - PointMixer MLP-Mixer for Point Cloud Understandin.pdf}
}

@article{choOpenCellProteomescaleEndogenous2021,
  title = {{{OpenCell}}: Proteome-Scale Endogenous Tagging Enables the Cartography of Human Cellular Organization},
  shorttitle = {{{OpenCell}}},
  author = {Cho, Nathan H. and Cheveralls, Keith C. and Brunner, Andreas-David and Kim, Kibeom and Michaelis, Andr\'e C. and Raghavan, Preethi and Kobayashi, Hirofumi and Savy, Laura and Li, Jason Y. and Canaj, Hera and Kim, James Y. S. and Stewart, Edna M. and Gnann, Christian and McCarthy, Frank and Cabrera, Joana P. and Brunetti, Rachel M. and Chhun, Bryant B. and Dingle, Greg and Hein, Marco Y. and Huang, Bo and Mehta, Shalin B. and Weissman, Jonathan S. and G\'omez-Sj\"oberg, Rafael and Itzhak, Daniel N. and Royer, Loic A. and Mann, Matthias and Leonetti, Manuel D.},
  date = {2021-12-08},
  pages = {2021.03.29.437450},
  publisher = {{bioRxiv}},
  url = {https://www.biorxiv.org/content/10.1101/2021.03.29.437450v2},
  urldate = {2022-05-30},
  abstract = {Elucidating the wiring diagram of the human cell is a central goal of the post-genomic era. We combined genome engineering, confocal live-cell imaging, mass spectrometry and data science to systematically map the localization and interactions of human proteins. Our approach provides a data-driven description of the molecular and spatial networks that organize the proteome. Unsupervised clustering of these networks delineates functional communities that facilitate biological discovery, and uncovers that RNA-binding proteins form a specific sub-group defined by unique interaction and localization properties. Furthermore, we discover that remarkably precise functional information can be derived from protein localization patterns, which often contain enough information to identify molecular interactions. Paired with a fully interactive website opencell.czbiohub.org, we provide a resource for the quantitative cartography of human cellular organization.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/KF62D76W/Cho et al. - 2021 - OpenCell proteome-scale endogenous tagging enable.pdf}
}

@online{choshenFusingFinetunedModels2022,
  title = {Fusing Finetuned Models for Better Pretraining},
  author = {Choshen, Leshem and Venezian, Elad and Slonim, Noam and Katz, Yoav},
  date = {2022-04-06},
  number = {arXiv:2204.03044},
  eprint = {arXiv:2204.03044},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.03044},
  urldate = {2022-12-15},
  abstract = {Pretrained models are the standard starting point for training. This approach consistently outperforms the use of a random initialization. However, pretraining is a costly endeavour that few can undertake. In this paper, we create better base models at hardly any cost, by fusing multiple existing fine tuned models into one. Specifically, we fuse by averaging the weights of these models. We show that the fused model results surpass the pretrained model ones. We also show that fusing is often better than intertraining. We find that fusing is less dependent on the target task. Furthermore, weight decay nullifies intertraining effects but not those of fusing.},
  pubstate = {preprint},
  keywords = {model fusion},
  file = {/Users/lukakuma/Zotero/storage/86ZGRQP2/Choshen et al. - 2022 - Fusing finetuned models for better pretraining.pdf;/Users/lukakuma/Zotero/storage/LK2A64M6/2204.html}
}

@article{chowdheryPaLMScalingLanguage2022,
  title = {{{PaLM}}: {{Scaling Language Modeling}} with {{Pathways}}},
  shorttitle = {{{PaLM}}},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  date = {2022-04-05},
  url = {https://arxiv.org/abs/2204.02311v2},
  urldate = {2022-04-09},
  abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  langid = {english},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/BYS5ZNKA/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf;/Users/lukakuma/Zotero/storage/AJS8JRRH/2204.html}
}

@online{christianoDeepReinforcementLearning2017,
  title = {Deep Reinforcement Learning from Human Preferences},
  author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  date = {2017-07-13},
  number = {arXiv:1706.03741},
  eprint = {arXiv:1706.03741},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.03741},
  urldate = {2022-10-28},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
  pubstate = {preprint},
  keywords = {alignment},
  file = {/Users/lukakuma/Zotero/storage/E7DX23X2/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf;/Users/lukakuma/Zotero/storage/6RGMBV3W/1706.html}
}

@online{christianoSupervisingStrongLearners2018,
  title = {Supervising Strong Learners by Amplifying Weak Experts},
  author = {Christiano, Paul and Shlegeris, Buck and Amodei, Dario},
  date = {2018-10-19},
  number = {arXiv:1810.08575},
  eprint = {arXiv:1810.08575},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1810.08575},
  urldate = {2022-12-28},
  abstract = {Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/3THNNEGC/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf;/Users/lukakuma/Zotero/storage/6VKS7H6U/1810.html}
}

@online{chungScalingInstructionFinetunedLanguage2022,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  date = {2022-10-21},
  number = {arXiv:2210.11416},
  eprint = {arXiv:2210.11416},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.11416},
  urldate = {2022-10-24},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  pubstate = {preprint},
  keywords = {alignment,finetuning,read},
  file = {/Users/lukakuma/Zotero/storage/6W77VJLI/v5.pdf;/Users/lukakuma/Zotero/storage/8FT2VCDQ/Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf;/Users/lukakuma/Zotero/storage/MGHYQ3KF/2210.html}
}

@online{clarkELECTRAPretrainingText2020,
  title = {{{ELECTRA}}: {{Pre-training Text Encoders}} as {{Discriminators Rather Than Generators}}},
  shorttitle = {{{ELECTRA}}},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  date = {2020-03-23},
  number = {arXiv:2003.10555},
  eprint = {arXiv:2003.10555},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2003.10555},
  urldate = {2022-10-31},
  abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/WE8IM4TB/Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminat.pdf}
}

@online{clarkTyDiQABenchmark2020,
  title = {{{TyDi QA}}: {{A Benchmark}} for {{Information-Seeking Question Answering}} in {{Typologically Diverse Languages}}},
  shorttitle = {{{TyDi QA}}},
  author = {Clark, Jonathan H. and Choi, Eunsol and Collins, Michael and Garrette, Dan and Kwiatkowski, Tom and Nikolaev, Vitaly and Palomaki, Jennimaria},
  date = {2020-03-10},
  number = {arXiv:2003.05002},
  eprint = {arXiv:2003.05002},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2003.05002},
  urldate = {2023-01-06},
  abstract = {Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA---a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology---the set of linguistic features each language expresses---such that we expect models performing well on this set to generalize across a large number of the world's languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don't know the answer yet, and the data is collected directly in each language without the use of translation.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/GKZ6CCHR/Clark et al. - 2020 - TyDi QA A Benchmark for Information-Seeking Quest.pdf;/Users/lukakuma/Zotero/storage/VJ8GW88G/2003.html}
}

@online{clarkUnifiedScalingLaws2022,
  title = {Unified {{Scaling Laws}} for {{Routed Language Models}}},
  author = {Clark, Aidan and family=Casas, given=Diego, prefix=de las, useprefix=false and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and family=Driessche, given=George, prefix=van den, useprefix=false and Rutherford, Eliza and Hennigan, Tom and Johnson, Matthew and Millican, Katie and Cassirer, Albin and Jones, Chris and Buchatskaya, Elena and Budden, David and Sifre, Laurent and Osindero, Simon and Vinyals, Oriol and Rae, Jack and Elsen, Erich and Kavukcuoglu, Koray and Simonyan, Karen},
  date = {2022-02-09},
  number = {arXiv:2202.01169},
  eprint = {arXiv:2202.01169},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.01169},
  urldate = {2022-08-09},
  abstract = {The performance of a language model has been shown to be effectively modeled as a power-law in its parameter count. Here we study the scaling behaviors of Routing Networks: architectures that conditionally use only a subset of their parameters while processing an input. For these models, parameter count and computational requirement form two independent axes along which an increase leads to better performance. In this work we derive and justify scaling laws defined on these two variables which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques. Afterwards we provide two applications of these laws: first deriving an Effective Parameter Count along which all models scale at the same rate, and then using the scaling coefficients to give a quantitative comparison of the three routing techniques considered. Our analysis derives from an extensive evaluation of Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters.},
  pubstate = {preprint},
  keywords = {read,scaling law,sparsity},
  file = {/Users/lukakuma/Zotero/storage/8JN3AXS3/Clark et al. - 2022 - Unified Scaling Laws for Routed Language Models.pdf;/Users/lukakuma/Zotero/storage/895FT3P3/2202.html}
}

@online{cluneAIGAsAIgeneratingAlgorithms2020,
  title = {{{AI-GAs}}: {{AI-generating}} Algorithms, an Alternate Paradigm for Producing General Artificial Intelligence},
  shorttitle = {{{AI-GAs}}},
  author = {Clune, Jeff},
  date = {2020-01-31},
  number = {arXiv:1905.10985},
  eprint = {arXiv:1905.10985},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.10985},
  urldate = {2022-11-18},
  abstract = {Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces required for intelligence, with the implicit assumption that some future group will complete the Herculean task of figuring out how to combine all of those pieces into a complex thinking machine. I call this the "manual AI approach". This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend in machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. I argue that either approach could produce general AI first, and both are scientifically worthwhile irrespective of which is the fastest path. Because both are promising, yet the ML community is currently committed to the manual approach, I argue that our community should increase its research investment in the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss AI-GA-specific safety and ethical considerations. Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/TBGAWKFS/Clune - 2020 - AI-GAs AI-generating algorithms, an alternate par.pdf;/Users/lukakuma/Zotero/storage/MY94IICY/1905.html}
}

@online{cobbePhasicPolicyGradient2020,
  title = {Phasic {{Policy Gradient}}},
  author = {Cobbe, Karl and Hilton, Jacob and Klimov, Oleg and Schulman, John},
  date = {2020-09-09},
  number = {arXiv:2009.04416},
  eprint = {arXiv:2009.04416},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.04416},
  urldate = {2022-07-23},
  abstract = {We introduce Phasic Policy Gradient (PPG), a reinforcement learning framework which modifies traditional on-policy actor-critic methods by separating policy and value function training into distinct phases. In prior methods, one must choose between using a shared network or separate networks to represent the policy and value function. Using separate networks avoids interference between objectives, while using a shared network allows useful features to be shared. PPG is able to achieve the best of both worlds by splitting optimization into two phases, one that advances training and one that distills features. PPG also enables the value function to be more aggressively optimized with a higher level of sample reuse. Compared to PPO, we find that PPG significantly improves sample efficiency on the challenging Procgen Benchmark.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/5FSYK2PP/Cobbe et al. - 2020 - Phasic Policy Gradient.pdf;/Users/lukakuma/Zotero/storage/2GG9QYB4/2009.html}
}

@online{cobbeTrainingVerifiersSolve2021,
  title = {Training {{Verifiers}} to {{Solve Math Word Problems}}},
  author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  date = {2021-11-17},
  number = {arXiv:2110.14168},
  eprint = {arXiv:2110.14168},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.14168},
  urldate = {2022-11-19},
  abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
  pubstate = {preprint},
  keywords = {formal reasoning,read},
  file = {/Users/lukakuma/Zotero/storage/Q8SE574L/Cobbe et al. - 2021 - Training Verifiers to Solve Math Word Problems.pdf;/Users/lukakuma/Zotero/storage/DYGHSIDY/2110.html}
}

@book{cohenComputerScientistGuide2007,
  title = {A {{Computer Scientist}}'s {{Guide}} to {{Cell Biology}}:},
  author = {Cohen, William W},
  date = {2007},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/Q77BB8XR/Cohen - A Computer Scientist‚Äôs Guide to Cell Biology.pdf}
}

@online{cohenDynamicPlanningOpenEnded2022,
  title = {Dynamic {{Planning}} in {{Open-Ended Dialogue}} Using {{Reinforcement Learning}}},
  author = {Cohen, Deborah and Ryu, Moonkyung and Chow, Yinlam and Keller, Orgad and Greenberg, Ido and Hassidim, Avinatan and Fink, Michael and Matias, Yossi and Szpektor, Idan and Boutilier, Craig and Elidan, Gal},
  date = {2022-07-25},
  number = {arXiv:2208.02294},
  eprint = {arXiv:2208.02294},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.02294},
  urldate = {2023-02-20},
  abstract = {Despite recent advances in natural language understanding and generation, and decades of research on the development of conversational bots, building automated agents that can carry on rich open-ended conversations with humans "in the wild" remains a formidable challenge. In this work we develop a real-time, open-ended dialogue system that uses reinforcement learning (RL) to power a bot's conversational skill at scale. Our work pairs the succinct embedding of the conversation state generated using SOTA (supervised) language models with RL techniques that are particularly suited to a dynamic action space that changes as the conversation progresses. Trained using crowd-sourced data, our novel system is able to substantially exceeds the (strong) baseline supervised model with respect to several metrics of interest in a live experiment with real users of the Google Assistant.},
  pubstate = {preprint},
  keywords = {RLHF},
  file = {/Users/lukakuma/Zotero/storage/VB6WEFLN/Cohen et al. - 2022 - Dynamic Planning in Open-Ended Dialogue using Rein.pdf;/Users/lukakuma/Zotero/storage/2YYIDV2S/2208.html}
}

@online{CompetingArtificialIntelligence2020,
  title = {Competing in {{Artificial Intelligence Chips}}: {{China}}'s {{Challenge}} amid {{Technology War}}},
  shorttitle = {Competing in {{Artificial Intelligence Chips}}},
  date = {2020-03-26T09:00:00-04:00},
  url = {https://www.cigionline.org/publications/competing-artificial-intelligence-chips-chinas-challenge-amid-technology-war/},
  urldate = {2022-04-14},
  abstract = {This special report assesses the challenges that China is facing in developing its artificial intelligence (AI) industry due to unprecedented US technology export restrictions. A central proposition is that China's achievements in AI lack a robust foundation in leading-edge AI chips, and thus the country is vulnerable to externally imposed supply disruptions. Success in AI requires mastery of data, algorithms and computing power, which, in turn, is determined by the performance of AI chips. Increasing computing power that is cost-effective and energy-saving is the indispensable third component of this magic AI triangle.},
  langid = {english},
  organization = {{Centre for International Governance Innovation}},
  file = {/Users/lukakuma/Zotero/storage/X3TFEZIU/2020 - Competing in Artificial Intelligence Chips China‚Äô.pdf}
}

@article{constantinescuOrganizingConceptualKnowledge2016,
  title = {Organizing Conceptual Knowledge in Humans with a Gridlike Code},
  author = {Constantinescu, Alexandra O. and O'Reilly, Jill X. and Behrens, Timothy E. J.},
  date = {2016-06-17},
  journaltitle = {Science},
  volume = {352},
  number = {6292},
  pages = {1464--1468},
  publisher = {{American Association for the Advancement of Science}},
  url = {https://www.science.org/doi/10.1126/science.aaf0941},
  urldate = {2022-06-18},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/MPXLQ2SL/Constantinescu et al. - 2016 - Organizing conceptual knowledge in humans with a g.pdf}
}

@book{coverElementsInformationTheory2006,
  title = {Elements of {{Information Theory Second Edition}} (Solution Manual)},
  author = {Cover, Thomas M and Thomas, Joy A},
  date = {2006},
  langid = {english},
  keywords = {information theory},
  file = {/Users/lukakuma/Zotero/storage/JY52WD5I/Cover and Thomas - Elements of Information Theory Second Edition.pdf}
}

@book{coverElementsInformationTheory2006a,
  title = {Elements of {{Information Theory Second Edition}}},
  author = {Cover, Thomas M and Thomas, Joy A},
  date = {2006},
  langid = {english},
  keywords = {information theory},
  file = {/Users/lukakuma/Zotero/storage/2KYY4YUW/Cover and Thomas - ELEMENTS OF INFORMATION THEORY.pdf}
}

@article{cramerAlphaFold2FutureStructural2021,
  title = {{{AlphaFold2}} and the Future of Structural Biology},
  author = {Cramer, Patrick},
  date = {2021-09},
  journaltitle = {Nature Structural \& Molecular Biology},
  shortjournal = {Nat Struct Mol Biol},
  volume = {28},
  number = {9},
  pages = {704--705},
  publisher = {{Nature Publishing Group}},
  issn = {1545-9985},
  url = {https://www.nature.com/articles/s41594-021-00650-1},
  urldate = {2022-05-26},
  issue = {9},
  langid = {english},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/IMP6Q2Y6/Cramer - 2021 - AlphaFold2 and the future of structural biology.pdf;/Users/lukakuma/Zotero/storage/HLFLG5A5/s41594-021-00650-1.html}
}

@online{CredibleNeutralityGuiding2020,
  title = {Credible {{Neutrality As A Guiding Principle}}},
  date = {2020-01-04T00:00:00},
  url = {https://nakamoto.com/credible-neutrality/},
  urldate = {2022-03-15},
  abstract = {When building mechanisms that decide high-stakes outcomes, it's important for those mechanisms to be credibly neutral.},
  langid = {english},
  organization = {{NAKAMOTO}},
  file = {/Users/lukakuma/Zotero/storage/ZF8ESQK5/credible-neutrality.html}
}

@online{creswellFaithfulReasoningUsing2022,
  title = {Faithful {{Reasoning Using Large Language Models}}},
  author = {Creswell, Antonia and Shanahan, Murray},
  date = {2022-08-30},
  number = {arXiv:2208.14271},
  eprint = {arXiv:2208.14271},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.14271},
  urldate = {2022-11-02},
  abstract = {Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.},
  langid = {english},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/CYSDS9UM/2208.14271.pdf}
}

@online{creswellSelectionInferenceExploitingLarge2022,
  title = {Selection-{{Inference}}: {{Exploiting Large Language Models}} for {{Interpretable Logical Reasoning}}},
  shorttitle = {Selection-{{Inference}}},
  author = {Creswell, Antonia and Shanahan, Murray and Higgins, Irina},
  date = {2022-05-19},
  number = {arXiv:2205.09712},
  eprint = {arXiv:2205.09712},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.09712},
  urldate = {2022-12-28},
  abstract = {Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100\% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/J478HECI/Creswell et al. - 2022 - Selection-Inference Exploiting Large Language Mod.pdf;/Users/lukakuma/Zotero/storage/V5JEE6QH/2205.html}
}

@article{CryptoeconomicsLimitationGovernance2021,
  title = {Cryptoeconomics as a {{Limitation}} on {{Governance}}},
  date = {2021-08-22},
  publisher = {{OSF}},
  url = {https://osf.io/dzasq/},
  urldate = {2022-03-09},
  abstract = {Hosted on the Open Science Framework},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/VXH5G2DV/wzf85.html}
}

@article{dabneyDistributionalCodeValue2020,
  title = {A Distributional Code for Value in Dopamine-Based Reinforcement Learning},
  author = {Dabney, Will and Kurth-Nelson, Zeb and Uchida, Naoshige and Starkweather, Clara Kwon and Hassabis, Demis and Munos, R\'emi and Botvinick, Matthew},
  date = {2020-01},
  journaltitle = {Nature},
  volume = {577},
  number = {7792},
  pages = {671--675},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/s41586-019-1924-6},
  urldate = {2022-07-08},
  abstract = {Since its introduction, the reward prediction error theory of dopamine has explained a wealth of empirical phenomena, providing a unifying framework for understanding the representation of reward and value in the brain1\textendash 3. According to the now canonical theory, reward predictions are represented as a single scalar quantity, which supports learning about the expectation, or mean, of stochastic outcomes. Here we propose an account of dopamine-based reinforcement learning inspired by recent artificial intelligence research on distributional reinforcement learning4\textendash 6. We hypothesized that the brain represents possible future rewards not as a single mean, but instead as a probability distribution, effectively representing multiple future outcomes simultaneously and in parallel. This idea implies a set of empirical predictions, which we tested using single-unit recordings from mouse ventral tegmental area. Our findings provide strong evidence for a neural realization of distributional reinforcement learning.},
  issue = {7792},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/X38DYTJS/Dabney et al. - 2020 - A distributional code for value in dopamine-based .pdf}
}

@article{daigavaneUnderstandingConvolutionsGraphs2021,
  title = {Understanding {{Convolutions}} on {{Graphs}}},
  author = {Daigavane, Ameya and Ravindran, Balaraman and Aggarwal, Gaurav},
  date = {2021-09-02},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {6},
  number = {9},
  pages = {e32},
  issn = {2476-0757},
  url = {https://distill.pub/2021/understanding-gnns},
  urldate = {2022-03-29},
  abstract = {Understanding the building blocks and design choices of graph neural networks.},
  langid = {english},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/F8SUEH6F/understanding-gnns.html}
}

@online{daiKnowledgeNeuronsPretrained2022,
  title = {Knowledge {{Neurons}} in {{Pretrained Transformers}}},
  author = {Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  date = {2022-03-09},
  number = {arXiv:2104.08696},
  eprint = {arXiv:2104.08696},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.08696},
  urldate = {2022-11-02},
  abstract = {Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus (Petroni et al., 2019; Jiang et al., 2020b). In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers. The code is available at https://github.com/ Hunter-DDM/knowledge-neurons.},
  langid = {english},
  pubstate = {preprint},
  keywords = {knowledge},
  file = {/Users/lukakuma/Zotero/storage/4AXD2UPT/Dai et al. - 2022 - Knowledge Neurons in Pretrained Transformers.pdf}
}

@unpublished{daiTransformerXLAttentiveLanguage2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  date = {2019-06-02},
  number = {arXiv:1901.02860},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1901.02860},
  urldate = {2022-05-13},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  keywords = {(ext) Gato},
  file = {/Users/lukakuma/Zotero/storage/CLL42VJI/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/Users/lukakuma/Zotero/storage/TIKEI8YL/1901.html}
}

@online{dalviExplainingAnswersEntailment2022,
  title = {Explaining {{Answers}} with {{Entailment Trees}}},
  author = {Dalvi, Bhavana and Jansen, Peter and Tafjord, Oyvind and Xie, Zhengnan and Smith, Hannah and Pipatanangkura, Leighanna and Clark, Peter},
  date = {2022-05-28},
  number = {arXiv:2104.08661},
  eprint = {arXiv:2104.08661},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.08661},
  urldate = {2022-11-02},
  abstract = {Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a ``rationale''). If this could be done, new opportunities for understanding and debugging the system's reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK 1 , the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35\% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations.},
  langid = {english},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/3N2J9BY4/Dalvi et al. - 2022 - Explaining Answers with Entailment Trees.pdf}
}

@inproceedings{danihelkaPolicyImprovementPlanning2021,
  title = {Policy Improvement by Planning with {{Gumbel}}},
  author = {Danihelka, Ivo and Guez, Arthur and Schrittwieser, Julian and Silver, David},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=bERaNdoegnO},
  urldate = {2022-05-15},
  abstract = {AlphaZero is a powerful reinforcement learning algorithm based on approximate policy iteration and tree search. However, AlphaZero can fail to improve its policy network, if not visiting all...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/YWEB6ZWX/Danihelka et al. - 2021 - Policy improvement by planning with Gumbel.pdf;/Users/lukakuma/Zotero/storage/YBIWPFW2/forum.html}
}

@online{daoFlashAttentionFastMemoryEfficient2022,
  title = {{{FlashAttention}}: {{Fast}} and {{Memory-Efficient Exact Attention}} with {{IO-Awareness}}},
  shorttitle = {{{FlashAttention}}},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R\'e, Christopher},
  date = {2022-06-23},
  number = {arXiv:2205.14135},
  eprint = {arXiv:2205.14135},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.14135},
  urldate = {2022-09-24},
  abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\$\textbackslash times\$ speedup on GPT-2 (seq. length 1K), and 2.4\$\textbackslash times\$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/63DYSHSI/Dao et al. - 2022 - FlashAttention Fast and Memory-Efficient Exact At.pdf;/Users/lukakuma/Zotero/storage/4C3GEQZS/2205.html}
}

@online{dasguptaCollaboratingLanguageModels2023,
  title = {Collaborating with Language Models for Embodied Reasoning},
  author = {Dasgupta, Ishita and Kaeser-Chen, Christine and Marino, Kenneth and Ahuja, Arun and Babayan, Sheila and Hill, Felix and Fergus, Rob},
  date = {2023-02-01},
  number = {arXiv:2302.00763},
  eprint = {arXiv:2302.00763},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.00763},
  urldate = {2023-02-20},
  abstract = {Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.},
  pubstate = {preprint},
  keywords = {action/digital},
  file = {/Users/lukakuma/Zotero/storage/N7DGMNEC/Dasgupta et al. - 2023 - Collaborating with language models for embodied re.pdf;/Users/lukakuma/Zotero/storage/NZZJVT2Y/2302.html}
}

@online{dasguptaLanguageModelsShow2022,
  title = {Language Models Show Human-like Content Effects on Reasoning},
  author = {Dasgupta, Ishita and Lampinen, Andrew K. and Chan, Stephanie C. Y. and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L. and Hill, Felix},
  date = {2022-07-14},
  number = {arXiv:2207.07051},
  eprint = {arXiv:2207.07051},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.07051},
  urldate = {2023-01-08},
  abstract = {Abstract reasoning is a key ability for an intelligent system. Large language models achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect, and depends on our knowledge and beliefs about the content of the reasoning problem. For example, humans reason much more reliably about logical rules that are grounded in everyday situations than arbitrary rules about abstract attributes. The training experiences of language models similarly endow them with prior expectations that reflect human knowledge and beliefs. We therefore hypothesized that language models would show human-like content effects on abstract reasoning problems. We explored this hypothesis across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task (Wason, 1968). We find that state of the art large language models (with 7 or 70 billion parameters; Hoffman et al., 2022) reflect many of the same patterns observed in humans across these tasks -- like humans, models reason more effectively about believable situations than unrealistic or abstract ones. Our findings have implications for understanding both these cognitive effects, and the factors that contribute to language model performance.},
  pubstate = {preprint},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/FHK2WJ7F/Dasgupta et al. - 2022 - Language models show human-like content effects on.pdf;/Users/lukakuma/Zotero/storage/79KJZQBD/2207.html}
}

@online{dathathriPlugPlayLanguage2020,
  title = {Plug and {{Play Language Models}}: {{A Simple Approach}} to {{Controlled Text Generation}}},
  shorttitle = {Plug and {{Play Language Models}}},
  author = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  date = {2020-03-03},
  number = {arXiv:1912.02164},
  eprint = {arXiv:1912.02164},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1912.02164},
  urldate = {2022-11-02},
  abstract = {Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/FBGW4EHK/Dathathri et al. - 2020 - Plug and Play Language Models A Simple Approach t.pdf}
}

@article{daviesAdvancingMathematicsGuiding2021a,
  title = {Advancing Mathematics by Guiding Human Intuition with {{AI}}},
  author = {Davies, Alex and Veli\v{c}kovi\'c, Petar and Buesing, Lars and Blackwell, Sam and Zheng, Daniel and Toma\v{s}ev, Nenad and Tanburn, Richard and Battaglia, Peter and Blundell, Charles and Juh\'asz, Andr\'as and Lackenby, Marc and Williamson, Geordie and Hassabis, Demis and Kohli, Pushmeet},
  date = {2021-12-02},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {600},
  number = {7887},
  pages = {70--74},
  issn = {0028-0836, 1476-4687},
  url = {https://www.nature.com/articles/s41586-021-04086-x},
  urldate = {2022-12-22},
  abstract = {Abstract                            The practice of mathematics involves discovering patterns and using these to formulate and prove conjectures, resulting in theorems. Since the 1960s, mathematicians have used computers to assist in the discovery of patterns and formulation of conjectures               1               , most famously in the Birch and Swinnerton-Dyer conjecture               2               , a Millennium Prize Problem               3               . Here we provide examples of new fundamental results in pure mathematics that have been discovered with the assistance of machine learning\textemdash demonstrating a method by which machine learning can aid mathematicians in discovering new conjectures and theorems. We propose a process of using machine learning to discover potential patterns and relations between mathematical objects, understanding them with attribution techniques and using these observations to guide intuition and propose conjectures. We outline this machine-learning-guided framework and demonstrate its successful application to current research questions in distinct areas of pure mathematics, in each case showing how it led to meaningful mathematical contributions on important open problems: a new connection between the algebraic and geometric structure of knots, and a candidate algorithm predicted by the combinatorial invariance conjecture for symmetric groups               4               . Our work may serve as a model for collaboration between the fields of mathematics and artificial intelligence (AI) that can achieve surprising results by leveraging the respective strengths of mathematicians and machine learning.},
  langid = {english},
  keywords = {DeepMind,formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/XB5NQEMV/Davies et al. - 2021 - Advancing mathematics by guiding human intuition w.pdf;/Users/lukakuma/Zotero/storage/99SXIQSQ/s41586-021-04086-x.html}
}

@unpublished{deacNeuralAlgorithmicReasoners2021,
  title = {Neural {{Algorithmic Reasoners}} Are {{Implicit Planners}}},
  author = {Deac, Andreea and Veli\v{c}kovi\'c, Petar and Milinkovi\'c, Ognjen and Bacon, Pierre-Luc and Tang, Jian and Nikoli\'c, Mladen},
  date = {2021-10-11},
  eprint = {2110.05442},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2110.05442},
  urldate = {2022-04-23},
  abstract = {Implicit planning has emerged as an elegant technique for combining learned models of the world with end-to-end model-free reinforcement learning. We study the class of implicit planners inspired by value iteration, an algorithm that is guaranteed to yield perfect policies in fully-specified tabular environments. We find that prior approaches either assume that the environment is provided in such a tabular form -- which is highly restrictive -- or infer "local neighbourhoods" of states to run value iteration over -- for which we discover an algorithmic bottleneck effect. This effect is caused by explicitly running the planning algorithm based on scalar predictions in every state, which can be harmful to data efficiency if such scalars are improperly predicted. We propose eXecuted Latent Value Iteration Networks (XLVINs), which alleviate the above limitations. Our method performs all planning computations in a high-dimensional latent space, breaking the algorithmic bottleneck. It maintains alignment with value iteration by carefully leveraging neural graph-algorithmic reasoning and contrastive self-supervised learning. Across eight low-data settings -- including classical control, navigation and Atari -- XLVINs provide significant improvements to data efficiency against value iteration-based implicit planners, as well as relevant model-free baselines. Lastly, we empirically verify that XLVINs can closely align with value iteration.},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/UEFERZ6V/Deac et al. - 2021 - Neural Algorithmic Reasoners are Implicit Planners.pdf;/Users/lukakuma/Zotero/storage/XLSI3ZLQ/2110.html}
}

@video{deepmindUsingAIAccelerate2022,
  title = {Using {{AI}} to Accelerate Scientific Discovery - {{Demis Hassabis}} ({{Crick Insight Lecture Series}})},
  editor = {{DeepMind}},
  date = {2022-03-04},
  url = {https://www.youtube.com/watch?v=XtJVLOe4cfs},
  urldate = {2022-06-06},
  abstract = {Using AI to accelerate scientific discovery Demis Hassabis, co-founder and CEO, DeepMind At The Francis Crick Institute in King's Cross, London Abstract: The past decade has seen incredible advances in the field of Artificial Intelligence (AI). DeepMind has been in the vanguard of many of these big breakthroughs, pioneering the development of self-learning systems like AlphaGo, the first program to beat the world champion at the complex game of Go. Games have proven to be a great training ground for developing and testing AI algorithms, but the aim at DeepMind has always been to build general learning systems ultimately capable of solving important problems in the real world. Excitingly, I believe we are on the cusp of a new era in science with AI poised to be a powerful tool for accelerating scientific discovery itself. We recently demonstrated this potential with our AlphaFold system, a solution to the 50-year grand challenge of protein structure prediction, culminating in the release of the most accurate and complete picture of the human proteome. The speaker: Demis Hassabis is the Founder and CEO of DeepMind, the world's leading AI research company that aims to solve intelligence to advance science and benefit humanity In 2016, DeepMind developed AlphaGo, the first program to beat a world champion at the complex game of Go. In 2020, its Alphafold program was heralded as a solution to the 50-year grand challenge of protein structure prediction and in 2021, DeepMind launched the AlphaFold Protein Structure Database, which offers the most complete and accurate picture of the human proteome to date.   A chess prodigy, Demis reached master standard aged 13, and went on to program the multi-million selling simulation game Theme Park aged 17. After graduating from Cambridge University in computer science, he founded pioneering videogames company Elixir Studios, and completed a PhD in cognitive neuroscience at University College London. Science listed his neuroscience research on imagination as one of 2007's top ten breakthroughs, and in 2021, AlphaFold2 was selected as the Breakthrough of the Year. He is a Fellow of the Royal Society and the Royal Academy of Engineering. In 2017 he featured in the Time 100 list of most influential people, and in 2018 he was awarded a CBE for services to science and technology.},
  editortype = {director}
}

@video{deepmindWelcomeDeepMindEmbarking2022,
  title = {Welcome to {{DeepMind}}: {{Embarking}} on One of the Greatest Adventures in Scientific History},
  shorttitle = {Welcome to {{DeepMind}}},
  editor = {{DeepMind}},
  date = {2022-05-05},
  url = {https://www.youtube.com/watch?v=b6e8CCPp2Kc},
  urldate = {2022-06-06},
  abstract = {At DeepMind, we're embarking on one of the greatest adventures in scientific history. Our mission is to solve intelligence, to advance science and benefit humanity.  To make this possible, we bring together scientists, designers, engineers, ethicists, and more, to research and build safe artificial intelligence systems that can help transform society for the better. By combining creative thinking with our dedicated, scientific approach, we're unlocking new ways of solving complex problems and working to develop a more general and capable problem-solving system, known as artificial general intelligence (AGI). Guided by safety and ethics, this invention could help society find answers to some of the most important challenges facing society today.  We regularly partner with academia and nonprofit organisations, and our technologies are used across Google devices by millions of people every day. From solving a 50-year-old grand challenge in biology with AlphaFold and synthesising voices with WaveNet, to mastering complex games with AlphaZero and preserving wildlife in the Serengeti, our novel advances make a positive and lasting impact.  Incredible ideas thrive when diverse people join together. With headquarters in London and research labs in Paris, New York, Montreal, Edmonton, and Mountain View, CA, we're always looking for great people from all walks of life to join our mission. Learn more at deepmind.com/about and apply for open roles at deepmind.com/careers. \#LifeAtDeepMind \#artificialintelligence \#AGI \#socialimpact},
  editortype = {director}
}

@article{degraveMagneticControlTokamak2022,
  title = {Magnetic Control of Tokamak Plasmas through Deep Reinforcement Learning},
  author = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and family=Casas, given=Diego, prefix=de las, useprefix=true and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray and Hassabis, Demis and Riedmiller, Martin},
  date = {2022-02},
  journaltitle = {Nature},
  volume = {602},
  number = {7897},
  pages = {414--419},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/s41586-021-04301-9},
  urldate = {2022-03-11},
  abstract = {Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak \`a Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and `snowflake' configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained `droplets' on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
  issue = {7897},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/F4KFNCFU/Degrave et al. - 2022 - Magnetic control of tokamak plasmas through deep r.pdf;/Users/lukakuma/Zotero/storage/5JYZ7Y68/s41586-021-04301-9.html}
}

@online{dehghaniScalingVisionTransformers2023,
  title = {Scaling {{Vision Transformers}} to 22 {{Billion Parameters}}},
  author = {Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and Jenatton, Rodolphe and Beyer, Lucas and Tschannen, Michael and Arnab, Anurag and Wang, Xiao and Riquelme, Carlos and Minderer, Matthias and Puigcerver, Joan and Evci, Utku and Kumar, Manoj and family=Steenkiste, given=Sjoerd, prefix=van, useprefix=true and Elsayed, Gamaleldin F. and Mahendran, Aravindh and Yu, Fisher and Oliver, Avital and Huot, Fantine and Bastings, Jasmijn and Collier, Mark Patrick and Gritsenko, Alexey and Birodkar, Vighnesh and Vasconcelos, Cristina and Tay, Yi and Mensink, Thomas and Kolesnikov, Alexander and Paveti\'c, Filip and Tran, Dustin and Kipf, Thomas and Lu\v{c}i\'c, Mario and Zhai, Xiaohua and Keysers, Daniel and Harmsen, Jeremiah and Houlsby, Neil},
  date = {2023-02-10},
  number = {arXiv:2302.05442},
  eprint = {arXiv:2302.05442},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.05442},
  urldate = {2023-02-14},
  abstract = {The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for "LLM-like" scaling in vision, and provides key steps towards getting there.},
  pubstate = {preprint},
  keywords = {ViT},
  file = {/Users/lukakuma/Zotero/storage/TC8GEQG8/Dehghani et al. - 2023 - Scaling Vision Transformers to 22 Billion Paramete.pdf;/Users/lukakuma/Zotero/storage/STUETAMP/2302.html}
}

@online{dehghaniUniversalTransformers2019,
  title = {Universal {{Transformers}}},
  author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, \L ukasz},
  date = {2019-03-05},
  number = {arXiv:1807.03819},
  eprint = {arXiv:1807.03819},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1807.03819},
  urldate = {2022-09-02},
  abstract = {Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/QJFCIHSN/Dehghani et al. - 2019 - Universal Transformers.pdf;/Users/lukakuma/Zotero/storage/3KJ38CWF/1807.html}
}

@book{deisenrothMathematicsMachineLearning2020,
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  date = {2020},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/ZD24LKEQ/Deisenroth et al. - Mathematics for Machine Learning.pdf}
}

@unpublished{deleuBayesianStructureLearning2022,
  title = {Bayesian {{Structure Learning}} with {{Generative Flow Networks}}},
  author = {Deleu, Tristan and G\'ois, Ant\'onio and Emezue, Chris and Rankawat, Mansi and Lacoste-Julien, Simon and Bauer, Stefan and Bengio, Yoshua},
  date = {2022-02-28},
  eprint = {2202.13903},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2202.13903},
  urldate = {2022-05-10},
  abstract = {In Bayesian structure learning, we are interested in inferring a distribution over the directed acyclic graph (DAG) structure of Bayesian networks, from data. Defining such a distribution is very challenging, due to the combinatorially large sample space, and approximations based on MCMC are often required. Recently, a novel class of probabilistic models, called Generative Flow Networks (GFlowNets), have been introduced as a general framework for generative modeling of discrete and composite objects, such as graphs. In this work, we propose to use a GFlowNet as an alternative to MCMC for approximating the posterior distribution over the structure of Bayesian networks, given a dataset of observations. Generating a sample DAG from this approximate distribution is viewed as a sequential decision problem, where the graph is constructed one edge at a time, based on learned transition probabilities. Through evaluation on both simulated and real data, we show that our approach, called DAG-GFlowNet, provides an accurate approximation of the posterior over DAGs, and it compares favorably against other methods based on MCMC or variational inference.},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/3YGNAJMW/Deleu et al. - 2022 - Bayesian Structure Learning with Generative Flow N.pdf;/Users/lukakuma/Zotero/storage/HN58LAML/2202.html}
}

@online{DemocracyActuallyGood2021,
  title = {Democracy {{Actually Is Good}} at {{Governing Infrastructure}}, and {{Infrastructure Is Exactly What Democracy Should Govern}}},
  date = {2021-09-07},
  url = {https://www.radicalxchange.org/media/blog/democracy-actually-is-good-at-governing-infrastructure-and-infrastructure-is-exactly-what-democracy-should-govern/},
  urldate = {2021-11-09},
  abstract = {We are a community of activists, artists, entrepreneurs, and scholars committed to using mechanism design to inspire radical social change.},
  langid = {english},
  organization = {{RadicalxChange}},
  file = {/Users/lukakuma/Zotero/storage/637447G5/democracy-actually-is-good-at-governing-infrastructure-and-infrastructure-is-exactly-what-democ.html}
}

@online{dennisEmergentComplexityZeroshot2021,
  title = {Emergent {{Complexity}} and {{Zero-shot Transfer}} via {{Unsupervised Environment Design}}},
  author = {Dennis, Michael and Jaques, Natasha and Vinitsky, Eugene and Bayen, Alexandre and Russell, Stuart and Critch, Andrew and Levine, Sergey},
  date = {2021-02-03},
  number = {arXiv:2012.02096},
  eprint = {arXiv:2012.02096},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.02096},
  urldate = {2022-11-18},
  abstract = {A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/QUW9ITDE/Dennis et al. - 2021 - Emergent Complexity and Zero-shot Transfer via Uns.pdf;/Users/lukakuma/Zotero/storage/82AHV726/2012.html}
}

@online{dettmersLLMInt88bit2022,
  title = {{{LLM}}.Int8(): 8-Bit {{Matrix Multiplication}} for {{Transformers}} at {{Scale}}},
  shorttitle = {{{LLM}}.Int8()},
  author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  date = {2022-08-15},
  number = {arXiv:2208.07339},
  eprint = {arXiv:2208.07339},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.07339},
  urldate = {2022-08-18},
  abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs.},
  pubstate = {preprint},
  version = {1},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/B8PKDZPX/Dettmers et al. - 2022 - LLM.int8() 8-bit Matrix Multiplication for Transf.pdf;/Users/lukakuma/Zotero/storage/P24FSSRA/2208.html}
}

@unpublished{devinLearningModularNeural2016,
  title = {Learning {{Modular Neural Network Policies}} for {{Multi-Task}} and {{Multi-Robot Transfer}}},
  author = {Devin, Coline and Gupta, Abhishek and Darrell, Trevor and Abbeel, Pieter and Levine, Sergey},
  date = {2016-09-22},
  number = {arXiv:1609.07088},
  eprint = {1609.07088},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1609.07088},
  urldate = {2022-05-13},
  abstract = {Reinforcement learning (RL) can automate a wide variety of robotic skills, but learning each new skill requires considerable real-world data collection and manual representation engineering to design policy classes or features. Using deep reinforcement learning to train general purpose neural network policies alleviates some of the burden of manual representation engineering by using expressive policy classes, but exacerbates the challenge of data collection, since such methods tend to be less efficient than RL with low-dimensional, hand-designed representations. Transfer learning can mitigate this problem by enabling us to transfer information from one skill to another and even from one robot to another. We show that neural network policies can be decomposed into "task-specific" and "robot-specific" modules, where the task-specific modules are shared across robots, and the robot-specific modules are shared across all tasks on that robot. This allows for sharing task information, such as perception, between robots and sharing robot information, such as dynamics and kinematics, between tasks. We exploit this decomposition to train mix-and-match modules that can solve new robot-task combinations that were not seen during training. Using a novel neural network architecture, we demonstrate the effectiveness of our transfer method for enabling zero-shot generalization with a variety of robots and tasks in simulation for both visual and non-visual tasks.},
  keywords = {(ext) Gato},
  file = {/Users/lukakuma/Zotero/storage/ANWKWZQ6/Devin et al. - 2016 - Learning Modular Neural Network Policies for Multi.pdf;/Users/lukakuma/Zotero/storage/RGBYU7W9/1609.html}
}

@article{devlinBERTPretrainingDeep2018,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2018-10-11},
  url = {https://arxiv.org/abs/1810.04805v2},
  urldate = {2022-08-10},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  langid = {english},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/QKMV6H88/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@online{diaoActivePromptingChainofThought2023,
  title = {Active {{Prompting}} with {{Chain-of-Thought}} for {{Large Language Models}}},
  author = {Diao, Shizhe and Wang, Pengcheng and Lin, Yong and Zhang, Tong},
  date = {2023-02-23},
  number = {arXiv:2302.12246},
  eprint = {arXiv:2302.12246},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.12246},
  urldate = {2023-02-28},
  abstract = {The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-cot.},
  pubstate = {preprint},
  keywords = {calibration},
  file = {/Users/lukakuma/Zotero/storage/6AZZW656/Diao et al. - 2023 - Active Prompting with Chain-of-Thought for Large L.pdf;/Users/lukakuma/Zotero/storage/XAUIGBJ8/2302.html}
}

@video{dimitribertsekasLessonsAlphaZeroOptimal2021,
  title = {Lessons from {{AlphaZero}} for {{Optimal}}, {{Model Predictive}}, and {{Adaptive Control}}, {{Lecture}} at {{KTH}}},
  editor = {{Dimitri Bertsekas}},
  date = {2021-11-20},
  url = {https://www.youtube.com/watch?v=WQS7933ub9s},
  urldate = {2022-06-15},
  abstract = {Slides at http://web.mit.edu/dimitrib/www/abstr... Abstract: Some of the most exciting success stories in reinforcement learning have been in the area of games. Primary examples are the recent AlphaZero program (which plays chess), and the similarly structured and earlier (1990s) TD-Gammon program (which plays backgammon). These programs were trained off-line extensively using sophisticated approximate policy iteration algorithms and neural networks. Yet the AlphaZero player that has been obtained off-line is not used directly during on-line play (it is too inaccurate due to approximation errors that are inherent in off-line neural network training). Instead a separate on-line player is used, which is based on multistep lookahead and a terminal position evaluator that was trained using experience with the off-line player. The on-line player performs a form of policy improvement, which unlike the off-line player, is not degraded by neural network approximations. As a result, it greatly improves the performance of the off-line player.  Similarly, TD-Gammon performs on-line a policy improvement step using one-step or two-step lookahead minimization, which is not degraded by neural network approximations. To this end it uses an off-line neural-network trained terminal position evaluator, and importantly it also extends its on-line lookahead by rollout (simulation with the one-step lookahead player that uses the position evaluator).  A major lesson from AlphaZero and TD-Gammon is that performance of an off-line trained controller can be greatly improved by on-line approximation in value space,  with long lookahead (involving minimization or rollout with an off-line obtained policy, or both), and terminal cost approximation that is obtained off-line. The performance enhancement is often dramatic and is due to a simple fact, which is the focal point of this work:  On-line approximation in value space with one-step lookahead amounts to a step of Newton's method for solving Bellman's equation, while the starting point for the Newton step is based on the results of off-line training, and is enhanced by longer on-line lookahead minimization and on-line rollout.  Indeed the major determinant of the quality of the controller thus obtained is the Newton step that is performed on-line, while off-line training is a secondary contributor by comparison. This conceptual view of RL can be understood intuitively in terms of abstract models of infinite horizon DP and simple geometrical constructions. It manifests itself in model predictive control, and it helps to clarify the all-important stability issues within that context. Moreover on-line approximation in value space works well with changing problem parameters and on-line replanning, similar to indirect adaptive control. Here the Bellman equation is perturbed due to the parameter changes, but approximation in value space still operates as a Newton step.  In this work we aim to provide insights (often based on visualization), which explain the beneficial effects of on-line decision making on top of off-line training. One of our  principal aims is to show through the unifying principles of abstract DP and the algorithmic ideas of Newton's method that the AlphaZero/TD-Gammon ideas apply very broadly to deterministic and stochastic optimal control problems, involving both discrete and continuous search spaces. Moreover, these ideas can  be effectively integrated with other important methodologies such as model predictive control, adaptive control, decentralized control, discrete and Bayesian optimization, neural network-based value and policy approximations, and heuristic algorithms for discrete optimization.},
  editortype = {director}
}

@online{dinanAnticipatingSafetyIssues2021,
  title = {Anticipating {{Safety Issues}} in {{E2E Conversational AI}}: {{Framework}} and {{Tooling}}},
  shorttitle = {Anticipating {{Safety Issues}} in {{E2E Conversational AI}}},
  author = {Dinan, Emily and Abercrombie, Gavin and Bergman, A. Stevie and Spruit, Shannon and Hovy, Dirk and Boureau, Y.-Lan and Rieser, Verena},
  date = {2021-07-23},
  number = {arXiv:2107.03451},
  eprint = {arXiv:2107.03451},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.03451},
  urldate = {2022-12-05},
  abstract = {Over the last several years, end-to-end neural conversational agents have vastly improved in their ability to carry a chit-chat conversation with humans. However, these models are often trained on large datasets from the internet, and as a result, may learn undesirable behaviors from this data, such as toxic or otherwise harmful language. Researchers must thus wrestle with the issue of how and when to release these models. In this paper, we survey the problem landscape for safety for end-to-end conversational AI and discuss recent and related work. We highlight tensions between values, potential positive impact and potential harms, and provide a framework for making decisions about whether and how to release these models, following the tenets of value-sensitive design. We additionally provide a suite of tools to enable researchers to make better-informed decisions about training and releasing end-to-end conversational AI models.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/DVA4G9PJ/Dinan et al. - 2021 - Anticipating Safety Issues in E2E Conversational A.pdf;/Users/lukakuma/Zotero/storage/KCUCBPR7/2107.html}
}

@online{dinanWizardWikipediaKnowledgePowered2019,
  title = {Wizard of {{Wikipedia}}: {{Knowledge-Powered Conversational}} Agents},
  shorttitle = {Wizard of {{Wikipedia}}},
  author = {Dinan, Emily and Roller, Stephen and Shuster, Kurt and Fan, Angela and Auli, Michael and Weston, Jason},
  date = {2019-02-21},
  number = {arXiv:1811.01241},
  eprint = {arXiv:1811.01241},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1811.01241},
  urldate = {2022-12-06},
  abstract = {In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically "generate and hope" generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/GZU7T34R/Dinan et al. - 2019 - Wizard of Wikipedia Knowledge-Powered Conversatio.pdf;/Users/lukakuma/Zotero/storage/D7BYCV59/1811.html}
}

@unpublished{dingCognitiveGraphMultiHop2019,
  title = {Cognitive {{Graph}} for {{Multi-Hop Reading Comprehension}} at {{Scale}}},
  author = {Ding, Ming and Zhou, Chang and Chen, Qibin and Yang, Hongxia and Tang, Jie},
  date = {2019-06-04},
  eprint = {1905.05460},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1905.05460},
  urldate = {2022-04-15},
  abstract = {We propose a new CogQA framework for multi-hop question answering in web-scale documents. Inspired by the dual process theory in cognitive science, the framework gradually builds a \textbackslash textit\{cognitive graph\} in an iterative process by coordinating an implicit extraction module (System 1) and an explicit reasoning module (System 2). While giving accurate answers, our framework further provides explainable reasoning paths. Specifically, our implementation based on BERT and graph neural network efficiently handles millions of documents for multi-hop reasoning questions in the HotpotQA fullwiki dataset, achieving a winning joint \$F\_1\$ score of 34.9 on the leaderboard, compared to 23.6 of the best competitor.},
  file = {/Users/lukakuma/Zotero/storage/8RJ4YQQJ/Ding et al. - 2019 - Cognitive Graph for Multi-Hop Reading Comprehensio.pdf;/Users/lukakuma/Zotero/storage/CDWI9JYM/1905.html}
}

@online{dingCogView2FasterBetter2022,
  title = {{{CogView2}}: {{Faster}} and {{Better Text-to-Image Generation}} via {{Hierarchical Transformers}}},
  shorttitle = {{{CogView2}}},
  author = {Ding, Ming and Zheng, Wendi and Hong, Wenyi and Tang, Jie},
  date = {2022-05-27},
  number = {arXiv:2204.14217},
  eprint = {arXiv:2204.14217},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.14217},
  urldate = {2022-08-08},
  abstract = {The development of the transformer-based text-to-image models are impeded by its slow generation and complexity for high-resolution images. In this work, we put forward a solution based on hierarchical transformers and local parallel auto-regressive generation. We pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, Cross-modal general language model (CogLM), and finetune it for fast super-resolution. The new text-to-image system, CogView2, shows very competitive generation compared to concurrent state-of-the-art DALL-E-2, and naturally supports interactive text-guided editing on images.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/7VB79VAV/Ding et al. - 2022 - CogView2 Faster and Better Text-to-Image Generati.pdf;/Users/lukakuma/Zotero/storage/9B3RRWKJ/2204.html}
}

@unpublished{dingCogViewMasteringTexttoImage2021,
  title = {{{CogView}}: {{Mastering Text-to-Image Generation}} via {{Transformers}}},
  shorttitle = {{{CogView}}},
  author = {Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and Tang, Jie},
  date = {2021-11-05},
  eprint = {2105.13290},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2105.13290},
  urldate = {2022-04-13},
  abstract = {Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.},
  file = {/Users/lukakuma/Zotero/storage/7PBC2ZMT/Ding et al. - 2021 - CogView Mastering Text-to-Image Generation via Tr.pdf;/Users/lukakuma/Zotero/storage/KB9SZWJM/2105.html}
}

@article{diskinDistributedDeepLearning2021,
  title = {Distributed {{Deep Learning}} in {{Open Collaborations}}},
  author = {Diskin, Michael and Bukhtiyarov, Alexey and Ryabinin, Max and Saulnier, Lucile and Lhoest, Quentin and Sinitsin, Anton and Popov, Dmitry and Pyrkin, Dmitry and Kashirin, Maxim and Borzunov, Alexander and family=Moral, given=Albert Villanova, prefix=del, useprefix=true and Mazur, Denis and Kobelev, Ilia and Jernite, Yacine and Wolf, Thomas and Pekhimenko, Gennady},
  date = {2021-06-18},
  url = {https://arxiv.org/abs/2106.10207v2},
  urldate = {2022-03-08},
  abstract = {Modern deep learning applications require increasingly more compute to train state-of-the-art models. To address this demand, large corporations and institutions use dedicated High-Performance Computing clusters, whose construction and maintenance are both environmentally costly and well beyond the budget of most organizations. As a result, some research directions become the exclusive domain of a few large industrial and even fewer academic actors. To alleviate this disparity, smaller groups may pool their computational resources and run collaborative experiments that benefit all participants. This paradigm, known as grid- or volunteer computing, has seen successful applications in numerous scientific areas. However, using this approach for machine learning is difficult due to high latency, asymmetric bandwidth, and several challenges unique to volunteer computing. In this work, we carefully analyze these constraints and propose a novel algorithmic framework designed specifically for collaborative training. We demonstrate the effectiveness of our approach for SwAV and ALBERT pretraining in realistic conditions and achieve performance comparable to traditional setups at a fraction of the cost. Finally, we provide a detailed report of successful collaborative language model pretraining with 40 participants.},
  langid = {english},
  keywords = {*open source AI},
  file = {/Users/lukakuma/Zotero/storage/7FENMM7U/Diskin et al. - 2021 - Distributed Deep Learning in Open Collaborations.pdf;/Users/lukakuma/Zotero/storage/2SM6CENQ/2106.html}
}

@article{dociDNAScannerWeb2020,
  title = {{{DNA Scanner}}: A Web Application for Comparing {{DNA}} Synthesis Feasibility, Price and Turnaround Time across Vendors},
  shorttitle = {{{DNA Scanner}}},
  author = {Do\c{c}i, Gledon and Fuchs, Lukas and Kharbanda, Yash and Schickling, Paul and Zulkower, Valentin and Hillson, Nathan and Oberortner, Ernst and Swainston, Neil and Kabisch, Johannes},
  date = {2020-01-01},
  journaltitle = {Synthetic Biology},
  shortjournal = {Synthetic Biology},
  volume = {5},
  number = {1},
  pages = {ysaa011},
  issn = {2397-7000},
  url = {https://doi.org/10.1093/synbio/ysaa011},
  urldate = {2022-05-25},
  abstract = {DNA synthesis has become a major enabler of modern bioengineering, allowing scientists to simply order online in silico-designed DNA molecules. Rapidly decreasing DNA synthesis service prices and the concomitant increase of research and development scales bolstered by computer-aided DNA design tools and laboratory automation has driven up the demand for synthetic DNA. While vendors provide user-friendly online portals for purchasing synthetic DNA, customers still face the time-consuming task of checking each vendor of choice for their ability and pricing to synthesize the desired sequences. As a result, ordering large batches of DNA sequences can be a laborious manual procedure in an otherwise increasingly automatable workflow. Even when they are available, there is a high degree of technical knowledge and effort required to integrate vendors' application programming interfaces (APIs) into computer-aided DNA design tools or automated lab processes. Here, we introduce DNA Scanner, a software package comprising (i) a web-based user interface enabling users to compare the feasibility, price and turnaround time of synthetic DNA sequences across selected vendors and (ii) a Python API enabling integration of these functionalities into computer-aided DNA design tools and automated lab processes. We have developed DNA Scanner to uniformly streamline interactions between synthetic DNA vendors, members of the Global Biofoundry Alliance and the scientific community at large.},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/HYUE4MGS/Do√ßi et al. - 2020 - DNA Scanner a web application for comparing DNA s.pdf;/Users/lukakuma/Zotero/storage/E6IWNRFT/5892266.html}
}

@online{dodgeDocumentingLargeWebtext2021,
  title = {Documenting {{Large Webtext Corpora}}: {{A Case Study}} on the {{Colossal Clean Crawled Corpus}}},
  shorttitle = {Documenting {{Large Webtext Corpora}}},
  author = {Dodge, Jesse and Sap, Maarten and Marasovi\'c, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
  date = {2021-09-30},
  number = {arXiv:2104.08758},
  eprint = {arXiv:2104.08758},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.08758},
  urldate = {2022-11-02},
  abstract = {Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.},
  langid = {english},
  pubstate = {preprint},
  keywords = {pretraining},
  file = {/Users/lukakuma/Zotero/storage/MJCNSCTU/Dodge et al. - 2021 - Documenting Large Webtext Corpora A Case Study on.pdf}
}

@article{doellerEvidenceGridCells2010,
  title = {Evidence for Grid Cells in a Human Memory Network},
  author = {Doeller, Christian F. and Barry, Caswell and Burgess, Neil},
  date = {2010-02},
  journaltitle = {Nature},
  volume = {463},
  number = {7281},
  pages = {657--661},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/nature08704},
  urldate = {2022-06-18},
  abstract = {The discovery by Edvard Moser and colleagues that rats and mice possess an orientation map of their surroundings, produced and updated by a network of cerebral cortex neurons known as 'grid cells' was one of the most exciting neuroscientific findings in recent years. These cells provide a strikingly periodic representation of self-location. The question naturally arises, does a similar mechanism operate in humans? The answer is provided in a paper by Christian Doeller, Caswell Barry and Neil Burgess in which single-unit recordings of grid cells in freely moving rats were combined with whole-brain functional magnetic resonance imaging (fMRI) in humans navigating within virtual environments. Doeller et al. were able to detect a macroscopic fMRI signal representing a subject's position in a virtual reality environment that met the criteria for defining grid-cell encoding. Thus, humans appear to represent position and support spatial cognition in a manner very like that used by rodents.},
  issue = {7281},
  langid = {english},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/6AEGRTUF/annurev-neuro-062111-150343.pdf;/Users/lukakuma/Zotero/storage/CGYJDDSP/annurev.neuro.28.061604.135703.pdf;/Users/lukakuma/Zotero/storage/MMICK97G/Doeller et al. - 2010 - Evidence for grid cells in a human memory network.pdf;/Users/lukakuma/Zotero/storage/5SL7ELSN/nature08704.html}
}

@online{dogninReGenReinforcementLearning2021,
  title = {{{ReGen}}: {{Reinforcement Learning}} for {{Text}} and {{Knowledge Base Generation}} Using {{Pretrained Language Models}}},
  shorttitle = {{{ReGen}}},
  author = {Dognin, Pierre L. and Padhi, Inkit and Melnyk, Igor and Das, Payel},
  date = {2021-08-27},
  number = {arXiv:2108.12472},
  eprint = {arXiv:2108.12472},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.12472},
  urldate = {2023-02-20},
  abstract = {Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/ZWF9IKR5/Dognin et al. - 2021 - ReGen Reinforcement Learning for Text and Knowled.pdf;/Users/lukakuma/Zotero/storage/SBLIVDBY/2108.html}
}

@online{dohanLanguageModelCascades2022,
  title = {Language {{Model Cascades}}},
  author = {Dohan, David and Xu, Winnie and Lewkowycz, Aitor and Austin, Jacob and Bieber, David and Lopes, Raphael Gontijo and Wu, Yuhuai and Michalewski, Henryk and Saurous, Rif A. and Sohl-dickstein, Jascha and Murphy, Kevin and Sutton, Charles},
  date = {2022-07-21},
  number = {arXiv:2207.10342},
  eprint = {arXiv:2207.10342},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.10342},
  urldate = {2022-07-23},
  abstract = {Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow,5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/Q8IWE4DI/Dohan et al. - 2022 - Language Model Cascades.pdf;/Users/lukakuma/Zotero/storage/VY68Z6VL/2207.html}
}

@online{don-yehiyaColDFusionCollaborative2022,
  title = {{{ColD Fusion}}: {{Collaborative Descent}} for {{Distributed Multitask Finetuning}}},
  shorttitle = {{{ColD Fusion}}},
  author = {Don-Yehiya, Shachar and Venezian, Elad and Raffel, Colin and Slonim, Noam and Katz, Yoav and Choshen, Leshem},
  date = {2022-12-02},
  number = {arXiv:2212.01378},
  eprint = {arXiv:2212.01378},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.01378},
  urldate = {2022-12-12},
  abstract = {Pretraining has been shown to scale well with compute, data size and data diversity. Multitask learning trains on a mixture of supervised datasets and produces improved performance compared to self-supervised pretraining. Until now, massively multitask learning required simultaneous access to all datasets in the mixture and heavy compute resources that are only available to well-resourced teams. In this paper, we propose ColD Fusion, a method that provides the benefits of multitask learning but leverages distributed computation and requires limited communication and no sharing of data. Consequentially, ColD Fusion can create a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based on. We show that ColD Fusion yields comparable benefits to multitask pretraining by producing a model that (a) attains strong performance on all of the datasets it was multitask trained on and (b) is a better starting point for finetuning on unseen datasets. We find ColD Fusion outperforms RoBERTa and even previous multitask models. Specifically, when training and testing on 35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.45 points in average without any changes to the architecture.},
  pubstate = {preprint},
  keywords = {model fusion,read},
  file = {/Users/lukakuma/Zotero/storage/UR4G7LHR/Don-Yehiya et al. - 2022 - ColD Fusion Collaborative Descent for Distributed.pdf;/Users/lukakuma/Zotero/storage/PDVKXHBG/2212.html}
}

@online{dongSurveyIncontextLearning2022,
  title = {A {{Survey}} for {{In-context Learning}}},
  author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Li, Lei and Sui, Zhifang},
  date = {2022-12-31},
  number = {arXiv:2301.00234},
  eprint = {arXiv:2301.00234},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.00234},
  urldate = {2023-01-04},
  abstract = {With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few training examples. It has been a new trend exploring ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress, challenges, and future work in ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques of ICL, including training strategies, prompting strategies, and so on. Finally, we present the challenges of ICL and provide potential directions for further research. We hope our work can encourage more research on uncovering how ICL works and improving ICL in future work.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/QG9YGBXA/Dong et al. - 2022 - A Survey for In-context Learning.pdf;/Users/lukakuma/Zotero/storage/5RMLCI2E/2301.html}
}

@unpublished{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2022-04-20},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/P2AVJGNH/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf;/Users/lukakuma/Zotero/storage/WAN9CXTM/Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf;/Users/lukakuma/Zotero/storage/NH9W3QZW/2010.html}
}

@unpublished{downsGoogleScannedObjects2022,
  title = {Google {{Scanned Objects}}: {{A High-Quality Dataset}} of {{3D Scanned Household Items}}},
  shorttitle = {Google {{Scanned Objects}}},
  author = {Downs, Laura and Francis, Anthony and Koenig, Nate and Kinman, Brandon and Hickman, Ryan and Reymann, Krista and McHugh, Thomas B. and Vanhoucke, Vincent},
  date = {2022-04-25},
  eprint = {2204.11918},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.11918},
  urldate = {2022-04-27},
  abstract = {Interactive 3D simulations have enabled breakthroughs in robotics and computer vision, but simulating the broad diversity of environments needed for deep learning requires large corpora of photo-realistic 3D object models. To address this need, we present Google Scanned Objects, an open-source collection of over one thousand 3D-scanned household items released under a Creative Commons license; these models are preprocessed for use in Ignition Gazebo and the Bullet simulation platforms, but are easily adaptable to other simulators. We describe our object scanning and curation pipeline, then provide statistics about the contents of the dataset and its usage. We hope that the diversity, quality, and flexibility of Google Scanned Objects will lead to advances in interactive simulation, synthetic perception, and robotic learning.},
  file = {/Users/lukakuma/Zotero/storage/5HCW76IR/Downs et al. - 2022 - Google Scanned Objects A High-Quality Dataset of .pdf;/Users/lukakuma/Zotero/storage/JR7F649H/2204.html}
}

@article{droriNeuralNetworkSolves2022,
  title = {A {{Neural Network Solves}}, {{Explains}}, and {{Generates University Math Problems}} by {{Program Synthesis}} and {{Few-Shot Learning}} at {{Human Level}}},
  author = {Drori, Iddo and Zhang, Sarah and Shuttleworth, Reece and Tang, Leonard and Lu, Albert and Ke, Elizabeth and Liu, Kevin and Chen, Linda and Tran, Sunny and Cheng, Newman and Wang, Roman and Singh, Nikhil and Patti, Taylor L. and Lynch, Jayson and Shporer, Avi and Verma, Nakul and Wu, Eugene and Strang, Gilbert},
  date = {2022-08-09},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {119},
  number = {32},
  eprint = {2112.15594},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {e2123433119},
  issn = {0027-8424, 1091-6490},
  url = {http://arxiv.org/abs/2112.15594},
  urldate = {2023-02-20},
  abstract = {We demonstrate that a neural network pre-trained on text and fine-tuned on code solves mathematics course problems, explains solutions, and generates new questions at a human level. We automatically synthesize programs using few-shot learning and OpenAI's Codex transformer and execute them to solve course problems at 81\% automatic accuracy. We curate a new dataset of questions from MIT's largest mathematics courses (Single Variable and Multivariable Calculus, Differential Equations, Introduction to Probability and Statistics, Linear Algebra, and Mathematics for Computer Science) and Columbia University's Computational Linear Algebra. We solve questions from a MATH dataset (on Prealgebra, Algebra, Counting and Probability, Intermediate Algebra, Number Theory, and Precalculus), the latest benchmark of advanced mathematics problems designed to assess mathematical reasoning. We randomly sample questions and generate solutions with multiple modalities, including numbers, equations, and plots. The latest GPT-3 language model pre-trained on text automatically solves only 18.8\% of these university questions using zero-shot learning and 30.8\% using few-shot learning and the most recent chain of thought prompting. In contrast, program synthesis with few-shot learning using Codex fine-tuned on code generates programs that automatically solve 81\% of these questions. Our approach improves the previous state-of-the-art automatic solution accuracy on the benchmark topics from 8.8\% to 81.1\%. We perform a survey to evaluate the quality and difficulty of generated questions. This work is the first to automatically solve university-level mathematics course questions at a human level and the first work to explain and generate university-level mathematics course questions at scale, a milestone for higher education.},
  keywords = {*general tools,formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/9SQ5IWH4/Drori et al. - 2022 - A Neural Network Solves, Explains, and Generates U.pdf;/Users/lukakuma/Zotero/storage/UEQVFSNC/Drori et al. - 2022 - A neural network solves, explains, and generates u.pdf;/Users/lukakuma/Zotero/storage/Z8S4DTIT/2112.html}
}

@online{drozdovCompositionalSemanticParsing2022a,
  title = {Compositional {{Semantic Parsing}} with {{Large Language Models}}},
  author = {Drozdov, Andrew and Sch\"arli, Nathanael and Aky\"urek, Ekin and Scales, Nathan and Song, Xinying and Chen, Xinyun and Bousquet, Olivier and Zhou, Denny},
  date = {2022-09-29},
  number = {arXiv:2209.15003},
  eprint = {arXiv:2209.15003},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.15003},
  urldate = {2023-02-20},
  abstract = {Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1\% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/7LUA9DPF/Drozdov et al. - 2022 - Compositional Semantic Parsing with Large Language.pdf;/Users/lukakuma/Zotero/storage/L98XITN6/2209.html}
}

@article{duanBenchmarkingDeepReinforcement2016,
  title = {Benchmarking {{Deep Reinforcement Learning}} for {{Continuous Control}}},
  author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  date = {2016-04-22},
  url = {https://arxiv.org/abs/1604.06778v3},
  urldate = {2022-07-19},
  abstract = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/8MZ8SQCW/Duan et al. - 2016 - Benchmarking Deep Reinforcement Learning for Conti.pdf;/Users/lukakuma/Zotero/storage/TY3URV3E/1604.html}
}

@online{duanOneShotImitationLearning2017,
  title = {One-{{Shot Imitation Learning}}},
  author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly C. and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  date = {2017-12-04},
  number = {arXiv:1703.07326},
  eprint = {arXiv:1703.07326},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.07326},
  urldate = {2022-07-23},
  abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. Videos available at https://bit.ly/nips2017-oneshot .},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/XFKLC9ZN/Duan et al. - 2017 - One-Shot Imitation Learning.pdf;/Users/lukakuma/Zotero/storage/XTDDW6J6/1703.html}
}

@online{duaSuccessivePromptingDecomposing2022,
  title = {Successive {{Prompting}} for {{Decomposing Complex Questions}}},
  author = {Dua, Dheeru and Gupta, Shivanshu and Singh, Sameer and Gardner, Matt},
  date = {2022-12-08},
  number = {arXiv:2212.04092},
  eprint = {arXiv:2212.04092},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.04092},
  urldate = {2023-02-20},
  abstract = {Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce ``Successive Prompting'', where we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate a synthetic dataset which can be used to bootstrap a model's ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement of \textasciitilde 5\% absolute F1 on a few-shot version of the DROP dataset when compared with a state-of-the-art model with the same supervision.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/4V6F9ESX/Dua et al. - 2022 - Successive Prompting for Decomposing Complex Quest.pdf;/Users/lukakuma/Zotero/storage/TG7JVRUM/2212.html}
}

@article{duCogKRCognitiveGraph2021,
  title = {{{CogKR}}: {{Cognitive Graph}} for {{Multi-hop Knowledge Reasoning}}},
  shorttitle = {{{CogKR}}},
  author = {Du, Zhengxiao and Zhou, Chang and Yao, Jiangchao and Tu, Teng and Cheng, Letian and Yang, Hongxia and Zhou, Jingren and Tang, Jie},
  date = {2021-08-01},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  number = {01},
  pages = {1--1},
  publisher = {{IEEE Computer Society}},
  issn = {1041-4347},
  url = {https://www.computer.org/csdl/journal/tk/5555/01/09512424/1w0wzCuvnA4},
  urldate = {2022-04-15},
  abstract = {Inferring new facts from an existing knowledge graph with explainable reasoning processes is an important problem, known as knowledge graph (KG) reasoning. The problem is often formulated as finding the specific path that represents the query relation and connects the query entity and the correct answer. However, due to the limited expressiveness of individual paths, the majority of previous works failed to capture the complex subgraph structure in the graph. We propose CogKR that traverses the knowledge graph to conduct multi-hop reasoning. More specifically, motivated by the dual process theory from cognitive science, our framework is composed of an extension module and a reasoning module. By setting up a cognitive graph through iteratively coordinating the two modules, CogKR can cope with more complex reasoning scenarios in the form of subgraphs instead of individual paths. Experiments on three knowledge graph reasoning benchmarks demonstrate that CogKR achieves significant improvements in accuracy compared with previous methods while providing the explainable capacity. Moreover, we evaluate CogKR on the challenging one-shot link prediction task, exhibiting the superiority of the framework on accuracy and scalability compared to the state-of-the-art approaches.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/R4FVDBST/1w0wzCuvnA4.html}
}

@article{dudleyBiofoundryassistedExpressionCharacterization2021,
  title = {Biofoundry-Assisted Expression and Characterization of Plant Proteins},
  author = {Dudley, Quentin M and Cai, Yao-Min and Kallam, Kalyani and Debreyne, Hubert and Carrasco Lopez, Jose A and Patron, Nicola J},
  date = {2021-10-01},
  journaltitle = {Synthetic Biology},
  shortjournal = {Synthetic Biology},
  volume = {6},
  number = {1},
  pages = {ysab029},
  issn = {2397-7000},
  url = {https://doi.org/10.1093/synbio/ysab029},
  urldate = {2022-05-25},
  abstract = {Many goals in synthetic biology, including the elucidation and refactoring of biosynthetic pathways and the engineering of regulatory circuits and networks, require knowledge of protein function. In plants, the prevalence of large gene families means it can be particularly challenging to link specific functions to individual proteins. However, protein characterization has remained a technical bottleneck, often requiring significant effort to optimize expression and purification protocols. To leverage the ability of biofoundries to accelerate design\textendash built\textendash test\textendash learn cycles, we present a workflow for automated DNA assembly and cell-free expression of plant proteins that accelerates optimization and enables rapid screening of enzyme activity. First, we developed a phytobrick-compatible Golden Gate DNA assembly toolbox containing plasmid acceptors for cell-free expression using Escherichiacoli or wheat germ lysates as well as a set of N- and C-terminal tag parts for detection, purification and improved expression/folding. We next optimized automated assembly of miniaturized cell-free reactions using an acoustic liquid handling platform and then compared tag configurations to identify those that increase expression. We additionally developed a luciferase-based system for rapid quantification that requires a minimal 11\textendash amino acid tag and demonstrate facile removal of tags following synthesis. Finally, we show that several functional assays can be performed with cell-free protein synthesis reactions without the need for protein purification. Together, the combination of automated assembly of DNA parts and cell-free expression reactions should significantly increase the throughput of experiments to test and understand plant protein function and enable the direct reuse of DNA parts in downstream plant engineering workflows.},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/6VL93CXR/Dudley et al. - 2021 - Biofoundry-assisted expression and characterizatio.pdf;/Users/lukakuma/Zotero/storage/7IZHWSMD/6369019.html}
}

@online{dudzikGraphNeuralNetworks2022,
  title = {Graph {{Neural Networks}} Are {{Dynamic Programmers}}},
  author = {Dudzik, Andrew and Veli\v{c}kovi\'c, Petar},
  date = {2022-10-10},
  number = {arXiv:2203.15544},
  eprint = {arXiv:2203.15544},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.15544},
  urldate = {2022-11-14},
  abstract = {Recent advances in neural algorithmic reasoning with graph neural networks (GNNs) are propped up by the notion of algorithmic alignment. Broadly, a neural network will be better at learning to execute a reasoning task (in terms of sample complexity) if its individual components align well with the target algorithm. Specifically, GNNs are claimed to align with dynamic programming (DP), a general problem-solving strategy which expresses many polynomial-time algorithms. However, has this alignment truly been demonstrated and theoretically quantified? Here we show, using methods from category theory and abstract algebra, that there exists an intricate connection between GNNs and DP, going well beyond the initial observations over individual algorithms such as Bellman-Ford. Exposing this connection, we easily verify several prior findings in the literature, produce better-grounded GNN architectures for edge-centric tasks, and demonstrate empirical results on the CLRS algorithmic reasoning benchmark. We hope our exposition will serve as a foundation for building stronger algorithmically aligned GNNs.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/BFNBBJQZ/Dudzik and Veliƒçkoviƒá - 2022 - Graph Neural Networks are Dynamic Programmers.pdf;/Users/lukakuma/Zotero/storage/6KBXRKBQ/2203.html}
}

@online{duGLaMEfficientScaling2022,
  title = {{{GLaM}}: {{Efficient Scaling}} of {{Language Models}} with {{Mixture-of-Experts}}},
  shorttitle = {{{GLaM}}},
  author = {Du, Nan and Huang, Yanping and Dai, Andrew M. and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten and Zhou, Zongwei and Wang, Tao and Wang, Yu Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathleen and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  date = {2022-08-01},
  number = {arXiv:2112.06905},
  eprint = {arXiv:2112.06905},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.06905},
  urldate = {2022-10-18},
  abstract = {Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.},
  pubstate = {preprint},
  keywords = {compute efficient method,read,sparsity},
  file = {/Users/lukakuma/Zotero/storage/F5QRZXTG/Du et al. - 2022 - GLaM Efficient Scaling of Language Models with Mi.pdf;/Users/lukakuma/Zotero/storage/XNXU43MZ/2112.html}
}

@online{duGLMGeneralLanguage2022,
  title = {{{GLM}}: {{General Language Model Pretraining}} with {{Autoregressive Blank Infilling}}},
  shorttitle = {{{GLM}}},
  author = {Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  date = {2022-03-17},
  number = {arXiv:2103.10360},
  eprint = {arXiv:2103.10360},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.10360},
  urldate = {2022-08-08},
  abstract = {There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25x parameters of BERT Large , demonstrating its generalizability to different downstream tasks.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/L2ACP8WF/Du et al. - 2022 - GLM General Language Model Pretraining with Autor.pdf;/Users/lukakuma/Zotero/storage/S573ADIX/2103.html}
}

@online{duLearningSignalAgnosticManifolds2021,
  title = {Learning {{Signal-Agnostic Manifolds}} of {{Neural Fields}}},
  author = {Du, Yilun and Collins, Katherine M. and Tenenbaum, Joshua B. and Sitzmann, Vincent},
  date = {2021-11-11},
  number = {arXiv:2111.06387},
  eprint = {arXiv:2111.06387},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.06387},
  urldate = {2022-08-29},
  abstract = {Deep neural networks have been used widely to learn the latent structure of datasets, across modalities such as images, shapes, and audio signals. However, existing models are generally modality-dependent, requiring custom architectures and objectives to process different classes of signals. We leverage neural fields to capture the underlying structure in image, shape, audio and cross-modal audiovisual domains in a modality-independent manner. We cast our task as one of learning a manifold, where we aim to infer a low-dimensional, locally linear subspace in which our data resides. By enforcing coverage of the manifold, local linearity, and local isometry, our model -- dubbed GEM -- learns to capture the underlying structure of datasets across modalities. We can then travel along linear regions of our manifold to obtain perceptually consistent interpolations between samples, and can further use GEM to recover points on our manifold and glean not only diverse completions of input images, but cross-modal hallucinations of audio or image signals. Finally, we show that by walking across the underlying manifold of GEM, we may generate new samples in our signal domains. Code and additional results are available at https://yilundu.github.io/gem/.},
  pubstate = {preprint},
  keywords = {functa},
  file = {/Users/lukakuma/Zotero/storage/LRU4MH92/Du et al. - 2021 - Learning Signal-Agnostic Manifolds of Neural Field.pdf;/Users/lukakuma/Zotero/storage/PFG5R42U/2111.html}
}

@online{dupontCOINNeuralCompression2022,
  title = {{{COIN}}++: {{Neural Compression Across Modalities}}},
  shorttitle = {{{COIN}}++},
  author = {Dupont, Emilien and Loya, Hrushikesh and Alizadeh, Milad and Goli\'nski, Adam and Teh, Yee Whye and Doucet, Arnaud},
  date = {2022-06-10},
  number = {arXiv:2201.12904},
  eprint = {arXiv:2201.12904},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.12904},
  urldate = {2022-08-29},
  abstract = {Neural compression algorithms are typically based on autoencoders that require specialized encoder and decoder architectures for different data modalities. In this paper, we propose COIN++, a neural compression framework that seamlessly handles a wide range of data modalities. Our approach is based on converting data to implicit neural representations, i.e. neural functions that map coordinates (such as pixel locations) to features (such as RGB values). Then, instead of storing the weights of the implicit neural representation directly, we store modulations applied to a meta-learned base network as a compressed code for the data. We further quantize and entropy code these modulations, leading to large compression gains while reducing encoding time by two orders of magnitude compared to baselines. We empirically demonstrate the effectiveness of our method by compressing various data modalities, from images and audio to medical and climate data.},
  pubstate = {preprint},
  keywords = {functa},
  file = {/Users/lukakuma/Zotero/storage/PACH8FM5/Dupont et al. - 2022 - COIN++ Neural Compression Across Modalities.pdf;/Users/lukakuma/Zotero/storage/68PRHTD9/2201.html}
}

@unpublished{dupontDataFunctaYour2022,
  title = {From Data to Functa: {{Your}} Data Point Is a Function and You Can Treat It like One},
  shorttitle = {From Data to Functa},
  author = {Dupont, Emilien and Kim, Hyunjik and Eslami, S. M. Ali and Rezende, Danilo and Rosenbaum, Dan},
  date = {2022-06-06},
  number = {arXiv:2201.12204},
  eprint = {2201.12204},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.12204},
  urldate = {2022-06-09},
  abstract = {It is common practice in deep learning to represent a measurement of the world on a discrete grid, e.g. a 2D grid of pixels. However, the underlying signal represented by these measurements is often continuous, e.g. the scene depicted in an image. A powerful continuous alternative is then to represent these measurements using an implicit neural representation, a neural function trained to output the appropriate measurement value for any input spatial location. In this paper, we take this idea to its next level: what would it take to perform deep learning on these functions instead, treating them as data? In this context we refer to the data as functa, and propose a framework for deep learning on functa. This view presents a number of challenges around efficient conversion from data to functa, compact representation of functa, and effectively solving downstream tasks on functa. We outline a recipe to overcome these challenges and apply it to a wide range of data modalities including images, 3D shapes, neural radiance fields (NeRF) and data on manifolds. We demonstrate that this approach has various compelling properties across data modalities, in particular on the canonical tasks of generative modeling, data imputation, novel view synthesis and classification.},
  keywords = {functa},
  file = {/Users/lukakuma/Zotero/storage/4UAWSVFU/Dupont et al. - 2022 - From data to functa Your data point is a function.pdf;/Users/lukakuma/Zotero/storage/A8WP3UGX/2201.html}
}

@article{dupontGenerativeModelsDistributions2021,
  title = {Generative {{Models}} as {{Distributions}} of {{Functions}}},
  author = {Dupont, Emilien and Teh, Yee Whye and Doucet, Arnaud},
  date = {2021-02-09},
  url = {https://arxiv.org/abs/2102.04776v4},
  urldate = {2022-08-29},
  abstract = {Generative models are typically trained on grid-like data such as images. As a result, the size of these models usually scales directly with the underlying grid resolution. In this paper, we abandon discretized grids and instead parameterize individual data points by continuous functions. We then build generative models by learning distributions over such functions. By treating data points as functions, we can abstract away from the specific type of data we train on and construct models that are agnostic to discretization. To train our model, we use an adversarial approach with a discriminator that acts on continuous signals. Through experiments on a wide variety of data modalities including images, 3D shapes and climate data, we demonstrate that our model can learn rich distributions of functions independently of data type and resolution.},
  langid = {english},
  keywords = {functa},
  file = {/Users/lukakuma/Zotero/storage/V2D4TS5E/Dupont et al. - 2021 - Generative Models as Distributions of Functions.pdf;/Users/lukakuma/Zotero/storage/N96X2IEF/2102.html}
}

@unpublished{duSurveyVisionLanguagePreTrained2022,
  title = {A {{Survey}} of {{Vision-Language Pre-Trained Models}}},
  author = {Du, Yifan and Liu, Zikang and Li, Junyi and Zhao, Wayne Xin},
  date = {2022-02-18},
  eprint = {2202.10936},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.10936},
  urldate = {2022-03-31},
  abstract = {As Transformer evolved, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre-training to the field of Vision-and-Language (V-L) learning and improve the performance on downstream tasks becomes a focus of multimodal learning. In this paper, we review the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the core content, we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image representations. We further present widely-used pre-training tasks, after which we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide multimodal researchers a synthesis and pointer to related research.},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/Q5WHBZAK/Du et al. - 2022 - A Survey of Vision-Language Pre-Trained Models.pdf;/Users/lukakuma/Zotero/storage/4KEPAKLZ/2202.html}
}

@unpublished{dwivediBenchmarkingGraphNeural2020,
  title = {Benchmarking {{Graph Neural Networks}}},
  author = {Dwivedi, Vijay Prakash and Joshi, Chaitanya K. and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  date = {2020-07-03},
  eprint = {2003.00982},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.00982},
  urldate = {2022-03-28},
  abstract = {Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/7UZW85K4/Dwivedi et al. - 2020 - Benchmarking Graph Neural Networks.pdf;/Users/lukakuma/Zotero/storage/LW2JANH7/2003.html}
}

@online{dwivediGeneralizationTransformerNetworks2021,
  title = {A {{Generalization}} of {{Transformer Networks}} to {{Graphs}}},
  author = {Dwivedi, Vijay Prakash and Bresson, Xavier},
  date = {2021-01-24},
  number = {arXiv:2012.09699},
  eprint = {arXiv:2012.09699},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.09699},
  urldate = {2022-08-08},
  abstract = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/B9TJY5ED/Dwivedi and Bresson - 2021 - A Generalization of Transformer Networks to Graphs.pdf;/Users/lukakuma/Zotero/storage/APWN84Q7/2012.html}
}

@book{easleyNetworksCrowdsMarkets2010,
  title = {Networks, {{Crowds}}, and {{Markets}}: {{A Book}} by {{David Easley}} and {{Jon Kleinberg}}},
  author = {Easley, David and Kleinberg, Jon},
  date = {2010},
  url = {http://www.cs.cornell.edu/home/kleinber/networks-book/},
  urldate = {2021-04-13},
  file = {/Users/lukakuma/Zotero/storage/27ALWSDU/Easley and Kleinberg - 2010 - Networks, Crowds, and Markets A Book by David Eas.pdf;/Users/lukakuma/Zotero/storage/24NFE964/networks-book.html}
}

@online{eichenbergMAGMAMultimodalAugmentation2022,
  title = {{{MAGMA}} -- {{Multimodal Augmentation}} of {{Generative Models}} through {{Adapter-based Finetuning}}},
  author = {Eichenberg, Constantin and Black, Sidney and Weinbach, Samuel and Parcalabescu, Letitia and Frank, Anette},
  date = {2022-10-24},
  number = {arXiv:2112.05253},
  eprint = {arXiv:2112.05253},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.05253},
  urldate = {2023-02-15},
  abstract = {Large-scale pretraining is fast becoming the norm in Vision-Language (VL) modeling. However, prevailing VL approaches are limited by the requirement for labeled data and the use of complex multi-step pretraining objectives. We present MAGMA - a simple method for augmenting generative language models with additional modalities using adapter-based finetuning. Building on Frozen, we train a series of VL models that autoregressively generate text from arbitrary combinations of visual and textual input. The pretraining is entirely end-to-end using a single language modeling objective, simplifying optimization compared to previous approaches. Importantly, the language model weights remain unchanged during training, allowing for transfer of encyclopedic knowledge and in-context learning abilities from language pretraining. MAGMA outperforms Frozen on open-ended generative tasks, achieving state of the art results on the OKVQA benchmark and competitive results on a range of other popular VL benchmarks, while pretraining on 0.2\% of the number of samples used to train SimVLM.},
  pubstate = {preprint},
  keywords = {2-adapter,7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/6H4FQP8J/Eichenberg et al. - 2022 - MAGMA -- Multimodal Augmentation of Generative Mod.pdf;/Users/lukakuma/Zotero/storage/C3NDPVZ8/2112.html}
}

@online{ektefaieGeometricMultimodalRepresentation2022,
  title = {Geometric Multimodal Representation Learning},
  author = {Ektefaie, Yasha and Dasoulas, George and Noori, Ayush and Farhat, Maha and Zitnik, Marinka},
  date = {2022-09-07},
  number = {arXiv:2209.03299},
  eprint = {arXiv:2209.03299},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.03299},
  urldate = {2022-09-09},
  abstract = {Graph-centric artificial intelligence (graph AI) has achieved remarkable success in modeling interacting systems prevalent in nature, from dynamical systems in biology to particle physics. The increasing heterogeneity of data calls for graph neural architectures that can combine multiple inductive biases. However, combining data from various sources is challenging because appropriate inductive bias may vary by data modality. Multimodal learning methods fuse multiple data modalities while leveraging cross-modal dependencies to address this challenge. Here, we survey 140 studies in graph-centric AI and realize that diverse data types are increasingly brought together using graphs and fed into sophisticated multimodal models. These models stratify into image-, language-, and knowledge-grounded multimodal learning. We put forward an algorithmic blueprint for multimodal graph learning based on this categorization. The blueprint serves as a way to group state-of-the-art architectures that treat multimodal data by choosing appropriately four different components. This effort can pave the way for standardizing the design of sophisticated multimodal architectures for highly complex real-world problems.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/RB8FDFVF/Ektefaie et al. - 2022 - Geometric multimodal representation learning.pdf;/Users/lukakuma/Zotero/storage/5VWQRQ3G/2209.html}
}

@article{el-hamamsyComputerScienceRobotics2021,
  title = {A Computer Science and Robotics Integration Model for Primary School: Evaluation of a Large-Scale in-Service {{K-4}} Teacher-Training Program},
  shorttitle = {A Computer Science and Robotics Integration Model for Primary School},
  author = {El-Hamamsy, Laila and Frederique, Lazzarotto and Bruno, Barbara and Roy, Didier and Cahlikova, Tereza and Chevalier, Morgane and Parriaux, Gabriel and Pellet, Jean-Philippe and Lanares, Jacques and Dehler, Jessica and Mondada, Francesco},
  date = {2021-05-01},
  journaltitle = {Education and Information Technologies},
  shortjournal = {Education and Information Technologies},
  volume = {26},
  doi = {10.1007/s10639-020-10355-5},
  abstract = {Integrating computer science (CS) into school curricula has become a worldwide preoccupation. Therefore, we present a CS and Robotics integration model and its validation through a large-scale pilot study in the administrative region of the Canton Vaud in Switzerland. Approximately 350 primary school teachers followed a mandatory CS continuing professional development program (CPD) of adapted format with a curriculum scaffolded by instruction modality. This included CS Unplugged activities that aim to teach CS concepts without the use of screens, and Robotics Unplugged activities that employed physical robots, without screens, to learn about robotics and CS concepts. Teachers evaluated positively the CPD and their representation of CS improved. Voluntary adoption rates reached 97\% during the CPD and 80\% the following year. These results combined with the underpinning literature support the generalisability of the model to other contexts.},
  file = {/Users/lukakuma/Zotero/storage/LMQL8ME4/El-Hamamsy et al. - 2021 - A computer science and robotics integration model .pdf}
}

@book{ElementsStatisticalLearning2017,
  title = {Elements of {{Statistical Learning}}: Data Mining, Inference, and Prediction. 2nd {{Edition}}.},
  date = {2017},
  url = {https://hastie.su.domains/ElemStatLearn/},
  urldate = {2022-05-11},
  file = {/Users/lukakuma/Zotero/storage/F8NE547W/Elements of Statistical Learning data mining, inf.pdf;/Users/lukakuma/Zotero/storage/2STXUFPP/ElemStatLearn.html}
}

@book{elgamalNetworkInformationTheory2011,
  title = {Network Information Theory},
  author = {El Gamal, Abbas A. and Kim, Young-Han},
  date = {2011},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge ; New York}},
  isbn = {978-1-107-00873-1},
  langid = {english},
  pagetotal = {685},
  keywords = {information theory},
  annotation = {OCLC: ocn748328333},
  file = {/Users/lukakuma/Zotero/storage/K7RCKNCT/El Gamal and Kim - 2011 - Network information theory.pdf}
}

@online{Elondrop2022,
  title = {The {{Elondrop}}},
  date = {2022-04-28T18:22:11},
  url = {https://balajis.com/elondrop/},
  urldate = {2022-04-29},
  abstract = {Build the political support necessary to free Twitter with the largest airdrop in history.},
  langid = {english},
  organization = {{Balaji Srinivasan}},
  file = {/Users/lukakuma/Zotero/storage/HFLVGNE3/elondrop.html}
}

@article{emaniQuantumComputingFrontiers2021,
  title = {Quantum Computing at the Frontiers of Biological Sciences},
  author = {Emani, Prashant S. and Warrell, Jonathan and Anticevic, Alan and Bekiranov, Stefan and Gandal, Michael and McConnell, Michael J. and Sapiro, Guillermo and Aspuru-Guzik, Al\'an and Baker, Justin T. and Bastiani, Matteo and Murray, John D. and Sotiropoulos, Stamatios N. and Taylor, Jacob and Senthil, Geetha and Lehner, Thomas and Gerstein, Mark B. and Harrow, Aram W.},
  date = {2021-07},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {18},
  number = {7},
  pages = {701--709},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  url = {https://www.nature.com/articles/s41592-020-01004-3},
  urldate = {2022-05-26},
  abstract = {Computing plays a critical role in the biological sciences but faces increasing challenges of scale and complexity. Quantum computing, a computational paradigm exploiting the unique properties of quantum mechanical analogs of classical bits, seeks to address many of these challenges. We discuss the potential for quantum computing to aid in the merging of insights across different areas of biological sciences.},
  issue = {7},
  langid = {english},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/I5UT42M9/Emani et al. - 2021 - Quantum computing at the frontiers of biological s.pdf;/Users/lukakuma/Zotero/storage/JGCUJFRG/s41592-020-01004-3.html}
}

@article{engineeringbiologyresearchconsortiumEngineeringBiologyResearch2019,
  title = {Engineering {{Biology}}: {{A Research Roadmap}} for the {{Next-Generation Bioeconomy}} (2019)},
  shorttitle = {Engineering {{Biology}}},
  author = {Engineering Biology Research Consortium},
  date = {2019},
  publisher = {{Engineering Biology Research Consortium}},
  url = {https://roadmap.ebrc.org/resources/2019roadmap/},
  urldate = {2022-05-25},
  langid = {english},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/TK8TDZL7/Engineering Biology Research Consortium - 2019 - Engineering Biology A Research Roadmap for the Ne.pdf}
}

@inproceedings{erraqabiExplorationDrivenRepresentationLearning2021,
  title = {Exploration-{{Driven Representation Learning}} in {{Reinforcement Learning}}},
  author = {Erraqabi, Akram and Zhao, Mingde and Machado, Marlos C. and Bengio, Yoshua and Sukhbaatar, Sainbayar and Denoyer, Ludovic and Lazaric, Alessandro},
  date = {2021-06-13},
  url = {https://openreview.net/forum?id=6gEYTQFfnZh},
  urldate = {2022-06-13},
  abstract = {Learning reward-agnostic representations is an emerging paradigm in reinforcement learning. These representations can be leveraged for several purposes ranging from reward shaping to skill...},
  eventtitle = {{{ICML}} 2021 {{Workshop}} on {{Unsupervised Reinforcement Learning}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/ESD4DM69/Erraqabi et al. - 2021 - Exploration-Driven Representation Learning in Rein.pdf;/Users/lukakuma/Zotero/storage/ZIDKFCPZ/forum.html}
}

@unpublished{espeholtIMPALAScalableDistributed2018,
  title = {{{IMPALA}}: {{Scalable Distributed Deep-RL}} with {{Importance Weighted Actor-Learner Architectures}}},
  shorttitle = {{{IMPALA}}},
  author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  date = {2018-06-28},
  number = {arXiv:1802.01561},
  eprint = {1802.01561},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1802.01561},
  urldate = {2022-06-08},
  abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
  file = {/Users/lukakuma/Zotero/storage/5RYBJIJR/Espeholt et al. - 2018 - IMPALA Scalable Distributed Deep-RL with Importan.pdf;/Users/lukakuma/Zotero/storage/I6NWN3LN/1802.html}
}

@online{EssenceLinearAlgebra2010,
  title = {Essence of Linear Algebra - {{YouTube}}},
  date = {2010},
  url = {https://www.youtube.com/},
  urldate = {2022-08-22},
  abstract = {Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/GWTW3Y2H/playlist.html}
}

@article{evansProteinComplexPrediction2021,
  title = {Protein Complex Prediction with {{AlphaFold-Multimer}}},
  author = {Evans, Richard and O'Neill, Michael and Pritzel, Alexander and Antropova, Natasha and Senior, Andrew and Green, Tim and \v{Z}\'idek, Augustin and Bates, Russ and Blackwell, Sam and Yim, Jason and Ronneberger, Olaf and Bodenstein, Sebastian and Zielinski, Michal and Bridgland, Alex and Potapenko, Anna and Cowie, Andrew and Tunyasuvunakool, Kathryn and Jain, Rishub and Clancy, Ellen and Kohli, Pushmeet and Jumper, John and Hassabis, Demis},
  date = {2021-10-04},
  pages = {2021.10.04.463034},
  publisher = {{bioRxiv}},
  url = {https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1},
  urldate = {2022-05-17},
  abstract = {While the vast majority of well-structured single protein chains can now be predicted to high accuracy due to the recent AlphaFold [1] model, the prediction of multi-chain protein complexes remains a challenge in many cases. In this work, we demonstrate that an AlphaFold model trained specifically for multimeric inputs of known stoichiometry, which we call AlphaFold-Multimer, significantly increases accuracy of predicted multimeric interfaces over input-adapted single-chain AlphaFold while maintaining high intra-chain accuracy. On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] {$\geq$} 0.49) on 14 targets and high accuracy (DockQ {$\geq$} 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2]). We also predict structures for a large dataset of 4,433 recent protein complexes, from which we score all non-redundant interfaces with low template identity. For heteromeric interfaces we successfully predict the interface (DockQ {$\geq$} 0.23) in 67\% of cases, and produce high accuracy predictions (DockQ {$\geq$} 0.8) in 23\% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively. For homomeric interfaces we successfully predict the interface in 69\% of cases, and produce high accuracy predictions in 34\% of cases, an improvement of +5 percentage points in both instances.},
  langid = {english},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/HD7PEB48/Evans et al. - 2021 - Protein complex prediction with AlphaFold-Multimer.pdf;/Users/lukakuma/Zotero/storage/P6G9AC6D/2021.10.04.html}
}

@online{evansTruthfulAIDeveloping2021,
  title = {Truthful {{AI}}: {{Developing}} and Governing {{AI}} That Does Not Lie},
  shorttitle = {Truthful {{AI}}},
  author = {Evans, Owain and Cotton-Barratt, Owen and Finnveden, Lukas and Bales, Adam and Balwit, Avital and Wills, Peter and Righetti, Luca and Saunders, William},
  date = {2021-10-13},
  number = {arXiv:2110.06674},
  eprint = {arXiv:2110.06674},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.06674},
  urldate = {2022-12-28},
  abstract = {In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI "lies" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding "negligent falsehoods" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/ZTVEYSLF/Evans et al. - 2021 - Truthful AI Developing and governing AI that does.pdf;/Users/lukakuma/Zotero/storage/JQQV9TT3/2110.html}
}

@online{faltingsInteractiveTextGeneration2023,
  title = {Interactive {{Text Generation}}},
  author = {Faltings, Felix and Galley, Michel and Peng, Baolin and Brantley, Kiant\'e and Cai, Weixin and Zhang, Yizhe and Gao, Jianfeng and Dolan, Bill},
  date = {2023-03-01},
  number = {arXiv:2303.00908},
  eprint = {arXiv:2303.00908},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2303.00908},
  urldate = {2023-03-03},
  abstract = {Users interact with text, image, code, or other editors on a daily basis. However, machine learning models are rarely trained in the settings that reflect the interactivity between users and their editor. This is understandable as training AI models with real users is not only slow and costly, but what these models learn may be specific to user interface design choices. Unfortunately, this means most of the research on text, code, and image generation has focused on non-interactive settings, whereby the model is expected to get everything right without accounting for any input from a user who may be willing to help. We introduce a new Interactive Text Generation task that allows training generation models interactively without the costs of involving real users, by using user simulators that provide edits that guide the model towards a given target text. We train our interactive models using Imitation Learning, and our experiments against competitive non-interactive generation models show that models trained interactively are superior to their non-interactive counterparts, even when all models are given the same budget of user inputs or edits.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/KRB4DI9V/Faltings et al. - 2023 - Interactive Text Generation.pdf;/Users/lukakuma/Zotero/storage/SDJ4H8VT/2303.html}
}

@online{fanMineDojoBuildingOpenEnded2022,
  title = {{{MineDojo}}: {{Building Open-Ended Embodied Agents}} with {{Internet-Scale Knowledge}}},
  shorttitle = {{{MineDojo}}},
  author = {Fan, Linxi and Wang, Guanzhi and Jiang, Yunfan and Mandlekar, Ajay and Yang, Yuncong and Zhu, Haoyi and Tang, Andrew and Huang, De-An and Zhu, Yuke and Anandkumar, Anima},
  date = {2022-06-17},
  number = {arXiv:2206.08853},
  eprint = {arXiv:2206.08853},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.08853},
  urldate = {2022-06-25},
  abstract = {Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite and knowledge bases (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/NKIQSJQW/Fan et al. - 2022 - MineDojo Building Open-Ended Embodied Agents with.pdf;/Users/lukakuma/Zotero/storage/4DI5VY3Z/2206.html}
}

@article{farzanehBiofoundriesAreNucleating2021,
  title = {Biofoundries Are a Nucleating Hub for Industrial Translation},
  author = {Farzaneh, Tabasum and Freemont, Paul S},
  date = {2021-10-01},
  journaltitle = {Synthetic Biology},
  shortjournal = {Synthetic Biology},
  volume = {6},
  number = {1},
  pages = {ysab013},
  issn = {2397-7000},
  url = {https://doi.org/10.1093/synbio/ysab013},
  urldate = {2022-05-25},
  abstract = {Contemporary synthetic biology embraces the entire innovation pipeline; it is a transformative technology platform impacting new applications and improving existing industrial products and processes. However, challenges still emerge at the interface of upstream and downstream processes, integral to the value chain. It is now clear that biofoundries have a key role to play in addressing this; they provide unique and accessible infrastructure to drive the standardization necessary to deliver systematic design and engineering of biological systems and workflows. As for other biofoundries, the success of the London Biofoundry has been in part due to its expertise in establishing channels for industrial translation through its extensive strategic collaborations. It has also become cemented as a key component of various consortia and partnerships that serve the broader bioeconomy and industrial strategies. Adopting a networked approach enables links to be made between infrastructure, researchers, industrialists and policy makers to de-risk the economic challenges of scale-up, as well as contribute to the growing bioeconomy.},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/IYXN4U76/Farzaneh and Freemont - 2021 - Biofoundries are a nucleating hub for industrial t.pdf;/Users/lukakuma/Zotero/storage/ZSMFMM8F/6279071.html}
}

@article{FastReinforcementLearning2020,
  title = {Fast Reinforcement Learning with Generalized Policy Updates | {{PNAS}}},
  date = {2020},
  url = {https://www.pnas.org/doi/10.1073/pnas.1907370117},
  urldate = {2022-06-08},
  file = {/Users/lukakuma/Zotero/storage/97UC46KA/2020 - Fast reinforcement learning with generalized polic.pdf;/Users/lukakuma/Zotero/storage/E9NWBQVM/pnas.html}
}

@online{fedusReviewSparseExpert2022,
  title = {A {{Review}} of {{Sparse Expert Models}} in {{Deep Learning}}},
  author = {Fedus, William and Dean, Jeff and Zoph, Barret},
  date = {2022-09-04},
  number = {arXiv:2209.01667},
  eprint = {arXiv:2209.01667},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.01667},
  urldate = {2022-09-07},
  abstract = {Sparse expert models are a thirty-year old concept re-emerging as a popular architecture in deep learning. This class of architecture encompasses Mixture-of-Experts, Switch Transformers, Routing Networks, BASE layers, and others, all with the unifying idea that each example is acted on by a subset of the parameters. By doing so, the degree of sparsity decouples the parameter count from the compute per example allowing for extremely large, but efficient models. The resulting models have demonstrated significant improvements across diverse domains such as natural language processing, computer vision, and speech recognition. We review the concept of sparse expert models, provide a basic description of the common algorithms, contextualize the advances in the deep learning era, and conclude by highlighting areas for future work.},
  pubstate = {preprint},
  keywords = {read,sparsity},
  file = {/Users/lukakuma/Zotero/storage/ENB7ZBZN/Fedus et al. - 2022 - A Review of Sparse Expert Models in Deep Learning.pdf;/Users/lukakuma/Zotero/storage/H8AJFIZL/2209.html}
}

@unpublished{fedusSwitchTransformersScaling2021,
  title = {Switch {{Transformers}}: {{Scaling}} to {{Trillion Parameter Models}} with {{Simple}} and {{Efficient Sparsity}}},
  shorttitle = {Switch {{Transformers}}},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  date = {2021-01-11},
  eprint = {2101.03961},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2101.03961},
  urldate = {2022-03-11},
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  keywords = {read,sparsity},
  file = {/Users/lukakuma/Zotero/storage/EIJDMLGY/Fedus et al. - 2021 - Switch Transformers Scaling to Trillion Parameter.pdf;/Users/lukakuma/Zotero/storage/6SHZPUTK/2101.html}
}

@unpublished{ferdousBlockchainConsensusAlgorithms2020,
  title = {Blockchain {{Consensus Algorithms}}: {{A Survey}}},
  shorttitle = {Blockchain {{Consensus Algorithms}}},
  author = {Ferdous, Md Sadek and Chowdhury, Mohammad Jabed Morshed and Hoque, Mohammad A. and Colman, Alan},
  date = {2020-02-07},
  eprint = {2001.07091},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2001.07091},
  urldate = {2022-03-10},
  abstract = {In recent years, blockchain technology has received unparalleled attention from academia, industry, and governments all around the world. It is considered a technological breakthrough anticipated to disrupt several application domains. This has resulted in a plethora of blockchain systems for various purposes. However, many of these blockchain systems suffer from serious shortcomings related to their performance and security, which need to be addressed before any wide-scale adoption can be achieved. A crucial component of any blockchain system is its underlying consensus algorithm, which in many ways, determines its performance and security. Therefore, to address the limitations of different blockchain systems, several existing as well novel consensus algorithms have been introduced. A systematic analysis of these algorithms will help to understand how and why any particular blockchain performs the way it functions. However, the existing studies of consensus algorithms are not comprehensive. Those studies have incomplete discussions on the properties of the algorithms and fail to analyse several major blockchain consensus algorithms in terms of their scopes. This article fills this gap by analysing a wide range of consensus algorithms using a comprehensive taxonomy of properties and by examining the implications of different issues still prevalent in consensus algorithms in detail. The result of the analysis is presented in tabular formats, which provides a visual illustration of these algorithms in a meaningful way. We have also analysed more than hundred top crypto-currencies belonging to different categories of consensus algorithms to understand their properties and to implicate different trends in these crypto-currencies. Finally, we have presented a decision tree of algorithms to be used as a tool to test the suitability of consensus algorithms under different criteria.},
  file = {/Users/lukakuma/Zotero/storage/KAV5LRE3/Ferdous et al. - 2020 - Blockchain Consensus Algorithms A Survey.pdf;/Users/lukakuma/Zotero/storage/XCI8XITK/2001.html}
}

@online{fereydounianWhatFunctionsCan2022,
  title = {What {{Functions Can Graph Neural Networks Generate}}?},
  author = {Fereydounian, Mohammad and Hassani, Hamed and Karbasi, Amin},
  date = {2022-06-18},
  number = {arXiv:2202.08833},
  eprint = {arXiv:2202.08833},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.08833},
  urldate = {2022-12-20},
  abstract = {In this paper, we fully answer the above question through a key algebraic condition on graph functions, called \textbackslash textit\{permutation compatibility\}, that relates permutations of weights and features of the graph to functional constraints. We prove that: (i) a GNN, as a graph function, is necessarily permutation compatible; (ii) conversely, any permutation compatible function, when restricted on input graphs with distinct node features, can be generated by a GNN; (iii) for arbitrary node features (not necessarily distinct), a simple feature augmentation scheme suffices to generate a permutation compatible function by a GNN; (iv) permutation compatibility can be verified by checking only quadratically many functional constraints, rather than an exhaustive search over all the permutations; (v) GNNs can generate \textbackslash textit\{any\} graph function once we augment the node features with node identities, thus going beyond graph isomorphism and permutation compatibility. The above characterizations pave the path to formally study the intricate connection between GNNs and other algorithmic procedures on graphs. For instance, our characterization implies that many natural graph problems, such as min-cut value, max-flow value, max-clique size, and shortest path can be generated by a GNN using a simple feature augmentation. In contrast, the celebrated Weisfeiler-Lehman graph-isomorphism test fails whenever a permutation compatible function with identical features cannot be generated by a GNN. At the heart of our analysis lies a novel representation theorem that identifies basis functions for GNNs. This enables us to translate the properties of the target graph function into properties of the GNN's aggregation function.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/R38VPAAI/Fereydounian et al. - 2022 - What Functions Can Graph Neural Networks Generate.pdf;/Users/lukakuma/Zotero/storage/UR932DGM/2202.html}
}

@unpublished{finnModelAgnosticMetaLearningFast2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  date = {2017-07-18},
  number = {arXiv:1703.03400},
  eprint = {1703.03400},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1703.03400},
  urldate = {2022-06-08},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  file = {/Users/lukakuma/Zotero/storage/I6H7T3E3/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf;/Users/lukakuma/Zotero/storage/YWH6SEDR/1703.html}
}

@online{fintech.faisYouDonOwn2022,
  type = {Substack newsletter},
  title = {"{{You Don}}'t {{Own Web3}}": {{A Coinbase Curse}} and {{How VCs Sell Crypto}} to {{Retail}}},
  shorttitle = {"{{You Don}}'t {{Own Web3}}"},
  author = {family=(Fais), given=brown, prefix=guy in, useprefix=true},
  date = {2022-01-13},
  url = {https://startupsandecon.substack.com/p/you-dont-own-web3-a-coinbase-curse},
  urldate = {2022-01-16},
  abstract = {the one where Marc Andreessen blocks me},
  organization = {{startups and econ (Fais Khan)}},
  file = {/Users/lukakuma/Zotero/storage/FSYA6843/you-dont-own-web3-a-coinbase-curse.html}
}

@inproceedings{flennerhagBootstrappedMetaLearning2021,
  title = {Bootstrapped {{Meta-Learning}}},
  author = {Flennerhag, Sebastian and Schroecker, Yannick and Zahavy, Tom and family=Hasselt, given=Hado, prefix=van, useprefix=false and Silver, David and Singh, Satinder},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=b-ny3x071E5},
  urldate = {2022-05-15},
  abstract = {Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/IRSWPJAQ/Flennerhag et al. - 2021 - Bootstrapped Meta-Learning.pdf;/Users/lukakuma/Zotero/storage/IWNZA4UD/forum.html}
}

@online{florensaStochasticNeuralNetworks2017,
  title = {Stochastic {{Neural Networks}} for {{Hierarchical Reinforcement Learning}}},
  author = {Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
  date = {2017-04-10},
  number = {arXiv:1704.03012},
  eprint = {arXiv:1704.03012},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1704.03012},
  urldate = {2022-07-23},
  abstract = {Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/PJWEEWCT/Florensa et al. - 2017 - Stochastic Neural Networks for Hierarchical Reinfo.pdf;/Users/lukakuma/Zotero/storage/2KCMZT6M/1704.html}
}

@online{foersterLearningOpponentLearningAwareness2018,
  title = {Learning with {{Opponent-Learning Awareness}}},
  author = {Foerster, Jakob N. and Chen, Richard Y. and Al-Shedivat, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
  date = {2018-09-19},
  number = {arXiv:1709.04326},
  eprint = {arXiv:1709.04326},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1709.04326},
  urldate = {2022-07-23},
  abstract = {Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical RL, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes a term that accounts for the impact of one agent's policy on the anticipated parameter update of the other agents. Results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners' dilemma, while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the policy gradient estimator, making the method suitable for model-free RL. The method thus scales to large parameter and input spaces and nonlinear function approximators. We apply LOLA to a grid world task with an embedded social dilemma using recurrent policies and opponent modelling. By explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. The code is at github.com/alshedivat/lola.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/E693X6KK/Foerster et al. - 2018 - Learning with Opponent-Learning Awareness.pdf;/Users/lukakuma/Zotero/storage/GQY2HVRX/1709.html}
}

@report{foliusventuresYouXiZhuanTiJianGuoDaYeCongNuLiZhiZouXiangDuLi2022,
  title = {Ê∏∏Êàè‰∏ìÈ¢òÔºöÂª∫ÂõΩÂ§ß‰∏ö \textendash{} ‰ªéÂ•¥Èö∂Âà∂Ëµ∞ÂêëÁã¨Á´ã„ÄÇÂú®Êã•ÊúâÁâ©ÊùÉ,‰∫∫ÊùÉ, ÂèäËµÑÊú¨ËøõÂá∫Ëá™Áî±ÊùÉÁöÑÊ∏∏ÊàèÁîüÊÄÅ‰∏≠Ôºå{{P2EËÆæËÆ°Â∞Ü‰ΩïÂéª‰Ωï‰ªé}}},
  author = {Folius Ventures},
  date = {2022},
  file = {/Users/lukakuma/Zotero/storage/K8MM2ZGT/Ê∏∏Êàè‰∏ìÈ¢òÔºöÂª∫ÂõΩÂ§ß‰∏ö ‚Äì ‰ªéÂ•¥Èö∂Âà∂Ëµ∞ÂêëÁã¨Á´ã„ÄÇÂú®Êã•ÊúâÁâ©ÊùÉ,‰∫∫ÊùÉ, ÂèäËµÑÊú¨ËøõÂá∫Ëá™Áî±ÊùÉÁöÑÊ∏∏ÊàèÁîüÊÄÅ‰∏≠ÔºåP2EËÆæËÆ°Â∞Ü‰ΩïÂéª‰Ωï‰ªé.pdf}
}

@article{fosterReplayComesAge2017,
  title = {Replay {{Comes}} of {{Age}}.},
  author = {Foster, David J.},
  date = {2017},
  journaltitle = {Annual review of neuroscience},
  doi = {10.1146/annurev-neuro-072116-031538},
  abstract = {Hippocampal place cells take part in sequenced patterns of reactivation after behavioral experience, known as replay, and a focus on the phenomenology of replay is focused on. Hippocampal place cells take part in sequenced patterns of reactivation after behavioral experience, known as replay. Since replay was first reported, nearly 20 years ago, many new results have been found, necessitating revision of the original interpretations. We review some of these results with a focus on the phenomenology of replay.},
  file = {/Users/lukakuma/Zotero/storage/78DK7B7W/Foster - 2017 - Replay Comes of Age..pdf}
}

@online{fransMetaLearningShared2017,
  title = {Meta {{Learning Shared Hierarchies}}},
  author = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
  date = {2017-10-26},
  number = {arXiv:1710.09767},
  eprint = {arXiv:1710.09767},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.09767},
  urldate = {2022-07-23},
  abstract = {We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/MCILNEEU/Frans et al. - 2017 - Meta Learning Shared Hierarchies.pdf;/Users/lukakuma/Zotero/storage/U3QXYUN8/1710.html}
}

@online{friedInCoderGenerativeModel2022,
  title = {{{InCoder}}: {{A Generative Model}} for {{Code Infilling}} and {{Synthesis}}},
  shorttitle = {{{InCoder}}},
  author = {Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  date = {2022-04-17},
  number = {arXiv:2204.05999},
  eprint = {arXiv:2204.05999},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.05999},
  urldate = {2022-11-01},
  abstract = {Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https://sites.google.com/view/incoder-code-models},
  langid = {english},
  pubstate = {preprint},
  keywords = {code},
  file = {/Users/lukakuma/Zotero/storage/M7FPANLE/Fried et al. - 2022 - InCoder A Generative Model for Code Infilling and.pdf}
}

@article{fristonFreeenergyPrincipleUnified2010,
  title = {The Free-Energy Principle: A Unified Brain Theory?},
  shorttitle = {The Free-Energy Principle},
  author = {Friston, Karl},
  date = {2010-02},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {11},
  number = {2},
  pages = {127--138},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  url = {https://www.nature.com/articles/nrn2787},
  urldate = {2022-06-10},
  abstract = {Adaptive agents must occupy a limited repertoire of states and therefore minimize the long-term average of surprise associated with sensory exchanges with the world. Minimizing surprise enables them to resist a natural tendency to disorder.Surprise rests on predictions about sensations, which depend on an internal generative model of the world. Although surprise cannot be measured directly, a free-energy bound on surprise can be, suggesting that agents minimize free energy by changing their predictions (perception) or by changing the predicted sensory inputs (action).Perception optimizes predictions by minimizing free energy with respect to synaptic activity (perceptual inference), efficacy (learning and memory) and gain (attention and salience). This furnishes Bayes-optimal (probabilistic) representations of what caused sensations (providing a link to the Bayesian brain hypothesis).Bayes-optimal perception is mathematically equivalent to predictive coding and maximizing the mutual information between sensations and the representations of their causes. This is a probabilistic generalization of the principle of efficient coding (the infomax principle) or the minimum-redundancy principle.Learning under the free-energy principle can be formulated in terms of optimizing the connection strengths in hierarchical models of the sensorium. This rests on associative plasticity to encode causal regularities and appeals to the same synaptic mechanisms as those underlying cell assembly formation.Action under the free-energy principle reduces to suppressing sensory prediction errors that depend on predicted (expected or desired) movement trajectories. This provides a simple account of motor control, in which action is enslaved by perceptual (proprioceptive) predictions.Perceptual predictions rest on prior expectations about the trajectory or movement through the agent's state space. These priors can be acquired (as empirical priors during hierarchical inference) or they can be innate (epigenetic) and therefore subject to selective pressure.Predicted motion or state transitions realized by action correspond to policies in optimal control theory and reinforcement learning. In this context, value is inversely proportional to surprise (and implicitly free energy), and rewards correspond to innate priors that constrain policies.},
  issue = {2},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/M9W88U5L/Friston - 2010 - The free-energy principle a unified brain theory.pdf;/Users/lukakuma/Zotero/storage/ZVZE24UQ/nrn2787.html}
}

@article{fristonPredictiveCodingFreeenergy2009,
  title = {Predictive Coding under the Free-Energy Principle},
  author = {Friston, Karl and Kiebel, Stefan},
  date = {2009-05-12},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {364},
  number = {1521},
  eprint = {19528002},
  eprinttype = {pmid},
  pages = {1211--1221},
  issn = {0962-8436},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2666703/},
  urldate = {2022-08-12},
  abstract = {This paper considers prediction and perceptual categorization as an inference problem that is solved by the brain. We assume that the brain models the world as a hierarchy or cascade of dynamical systems that encode causal structure in the sensorium. Perception is equated with the optimization or inversion of these internal models, to explain sensory data. Given a model of how sensory data are generated, we can invoke a generic approach to model inversion, based on a free energy bound on the model's evidence. The ensuing free-energy formulation furnishes equations that prescribe the process of recognition, i.e. the dynamics of neuronal activity that represent the causes of sensory input. Here, we focus on a very general model, whose hierarchical and dynamical structure enables simulated brains to recognize and predict trajectories or sequences of sensory states. We first review hierarchical dynamical models and their inversion. We then show that the brain has the necessary infrastructure to implement this inversion and illustrate this point using synthetic birds that can recognize and categorize birdsongs.},
  pmcid = {PMC2666703},
  file = {/Users/lukakuma/Zotero/storage/XSE2RXWB/Friston and Kiebel - 2009 - Predictive coding under the free-energy principle.pdf}
}

@online{fuEffectivenessParameterEfficientFineTuning2022,
  title = {On the {{Effectiveness}} of {{Parameter-Efficient Fine-Tuning}}},
  author = {Fu, Zihao and Yang, Haoran and So, Anthony Man-Cho and Lam, Wai and Bing, Lidong and Collier, Nigel},
  date = {2022-11-28},
  number = {arXiv:2211.15583},
  eprint = {arXiv:2211.15583},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.15583},
  urldate = {2023-02-16},
  abstract = {Fine-tuning pre-trained models has been ubiquitously proven to be effective in a wide range of NLP tasks. However, fine-tuning the whole model is parameter inefficient as it always yields an entirely new model for each task. Currently, many research works propose to only fine-tune a small portion of the parameters while keeping most of the parameters shared across different tasks. These methods achieve surprisingly good performance and are shown to be more stable than their corresponding fully fine-tuned counterparts. However, such kind of methods is still not well understood. Some natural questions arise: How does the parameter sparsity lead to promising performance? Why is the model more stable than the fully fine-tuned models? How to choose the tunable parameters? In this paper, we first categorize the existing methods into random approaches, rule-based approaches, and projection-based approaches based on how they choose which parameters to tune. Then, we show that all of the methods are actually sparse fine-tuned models and conduct a novel theoretical analysis of them. We indicate that the sparsity is actually imposing a regularization on the original model by controlling the upper bound of the stability. Such stability leads to better generalization capability which has been empirically observed in a lot of recent research works. Despite the effectiveness of sparsity grounded by our theory, it still remains an open problem of how to choose the tunable parameters. To better choose the tunable parameters, we propose a novel Second-order Approximation Method (SAM) which approximates the original problem with an analytically solvable optimization function. The tunable parameters are determined by directly optimizing the approximation function. The experimental results show that our proposed SAM model outperforms many strong baseline models and it also verifies our theoretical analysis.},
  pubstate = {preprint},
  keywords = {*peft},
  file = {/Users/lukakuma/Zotero/storage/R8FFQG9X/Fu et al. - 2022 - On the Effectiveness of Parameter-Efficient Fine-T.pdf;/Users/lukakuma/Zotero/storage/EF5M7LWT/2211.html}
}

@online{fujimotoAddressingFunctionApproximation2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott and family=Hoof, given=Herke, prefix=van, useprefix=true and Meger, David},
  date = {2018-10-22},
  number = {arXiv:1802.09477},
  eprint = {arXiv:1802.09477},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.09477},
  urldate = {2022-07-20},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/Y3RAKA7V/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf;/Users/lukakuma/Zotero/storage/W9G34DA9/1802.html}
}

@online{fujimotoMinimalistApproachOffline2021,
  title = {A {{Minimalist Approach}} to {{Offline Reinforcement Learning}}},
  author = {Fujimoto, Scott and Gu, Shixiang Shane},
  date = {2021-12-03},
  number = {arXiv:2106.06860},
  eprint = {arXiv:2106.06860},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.06860},
  urldate = {2022-08-04},
  abstract = {Offline reinforcement learning (RL) defines the task of learning from a fixed batch of data. Due to errors in value estimation from out-of-distribution actions, most offline RL algorithms take the approach of constraining or regularizing the policy with the actions contained in the dataset. Built on pre-existing RL algorithms, modifications to make an RL algorithm work offline comes at the cost of additional complexity. Offline RL algorithms introduce new hyperparameters and often leverage secondary components such as generative models, while adjusting the underlying RL algorithm. In this paper we aim to make a deep RL algorithm work while making minimal changes. We find that we can match the performance of state-of-the-art offline RL algorithms by simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. The resulting algorithm is a simple to implement and tune baseline, while more than halving the overall run time by removing the additional computational overhead of previous methods.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/I6Z7H5V4/Fujimoto and Gu - 2021 - A Minimalist Approach to Offline Reinforcement Lea.pdf;/Users/lukakuma/Zotero/storage/9FDKWPSA/2106.html}
}

@article{fungBenchmarkingGraphNeural2021,
  title = {Benchmarking Graph Neural Networks for Materials Chemistry},
  author = {Fung, Victor and Zhang, Jiaxin and Juarez, Eric and Sumpter, Bobby G.},
  date = {2021-06-03},
  journaltitle = {npj Computational Materials},
  shortjournal = {npj Comput Mater},
  volume = {7},
  number = {1},
  pages = {1--8},
  publisher = {{Nature Publishing Group}},
  issn = {2057-3960},
  url = {https://www.nature.com/articles/s41524-021-00554-0},
  urldate = {2022-05-12},
  abstract = {Graph neural networks (GNNs) have received intense interest as a rapidly expanding class of machine learning models remarkably well-suited for materials applications. To date, a number of successful GNNs have been proposed and demonstrated for systems ranging from crystal stability to electronic property prediction and to surface chemistry and heterogeneous catalysis. However, a consistent benchmark of these models remains lacking, hindering the development and consistent evaluation of new models in the materials field. Here, we present a workflow and testing platform, MatDeepLearn, for quickly and reproducibly assessing and comparing GNNs and other machine learning models. We use this platform to optimize and evaluate a selection of top performing GNNs on several representative datasets in computational materials chemistry. From our investigations we note the importance of hyperparameter selection and find roughly similar performances for the top models once optimized. We identify several strengths in GNNs over conventional models in cases with compositionally diverse datasets and in its overall flexibility with respect to inputs, due to learned rather than defined representations. Meanwhile several weaknesses of GNNs are also observed including high data requirements, and suggestions for further improvement for applications in materials chemistry are discussed.},
  issue = {1},
  langid = {english},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/X7IV3YQG/Fung et al. - 2021 - Benchmarking graph neural networks for materials c.pdf}
}

@online{furutaGeneralizedDecisionTransformer2022,
  title = {Generalized {{Decision Transformer}} for {{Offline Hindsight Information Matching}}},
  author = {Furuta, Hiroki and Matsuo, Yutaka and Gu, Shixiang Shane},
  date = {2022-02-04},
  number = {arXiv:2111.10364},
  eprint = {arXiv:2111.10364},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.10364},
  urldate = {2022-07-23},
  abstract = {How to extract as much learning signal from each trajectory data has been a key problem in reinforcement learning (RL), where sample inefficiency has posed serious challenges for practical applications. Recent works have shown that using expressive policy function approximators and conditioning on future trajectory information -- such as future states in hindsight experience replay or returns-to-go in Decision Transformer (DT) -- enables efficient learning of multi-task policies, where at times online RL is fully replaced by offline behavioral cloning, e.g. sequence modeling. We demonstrate that all these approaches are doing hindsight information matching (HIM) -- training policies that can output the rest of trajectory that matches some statistics of future state information. We present Generalized Decision Transformer (GDT) for solving any HIM problem, and show how different choices for the feature function and the anti-causal aggregator not only recover DT as a special case, but also lead to novel Categorical DT (CDT) and Bi-directional DT (BDT) for matching different statistics of the future. For evaluating CDT and BDT, we define offline multi-task state-marginal matching (SMM) and imitation learning (IL) as two generic HIM problems, propose a Wasserstein distance loss as a metric for both, and empirically study them on MuJoCo continuous control benchmarks. CDT, which simply replaces anti-causal summation with anti-causal binning in DT, enables the first effective offline multi-task SMM algorithm that generalizes well to unseen and even synthetic multi-modal state-feature distributions. BDT, which uses an anti-causal second transformer as the aggregator, can learn to model any statistics of the future and outperforms DT variants in offline multi-task IL. Our generalized formulations from HIM and GDT greatly expand the role of powerful sequence modeling architectures in modern RL.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/APN445GU/Furuta et al. - 2022 - Generalized Decision Transformer for Offline Hinds.pdf;/Users/lukakuma/Zotero/storage/NZ2XH6EY/2111.html}
}

@online{galkinGraphML20222021,
  title = {Graph {{ML}} in 2022: {{Where}} Are We Now?},
  shorttitle = {Graph {{ML}} in 2022},
  author = {Galkin, Michael},
  date = {2021-12-29T03:00:51},
  url = {https://mgalkin.medium.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0},
  urldate = {2021-12-30},
  abstract = {Hot trends and major advancements},
  langid = {english},
  organization = {{Medium}},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/A3T52ZPU/graph-ml-in-2022-where-are-we-now-f7f8242599e0.html}
}

@unpublished{gamalLectureNotesNetwork2011,
  title = {Lecture {{Notes}} on {{Network Information Theory}}},
  author = {Gamal, Abbas El and Kim, Young-Han},
  date = {2011-12-14},
  eprint = {1001.3404},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1001.3404},
  urldate = {2022-03-22},
  abstract = {These lecture notes have been converted to a book titled Network Information Theory published recently by Cambridge University Press. This book provides a significantly expanded exposition of the material in the lecture notes as well as problems and bibliographic notes at the end of each chapter. The authors are currently preparing a set of slides based on the book that will be posted in the second half of 2012. More information about the book can be found at http://www.cambridge.org/9781107008731/. The previous (and obsolete) version of the lecture notes can be found at http://arxiv.org/abs/1001.3404v4/.},
  version = {4},
  keywords = {information theory},
  file = {/Users/lukakuma/Zotero/storage/JMI3VCYE/Gamal and Kim - 2011 - Lecture Notes on Network Information Theory.pdf;/Users/lukakuma/Zotero/storage/KT6HEYW6/1001.html}
}

@inproceedings{ganguliPredictabilitySurpriseLarge2022,
  title = {Predictability and {{Surprise}} in {{Large Generative Models}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and DasSarma, Nova and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Kernion, Jackson and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Elhage, Nelson and Showk, Sheer El and Fort, Stanislav and Hatfield-Dodds, Zac and Johnston, Scott and Kravec, Shauna and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Amodei, Dario and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Chris and Clark, Jack},
  date = {2022-06-21},
  eprint = {2202.07785},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1747--1764},
  url = {http://arxiv.org/abs/2202.07785},
  urldate = {2022-12-21},
  abstract = {Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.},
  keywords = {scaling law},
  file = {/Users/lukakuma/Zotero/storage/34RVD7HC/Ganguli et al. - 2022 - Predictability and Surprise in Large Generative Mo.pdf;/Users/lukakuma/Zotero/storage/2KT82PW9/2202.html}
}

@online{ganguliRedTeamingLanguage2022,
  title = {Red {{Teaming Language Models}} to {{Reduce Harms}}: {{Methods}}, {{Scaling Behaviors}}, and {{Lessons Learned}}},
  shorttitle = {Red {{Teaming Language Models}} to {{Reduce Harms}}},
  author = {Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and Jones, Andy and Bowman, Sam and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Elhage, Nelson and El-Showk, Sheer and Fort, Stanislav and Dodds, Zac Hatfield and Henighan, Tom and Hernandez, Danny and Hume, Tristan and Jacobson, Josh and Johnston, Scott and Kravec, Shauna and Olsson, Catherine and Ringer, Sam and Tran-Johnson, Eli and Amodei, Dario and Brown, Tom and Joseph, Nicholas and McCandlish, Sam and Olah, Chris and Kaplan, Jared and Clark, Jack},
  date = {2022-08-23},
  number = {arXiv:2209.07858},
  eprint = {arXiv:2209.07858},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.07858},
  urldate = {2022-10-27},
  abstract = {We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.},
  pubstate = {preprint},
  keywords = {alignment},
  file = {/Users/lukakuma/Zotero/storage/MW7MAC6P/Ganguli et al. - 2022 - Red Teaming Language Models to Reduce Harms Metho.pdf;/Users/lukakuma/Zotero/storage/IB7IPURL/2209.html}
}

@online{ganVisionLanguagePretrainingBasics2022,
  title = {Vision-{{Language Pre-training}}: {{Basics}}, {{Recent Advances}}, and {{Future Trends}}},
  shorttitle = {Vision-{{Language Pre-training}}},
  author = {Gan, Zhe and Li, Linjie and Li, Chunyuan and Wang, Lijuan and Liu, Zicheng and Gao, Jianfeng},
  date = {2022-10-17},
  number = {arXiv:2210.09263},
  eprint = {arXiv:2210.09263},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.09263},
  urldate = {2022-11-13},
  abstract = {This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: (\$i\$) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; (\$ii\$) VLP for core computer vision tasks, such as (open-set) image classification, object detection, and segmentation; and (\$iii\$) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, unified modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/IHXFPTWN/Gan et al. - 2022 - Vision-Language Pre-training Basics, Recent Advan.pdf;/Users/lukakuma/Zotero/storage/4Q578CAB/2210.html}
}

@online{gaoMakingPretrainedLanguage2021,
  title = {Making {{Pre-trained Language Models Better Few-shot Learners}}},
  author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  date = {2021-06-02},
  number = {arXiv:2012.15723},
  eprint = {arXiv:2012.15723},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.15723},
  urldate = {2022-11-02},
  abstract = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30\% absolute improvement, and 11\% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/GU29JBFT/Gao et al. - 2021 - Making Pre-trained Language Models Better Few-shot.pdf}
}

@unpublished{gaoObjectFolderMultisensoryObject2022,
  title = {{{ObjectFolder}} 2.0: {{A Multisensory Object Dataset}} for {{Sim2Real Transfer}}},
  shorttitle = {{{ObjectFolder}} 2.0},
  author = {Gao, Ruohan and Si, Zilin and Chang, Yen-Yu and Clarke, Samuel and Bohg, Jeannette and Fei-Fei, Li and Yuan, Wenzhen and Wu, Jiajun},
  date = {2022-04-05},
  eprint = {2204.02389},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2204.02389},
  urldate = {2022-04-13},
  abstract = {Objects play a crucial role in our everyday activities. Though multisensory object-centric learning has shown great potential lately, the modeling of objects in prior work is rather unrealistic. ObjectFolder 1.0 is a recent dataset that introduces 100 virtualized objects with visual, acoustic, and tactile sensory data. However, the dataset is small in scale and the multisensory data is of limited quality, hampering generalization to real-world scenarios. We present ObjectFolder 2.0, a large-scale, multisensory dataset of common household objects in the form of implicit neural representations that significantly enhances ObjectFolder 1.0 in three aspects. First, our dataset is 10 times larger in the amount of objects and orders of magnitude faster in rendering time. Second, we significantly improve the multisensory rendering quality for all three modalities. Third, we show that models learned from virtual objects in our dataset successfully transfer to their real-world counterparts in three challenging tasks: object scale estimation, contact localization, and shape reconstruction. ObjectFolder 2.0 offers a new path and testbed for multisensory learning in computer vision and robotics. The dataset is available at https://github.com/rhgao/ObjectFolder.},
  file = {/Users/lukakuma/Zotero/storage/ATQD7B2A/Gao et al. - 2022 - ObjectFolder 2.0 A Multisensory Object Dataset fo.pdf;/Users/lukakuma/Zotero/storage/3Y3YHZZJ/2204.html}
}

@online{gaoPALProgramaidedLanguage2023,
  title = {{{PAL}}: {{Program-aided Language Models}}},
  shorttitle = {{{PAL}}},
  author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  date = {2023-01-27},
  number = {arXiv:2211.10435},
  eprint = {arXiv:2211.10435},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.10435},
  urldate = {2023-02-02},
  abstract = {Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15\% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .},
  pubstate = {preprint},
  keywords = {*general tools,code},
  file = {/Users/lukakuma/Zotero/storage/LP5E3UGR/Gao et al. - 2022 - PAL Program-aided Language Models.pdf;/Users/lukakuma/Zotero/storage/UM9Y4753/Gao et al. - 2023 - PAL Program-aided Language Models.pdf;/Users/lukakuma/Zotero/storage/A8K68IDS/2211.html}
}

@online{gaoPile800GBDataset2020a,
  title = {The {{Pile}}: {{An 800GB Dataset}} of {{Diverse Text}} for {{Language Modeling}}},
  shorttitle = {The {{Pile}}},
  author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  date = {2020-12-31},
  number = {arXiv:2101.00027},
  eprint = {arXiv:2101.00027},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.00027},
  urldate = {2022-11-02},
  abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \textbackslash textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
  langid = {english},
  pubstate = {preprint},
  keywords = {pretraining},
  file = {/Users/lukakuma/Zotero/storage/G7JX7QYH/Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf;/Users/lukakuma/Zotero/storage/QX4UYKHI/2101.html}
}

@online{gaoPreciseZeroShotDense2022,
  title = {Precise {{Zero-Shot Dense Retrieval}} without {{Relevance Labels}}},
  author = {Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
  date = {2022-12-20},
  number = {arXiv:2212.10496},
  eprint = {arXiv:2212.10496},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.10496},
  urldate = {2022-12-23},
  abstract = {While dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings\textasciitilde (HyDE). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder\textasciitilde (e.g. Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages\textasciitilde (e.g. sw, ko, ja).},
  pubstate = {preprint},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/FCXCXUSS/Gao et al. - 2022 - Precise Zero-Shot Dense Retrieval without Relevanc.pdf;/Users/lukakuma/Zotero/storage/5QF74DSL/2212.html}
}

@online{gaoScalingLawsReward2022,
  title = {Scaling {{Laws}} for {{Reward Model Overoptimization}}},
  author = {Gao, Leo and Schulman, John and Hilton, Jacob},
  date = {2022-10-19},
  number = {arXiv:2210.10760},
  eprint = {arXiv:2210.10760},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.10760},
  urldate = {2022-11-25},
  abstract = {In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed "gold-standard" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-\$n\$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.},
  pubstate = {preprint},
  keywords = {scaling law},
  file = {/Users/lukakuma/Zotero/storage/QKE7TC7U/Gao et al. - 2022 - Scaling Laws for Reward Model Overoptimization.pdf;/Users/lukakuma/Zotero/storage/NGHTKHYP/2210.html}
}

@unpublished{gaudeletUtilisingGraphMachine2021,
  title = {Utilising {{Graph Machine Learning}} within {{Drug Discovery}} and {{Development}}},
  author = {Gaudelet, Thomas and Day, Ben and Jamasb, Arian R. and Soman, Jyothish and Regep, Cristian and Liu, Gertrude and Hayter, Jeremy B. R. and Vickers, Richard and Roberts, Charles and Tang, Jian and Roblin, David and Blundell, Tom L. and Bronstein, Michael M. and Taylor-King, Jake P.},
  date = {2021-02-10},
  eprint = {2012.05716},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2012.05716},
  urldate = {2022-05-01},
  abstract = {Graph Machine Learning (GML) is receiving growing interest within the pharmaceutical and biotechnology industries for its ability to model biomolecular structures, the functional relationships between them, and integrate multi-omic datasets - amongst other data types. Herein, we present a multidisciplinary academic-industrial review of the topic within the context of drug discovery and development. After introducing key terms and modelling approaches, we move chronologically through the drug development pipeline to identify and summarise work incorporating: target identification, design of small molecules and biologics, and drug repurposing. Whilst the field is still emerging, key milestones including repurposed drugs entering in vivo studies, suggest graph machine learning will become a modelling framework of choice within biomedical machine learning.},
  file = {/Users/lukakuma/Zotero/storage/NQFBPC8Y/Gaudelet et al. - 2021 - Utilising Graph Machine Learning within Drug Disco.pdf;/Users/lukakuma/Zotero/storage/MNWDRTA7/2012.html}
}

@unpublished{geAcceleratedDesignDeployment2022,
  title = {Accelerated {{Design}} and {{Deployment}} of {{Low-Carbon Concrete}} for {{Data Centers}}},
  author = {Ge, Xiou and Goodwin, Richard T. and Yu, Haizi and Romero, Pablo and Abdelrahman, Omar and Sudhalkar, Amruta and Kusuma, Julius and Cialdella, Ryan and Garg, Nishant and Varshney, Lav R.},
  date = {2022-04-11},
  eprint = {2204.05397},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.05397},
  urldate = {2022-04-29},
  abstract = {Concrete is the most widely used engineered material in the world with more than 10 billion tons produced annually. Unfortunately, with that scale comes a significant burden in terms of energy, water, and release of greenhouse gases and other pollutants; indeed 8\% of worldwide carbon emissions are attributed to the production of cement, a key ingredient in concrete. As such, there is interest in creating concrete formulas that minimize this environmental burden, while satisfying engineering performance requirements including compressive strength. Specifically for computing, concrete is a major ingredient in the construction of data centers. In this work, we use conditional variational autoencoders (CVAEs), a type of semi-supervised generative artificial intelligence (AI) model, to discover concrete formulas with desired properties. Our model is trained just using a small open dataset from the UCI Machine Learning Repository joined with environmental impact data from standard lifecycle analysis. Computational predictions demonstrate CVAEs can design concrete formulas with much lower carbon requirements than existing formulations while meeting design requirements. Next we report laboratory-based compressive strength experiments for five AI-generated formulations, which demonstrate that the formulations exceed design requirements. The resulting formulations were then used by Ozinga Ready Mix -- a concrete supplier -- to generate field-ready concrete formulations, based on local conditions and their expertise in concrete design. Finally, we report on how these formulations were used in the construction of buildings and structures in a Meta data center in DeKalb, IL, USA. Results from field experiments as part of this real-world deployment corroborate the efficacy of AI-generated low-carbon concrete mixes.},
  file = {/Users/lukakuma/Zotero/storage/CWWBEMYW/Ge et al. - 2022 - Accelerated Design and Deployment of Low-Carbon Co.pdf;/Users/lukakuma/Zotero/storage/UPKAGYQC/2204.html}
}

@inproceedings{geertsExpressivenessApproximationProperties2021,
  title = {Expressiveness and {{Approximation Properties}} of {{Graph Neural Networks}}},
  author = {Geerts, Floris and Reutter, Juan L.},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=wIzUeM3TAU},
  urldate = {2022-04-28},
  abstract = {Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/ADRH2937/Geerts and Reutter - 2021 - Expressiveness and Approximation Properties of Gra.pdf;/Users/lukakuma/Zotero/storage/DUHMQ938/forum.html}
}

@online{geExtensiblePromptsLanguage2022,
  title = {Extensible {{Prompts}} for {{Language Models}}},
  author = {Ge, Tao and Hu, Jing and Dong, Li and Mao, Shaoguang and Xia, Yan and Wang, Xun and Chen, Si-Qing and Wei, Furu},
  date = {2022-12-01},
  number = {arXiv:2212.00616},
  eprint = {arXiv:2212.00616},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.00616},
  urldate = {2023-01-23},
  abstract = {We propose eXtensible Prompt (X-Prompt) for prompting a large language model (LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words that are introduced to help represent what NL words hardly describe, allowing a prompt to be more descriptive. Like NL prompts, X-Prompt is out-of-distribution (OOD) robust, for which we propose context-guided learning with prompt augmentation to learn its imaginary words for general usability, enabling them to use in different prompt contexts for fine-grain specifications. The promising results of X-Prompt demonstrate its potential of approaching advanced interaction between humans and LLMs to bridge their communication gap.},
  pubstate = {preprint},
  keywords = {*peft,3-prefix tuning},
  file = {/Users/lukakuma/Zotero/storage/6W6XR542/Ge et al. - 2022 - Extensible Prompts for Language Models.pdf;/Users/lukakuma/Zotero/storage/DUVYD6DU/2212.html}
}

@online{gehmanRealToxicityPromptsEvaluatingNeural2020a,
  title = {{{RealToxicityPrompts}}: {{Evaluating Neural Toxic Degeneration}} in {{Language Models}}},
  shorttitle = {{{RealToxicityPrompts}}},
  author = {Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A.},
  date = {2020-09-25},
  number = {arXiv:2009.11462},
  eprint = {arXiv:2009.11462},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.11462},
  urldate = {2022-12-05},
  abstract = {Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning "bad" words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/8VC432FE/Gehman et al. - 2020 - RealToxicityPrompts Evaluating Neural Toxic Degen.pdf;/Users/lukakuma/Zotero/storage/FD5PR3HI/2009.html}
}

@online{geipingCrammingTrainingLanguage2022,
  title = {Cramming: {{Training}} a {{Language Model}} on a {{Single GPU}} in {{One Day}}},
  shorttitle = {Cramming},
  author = {Geiping, Jonas and Goldstein, Tom},
  date = {2022-12-28},
  number = {arXiv:2212.14034},
  eprint = {arXiv:2212.14034},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.14034},
  urldate = {2022-12-30},
  abstract = {Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/QJD3U7IU/Geiping and Goldstein - 2022 - Cramming Training a Language Model on a Single GP.pdf;/Users/lukakuma/Zotero/storage/DS3T8QKM/2212.html}
}

@online{gempDevelopingEvaluatingScaling2022,
  title = {Developing, {{Evaluating}} and {{Scaling Learning Agents}} in {{Multi-Agent Environments}}},
  author = {Gemp, Ian and Anthony, Thomas and Bachrach, Yoram and Bhoopchand, Avishkar and Bullard, Kalesha and Connor, Jerome and Dasagi, Vibhavari and De Vylder, Bart and Duenez-Guzman, Edgar and Elie, Romuald and Everett, Richard and Hennes, Daniel and Hughes, Edward and Khan, Mina and Lanctot, Marc and Larson, Kate and Lever, Guy and Liu, Siqi and Marris, Luke and McKee, Kevin R. and Muller, Paul and Perolat, Julien and Strub, Florian and Tacchetti, Andrea and Tarassov, Eugene and Wang, Zhe and Tuyls, Karl},
  date = {2022-09-22},
  number = {arXiv:2209.10958},
  eprint = {arXiv:2209.10958},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.10958},
  urldate = {2023-02-20},
  abstract = {The Game Theory \& Multi-Agent team at DeepMind studies several aspects of multi-agent learning ranging from computing approximations to fundamental concepts in game theory to simulating social dilemmas in rich spatial environments and training 3-d humanoids in difficult team coordination tasks. A signature aim of our group is to use the resources and expertise made available to us at DeepMind in deep reinforcement learning to explore multi-agent systems in complex environments and use these benchmarks to advance our understanding. Here, we summarise the recent work of our team and present a taxonomy that we feel highlights many important open challenges in multi-agent research.},
  pubstate = {preprint},
  keywords = {multiagent},
  file = {/Users/lukakuma/Zotero/storage/74Y4BJQK/Gemp et al. - 2022 - Developing, Evaluating and Scaling Learning Agents.pdf;/Users/lukakuma/Zotero/storage/UNC4A6H8/2209.html}
}

@article{gempEigenGameUnloadedWhen2021,
  title = {{{EigenGame Unloaded}}: {{When}} Playing Games Is Better than Optimizing},
  shorttitle = {{{EigenGame Unloaded}}},
  author = {Gemp, Ian and McWilliams, Brian and Vernade, Claire and Graepel, Thore},
  date = {2021-02-08},
  url = {https://arxiv.org/abs/2102.04152v2},
  urldate = {2022-06-10},
  abstract = {We build on the recently proposed EigenGame that views eigendecomposition as a competitive game. EigenGame's updates are biased if computed using minibatches of data, which hinders convergence and more sophisticated parallelism in the stochastic setting. In this work, we propose an unbiased stochastic update that is asymptotically equivalent to EigenGame, enjoys greater parallelism allowing computation on datasets of larger sample sizes, and outperforms EigenGame in experiments. We present applications to finding the principal components of massive datasets and performing spectral clustering of graphs. We analyze and discuss our proposed update in the context of EigenGame and the shift in perspective from optimization to games.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/BHKSVERT/Gemp et al. - 2021 - EigenGame Unloaded When playing games is better t.pdf;/Users/lukakuma/Zotero/storage/UIC9ZPWR/2102.html}
}

@online{georgievAlgorithmicConceptbasedExplainable2021,
  title = {Algorithmic {{Concept-based Explainable Reasoning}}},
  author = {Georgiev, Dobrik and Barbiero, Pietro and Kazhdan, Dmitry and Veli\v{c}kovi\'c, Petar and Li\`o, Pietro},
  date = {2021-07-15},
  number = {arXiv:2107.07493},
  eprint = {arXiv:2107.07493},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.07493},
  urldate = {2022-11-14},
  abstract = {Recent research on graph neural network (GNN) models successfully applied GNNs to classical graph algorithms and combinatorial optimisation problems. This has numerous benefits, such as allowing applications of algorithms when preconditions are not satisfied, or reusing learned models when sufficient training data is not available or can't be generated. Unfortunately, a key hindrance of these approaches is their lack of explainability, since GNNs are black-box models that cannot be interpreted directly. In this work, we address this limitation by applying existing work on concept-based explanations to GNN models. We introduce concept-bottleneck GNNs, which rely on a modification to the GNN readout mechanism. Using three case studies we demonstrate that: (i) our proposed model is capable of accurately learning concepts and extracting propositional formulas based on the learned concepts for each target class; (ii) our concept-based GNN models achieve comparative performance with state-of-the-art models; (iii) we can derive global graph concepts, without explicitly providing any supervision on graph-level concepts.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/2NWEVXCT/Georgiev et al. - 2021 - Algorithmic Concept-based Explainable Reasoning.pdf;/Users/lukakuma/Zotero/storage/TKN9KQIN/2107.html}
}

@article{georgievAlgorithmicConceptBasedExplainable2022,
  title = {Algorithmic {{Concept-Based Explainable Reasoning}}},
  author = {Georgiev, Dobrik and Barbiero, Pietro and Kazhdan, Dmitry and Veli\v{c}kovi\'c, Petar and Li\'o, Pietro},
  date = {2022-06-28},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {6},
  pages = {6685--6693},
  issn = {2374-3468},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/20623},
  urldate = {2022-12-22},
  abstract = {Recent research on graph neural network (GNN) models successfully applied GNNs to classical graph algorithms and combinatorial optimisation problems. This has numerous benefits, such as allowing applications of algorithms when preconditions are not satisfied, or reusing learned models when sufficient training data is not available or can't be generated. Unfortunately, a key hindrance of these approaches is their lack of explainability, since GNNs are black-box models that cannot be interpreted directly. In this work, we address this limitation by applying existing work on concept-based explanations to GNN models. We introduce concept-bottleneck GNNs, which rely on a modification to the GNN readout mechanism. Using three case studies we demonstrate that: (i) our proposed model is capable of accurately learning concepts and extracting propositional formulas based on the learned concepts for each target class; (ii) our concept-based GNN models achieve comparative performance with state-of-the-art models; (iii) we can derive global graph concepts, without explicitly providing any supervision on graph-level concepts.},
  issue = {6},
  langid = {english},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/LJLXS26W/Georgiev et al. - 2022 - Algorithmic Concept-Based Explainable Reasoning.pdf}
}

@online{gesmundoEvolutionaryApproachDynamic2022,
  title = {An {{Evolutionary Approach}} to {{Dynamic Introduction}} of {{Tasks}} in {{Large-scale Multitask Learning Systems}}},
  author = {Gesmundo, Andrea and Dean, Jeff},
  date = {2022-11-15},
  number = {arXiv:2205.12755},
  eprint = {arXiv:2205.12755},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.12755},
  urldate = {2023-03-02},
  abstract = {Multitask learning assumes that models capable of learning from multiple tasks can achieve better quality and efficiency via knowledge transfer, a key feature of human learning. Though, state of the art ML models rely on high customization for each task and leverage size and data scale rather than scaling the number of tasks. Also, continual learning, that adds the temporal aspect to multitask, is often focused to the study of common pitfalls such as catastrophic forgetting instead of being studied at a large scale as a critical component to build the next generation artificial intelligence.We propose an evolutionary method capable of generating large scale multitask models that support the dynamic addition of new tasks. The generated multitask models are sparsely activated and integrates a task-based routing that guarantees bounded compute cost and fewer added parameters per task as the model expands.The proposed method relies on a knowledge compartmentalization technique to achieve immunity against catastrophic forgetting and other common pitfalls such as gradient interference and negative transfer. We demonstrate empirically that the proposed method can jointly solve and achieve competitive results on 69public image classification tasks, for example improving the state of the art on a competitive benchmark such as cifar10 by achieving a 15\% relative error reduction compared to the best model trained on public data.},
  pubstate = {preprint},
  keywords = {evolutionary algorithm},
  file = {/Users/lukakuma/Zotero/storage/GI9YTDCT/Gesmundo and Dean - 2022 - An Evolutionary Approach to Dynamic Introduction o.pdf;/Users/lukakuma/Zotero/storage/U9IS6VZT/2205.html}
}

@online{gesmundoMultiagentFrameworkAsynchronous2022,
  title = {A {{Multiagent Framework}} for the {{Asynchronous}} and {{Collaborative Extension}} of {{Multitask ML Systems}}},
  author = {Gesmundo, Andrea},
  date = {2022-12-29},
  number = {arXiv:2209.14745},
  eprint = {arXiv:2209.14745},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.14745},
  urldate = {2023-03-02},
  abstract = {The traditional ML development methodology does not enable a large number of contributors, each with distinct objectives, to work collectively on the creation and extension of a shared intelligent system. Enabling such a collaborative methodology can accelerate the rate of innovation, increase ML technologies accessibility and enable the emergence of novel capabilities. We believe that this novel methodology for ML development can be demonstrated through a modularized representation of ML models and the definition of novel abstractions allowing to implement and execute diverse methods for the asynchronous use and extension of modular intelligent systems. We present a multiagent framework for the collaborative and asynchronous extension of dynamic large-scale multitask systems.},
  pubstate = {preprint},
  keywords = {evolutionary algorithm,multiagent},
  file = {/Users/lukakuma/Zotero/storage/D85J2HAK/Gesmundo - 2022 - A Multiagent Framework for the Asynchronous and Co.pdf;/Users/lukakuma/Zotero/storage/AX6JN7TS/2209.html}
}

@online{gesmundoMultipathAgentsModular2023,
  title = {Multipath Agents for Modular Multitask {{ML}} Systems},
  author = {Gesmundo, Andrea},
  date = {2023-02-06},
  number = {arXiv:2302.02721},
  eprint = {arXiv:2302.02721},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.02721},
  urldate = {2023-03-04},
  abstract = {A standard ML model is commonly generated by a single method that specifies aspects such as architecture, initialization, training data and hyperparameters configuration. The presented work introduces a novel methodology allowing to define multiple methods as distinct agents. Agents can collaborate and compete to generate and improve ML models for a given tasks. The proposed methodology is demonstrated with the generation and extension of a dynamic modular multitask ML system solving more than one hundred image classification tasks. Diverse agents can compete to produce the best performing model for a task by reusing the modules introduced to the system by competing agents. The presented work focuses on the study of agents capable of: 1) reusing the modules generated by concurrent agents, 2) activating in parallel multiple modules in a frozen state by connecting them with trainable modules, 3) condition the activation mixture on each data sample by using a trainable router module. We demonstrate that this simple per-sample parallel routing method can boost the quality of the combined solutions by training a fraction of the activated parameters.},
  pubstate = {preprint},
  keywords = {evolutionary algorithm,multiagent},
  file = {/Users/lukakuma/Zotero/storage/4MMCRJJ8/Gesmundo - 2023 - Multipath agents for modular multitask ML systems.pdf;/Users/lukakuma/Zotero/storage/PW2GGNJR/2302.html}
}

@online{giannouLoopedTransformersProgrammable2023,
  title = {Looped {{Transformers}} as {{Programmable Computers}}},
  author = {Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D. and Papailiopoulos, Dimitris},
  date = {2023-01-30},
  number = {arXiv:2301.13196},
  eprint = {arXiv:2301.13196},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.13196},
  urldate = {2023-02-02},
  abstract = {We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms to programs that can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/RST7CL9B/Giannou et al. - 2023 - Looped Transformers as Programmable Computers.pdf;/Users/lukakuma/Zotero/storage/P8ESCGJU/2301.html}
}

@unpublished{gilmerNeuralMessagePassing2017,
  title = {Neural {{Message Passing}} for {{Quantum Chemistry}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  date = {2017-06-12},
  eprint = {1704.01212},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1704.01212},
  urldate = {2022-03-28},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/MJGAMB6Z/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf;/Users/lukakuma/Zotero/storage/94Y4XXBW/1704.html}
}

@online{glaeseImprovingAlignmentDialogue2022,
  title = {Improving Alignment of Dialogue Agents via Targeted Human Judgements},
  author = {Glaese, Amelia and McAleese, Nat and Tr\k{e}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and Campbell-Gillingham, Lucy and Uesato, Jonathan and Huang, Po-Sen and Comanescu, Ramona and Yang, Fan and See, Abigail and Dathathri, Sumanth and Greig, Rory and Chen, Charlie and Fritz, Doug and Elias, Jaume Sanchez and Green, Richard and Mokr\'a, So\v{n}a and Fernando, Nicholas and Wu, Boxi and Foley, Rachel and Young, Susannah and Gabriel, Iason and Isaac, William and Mellor, John and Hassabis, Demis and Kavukcuoglu, Koray and Hendricks, Lisa Anne and Irving, Geoffrey},
  date = {2022-09-28},
  number = {arXiv:2209.14375},
  eprint = {arXiv:2209.14375},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.14375},
  urldate = {2022-12-20},
  abstract = {We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78\% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8\% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow,8-chat,alignment,multiagent,read,RLHF,self-play},
  file = {/Users/lukakuma/Zotero/storage/AZF4TQYY/Glaese et al. - 2022 - Improving alignment of dialogue agents via targete.pdf;/Users/lukakuma/Zotero/storage/QPBHK4KP/2209.html}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  edition = {Illustrated edition},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.``Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.''\textemdash Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  isbn = {978-0-262-03561-3},
  langid = {english},
  pagetotal = {800},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/4ZEZ8ZGV/Goodfellow et al. - 2016 - Deep Learning.pdf;/Users/lukakuma/Zotero/storage/PJ24CR76/www.deeplearningbook.org.html}
}

@online{gorbunovSecureDistributedTraining2022,
  title = {Secure {{Distributed Training}} at {{Scale}}},
  author = {Gorbunov, Eduard and Borzunov, Alexander and Diskin, Michael and Ryabinin, Max},
  date = {2022-06-28},
  number = {arXiv:2106.11257},
  eprint = {arXiv:2106.11257},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.11257},
  urldate = {2022-09-02},
  abstract = {Many areas of deep learning benefit from using increasingly larger neural networks trained on public data, as is the case for pre-trained models for NLP and computer vision. Training such models requires a lot of computational resources (e.g., HPC clusters) that are not available to small research groups and independent researchers. One way to address it is for several smaller groups to pool their computational resources together and train a model that benefits all participants. Unfortunately, in this case, any participant can jeopardize the entire training run by sending incorrect updates, deliberately or by mistake. Training in presence of such peers requires specialized distributed training algorithms with Byzantine tolerance. These algorithms often sacrifice efficiency by introducing redundant communication or passing all updates through a trusted server, making it infeasible to apply them to large-scale deep learning, where models can have billions of parameters. In this work, we propose a novel protocol for secure (Byzantine-tolerant) decentralized training that emphasizes communication efficiency.},
  pubstate = {preprint},
  keywords = {*open source AI},
  file = {/Users/lukakuma/Zotero/storage/MGM9MLLB/Gorbunov et al. - 2022 - Secure Distributed Training at Scale.pdf;/Users/lukakuma/Zotero/storage/JUB56P3V/2106.html}
}

@unpublished{goyalCoordinationNeuralModules2022,
  title = {Coordination {{Among Neural Modules Through}} a {{Shared Global Workspace}}},
  author = {Goyal, Anirudh and Didolkar, Aniket and Lamb, Alex and Badola, Kartikeya and Ke, Nan Rosemary and Rahaman, Nasim and Binas, Jonathan and Blundell, Charles and Mozer, Michael and Bengio, Yoshua},
  date = {2022-03-22},
  eprint = {2103.01197},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2103.01197},
  urldate = {2022-05-01},
  abstract = {Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions; object-centric architectures make use of graph neural networks to model interactions among entities. However, pairwise interactions may not achieve global coordination or a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise independent specialists.},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/Y8UYZ7AD/Goyal et al. - 2022 - Coordination Among Neural Modules Through a Shared.pdf;/Users/lukakuma/Zotero/storage/QBC27C5Y/2103.html}
}

@unpublished{goyalSelfsupervisedPretrainingVisual2021,
  title = {Self-Supervised {{Pretraining}} of {{Visual Features}} in the {{Wild}}},
  author = {Goyal, Priya and Caron, Mathilde and Lefaudeux, Benjamin and Xu, Min and Wang, Pengchao and Pai, Vivek and Singh, Mannat and Liptchinsky, Vitaliy and Misra, Ishan and Joulin, Armand and Bojanowski, Piotr},
  date = {2021-03-05},
  eprint = {2103.01988},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.01988},
  urldate = {2022-04-25},
  abstract = {Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2\% top-1 accuracy, surpassing the best self-supervised pretrained model by 1\% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9\% top-1 with access to only 10\% of ImageNet. Code: https://github.com/facebookresearch/vissl},
  file = {/Users/lukakuma/Zotero/storage/3ZR2PA2M/Goyal et al. - 2021 - Self-supervised Pretraining of Visual Features in .pdf;/Users/lukakuma/Zotero/storage/C45N2D79/2103.html}
}

@article{grewalGoingPointNeuron2021,
  title = {Going {{Beyond}} the {{Point Neuron}}: {{Active Dendrites}} and {{Sparse Representations}} for {{Continual Learning}}},
  shorttitle = {Going {{Beyond}} the {{Point Neuron}}},
  author = {Grewal, Karan and Forest, Jeremy and Cohen, Benjamin P. and Ahmad, Subutai},
  date = {2021-10-26},
  pages = {2021.10.25.465651},
  publisher = {{bioRxiv}},
  url = {https://www.biorxiv.org/content/10.1101/2021.10.25.465651v1},
  urldate = {2022-06-13},
  abstract = {Biological neurons integrate their inputs on dendrites using a diverse range of non-linear functions. However the majority of artificial neural networks (ANNs) ignore biological neurons' structural complexity and instead use simplified point neurons. Can dendritic properties add value to ANNs? In this paper we investigate this question in the context of continual learning, an area where ANNs suffer from catastrophic forgetting (i.e., ANNs are unable to learn new information without erasing what they previously learned). We propose that dendritic properties can help neurons learn context-specific patterns and invoke highly sparse context-specific subnetworks. Within a continual learning scenario, these task-specific subnetworks interfere minimally with each other and, as a result, the network remembers previous tasks significantly better than standard ANNs. We then show that by combining dendritic networks with Synaptic Intelligence (a biologically motivated method for complex weights) we can achieve significant resilience to catastrophic forgetting, more than either technique can achieve on its own. Our neuron model is directly inspired by the biophysics of sustained depolarization following dendritic NMDA spikes. Our research sheds light on how biological properties of neurons can be used to solve scenarios that are typically impossible for traditional ANNs to solve.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/E5G6U72T/Grewal et al. - 2021 - Going Beyond the Point Neuron Active Dendrites an.pdf;/Users/lukakuma/Zotero/storage/XSI88EPR/2021.10.25.html}
}

@article{greyMONETARYRESILIENCE2019,
  title = {{{MONETARY RESILIENCE}}},
  author = {Grey, Rohan},
  date = {2019-01-01},
  journaltitle = {Western New England Law Review},
  volume = {41},
  number = {3},
  pages = {505},
  url = {https://digitalcommons.law.wne.edu/lawreview/vol41/iss3/4},
  file = {/Users/lukakuma/Zotero/storage/LDG6FMQ8/4.html}
}

@unpublished{grillBootstrapYourOwn2020,
  title = {Bootstrap Your Own Latent: {{A}} New Approach to Self-Supervised {{Learning}}},
  shorttitle = {Bootstrap Your Own Latent},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch\'e, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R\'emi and Valko, Michal},
  date = {2020-09-10},
  eprint = {2006.07733},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.07733},
  urldate = {2022-04-25},
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3\textbackslash\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6\textbackslash\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  file = {/Users/lukakuma/Zotero/storage/3ZLUUBV2/Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf;/Users/lukakuma/Zotero/storage/MRXRGFWD/2006.html}
}

@article{grossmanPoliticsOrderInformal2020,
  title = {The {{Politics}} of {{Order}} in {{Informal Markets}}: {{Evidence}} from {{Lagos}}},
  shorttitle = {The {{Politics}} of {{Order}} in {{Informal Markets}}},
  author = {Grossman, Shelby},
  date = {2020-01},
  journaltitle = {World Politics},
  volume = {72},
  number = {1},
  pages = {47--79},
  publisher = {{Cambridge University Press}},
  issn = {0043-8871, 1086-3338},
  url = {https://www.cambridge.org/core/journals/world-politics/article/abs/politics-of-order-in-informal-markets-evidence-from-lagos/B3234307F029AFE5A4FAA2CD0F615FD2},
  urldate = {2022-03-09},
  abstract = {Property rights are important for economic exchange, but in many parts of the world, they are not publicly guaranteed. Private market associations can fill this gap by providing an institutional structure to enforce agreements, but with this power comes the ability to extort from group members. Under what circumstances do private associations provide a stable environment for economic activity? The author uses survey data collected from 1,179 randomly sampled traders across 199 markets in Lagos, Nigeria, and finds that markets maintain institutions to support trade not in the absence of government, but rather in response to active government interference. The author argues that associations develop protrade policies when threatened by politicians they perceive to be predatory and when the organizations can respond with threats of their own. The latter is easier when traders are not competing with one another. To maintain this balance of power, an association will not extort; it needs trader support to maintain the credibility of its threats to mobilize against predatory politicians.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/CNT9DDHN/Grossman - 2020 - The Politics of Order in Informal Markets Evidenc.pdf;/Users/lukakuma/Zotero/storage/KV8LVMIC/politics_of_order_in_informal_markets_evidence_from_lagos (1).pdf;/Users/lukakuma/Zotero/storage/MUHK4J2R/B3234307F029AFE5A4FAA2CD0F615FD2.html}
}

@online{groverLearningPolicyRepresentations2018,
  title = {Learning {{Policy Representations}} in {{Multiagent Systems}}},
  author = {Grover, Aditya and Al-Shedivat, Maruan and Gupta, Jayesh K. and Burda, Yura and Edwards, Harrison},
  date = {2018-07-31},
  number = {arXiv:1806.06464},
  eprint = {arXiv:1806.06464},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1806.06464},
  urldate = {2022-07-23},
  abstract = {Modeling agent behavior is central to understanding the emergence of complex phenomena in multiagent systems. Prior work in agent modeling has largely been task-specific and driven by hand-engineering domain-specific prior knowledge. We propose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a representation learning problem. Consequently, we construct a novel objective inspired by imitation learning and agent identification and design an algorithm for unsupervised learning of representations of agent policies. We demonstrate empirically the utility of the proposed framework in (i) a challenging high-dimensional competitive environment for continuous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/4JYY2MY9/Grover et al. - 2018 - Learning Policy Representations in Multiagent Syst.pdf;/Users/lukakuma/Zotero/storage/D7HUQIUD/1806.html}
}

@online{gruslysAdvantageRegretMatchingActorCritic2020,
  title = {The {{Advantage Regret-Matching Actor-Critic}}},
  author = {Gruslys, Audr\=unas and Lanctot, Marc and Munos, R\'emi and Timbers, Finbarr and Schmid, Martin and Perolat, Julien and Morrill, Dustin and Zambaldi, Vinicius and Lespiau, Jean-Baptiste and Schultz, John and Azar, Mohammad Gheshlaghi and Bowling, Michael and Tuyls, Karl},
  date = {2020-08-27},
  number = {arXiv:2008.12234},
  eprint = {arXiv:2008.12234},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2008.12234},
  urldate = {2023-02-20},
  abstract = {Regret minimization has played a key role in online learning, equilibrium computation in games, and reinforcement learning (RL). In this paper, we describe a general model-free RL method for no-regret learning based on repeated reconsideration of past behavior. We propose a model-free RL algorithm, the AdvantageRegret-Matching Actor-Critic (ARMAC): rather than saving past state-action data, ARMAC saves a buffer of past policies, replaying through them to reconstruct hindsight assessments of past behavior. These retrospective value estimates are used to predict conditional advantages which, combined with regret matching, produces a new policy. In particular, ARMAC learns from sampled trajectories in a centralized training setting, without requiring the application of importance sampling commonly used in Monte Carlo counterfactual regret (CFR) minimization; hence, it does not suffer from excessive variance in large environments. In the single-agent setting, ARMAC shows an interesting form of exploration by keeping past policies intact. In the multiagent setting, ARMAC in self-play approaches Nash equilibria on some partially-observable zero-sum benchmarks. We provide exploitability estimates in the significantly larger game of betting-abstracted no-limit Texas Hold'em.},
  pubstate = {preprint},
  keywords = {multiagent},
  file = {/Users/lukakuma/Zotero/storage/GCXZ6ZPU/Gruslys et al. - 2020 - The Advantage Regret-Matching Actor-Critic.pdf;/Users/lukakuma/Zotero/storage/QGC6WWM9/2008.html}
}

@thesis{guizzoEssentialMessageClaude2003,
  type = {Thesis},
  title = {The Essential Message : {{Claude Shannon}} and the Making of Information Theory},
  shorttitle = {The Essential Message},
  author = {Guizzo, Erico Marui},
  date = {2003},
  institution = {{Massachusetts Institute of Technology}},
  url = {https://dspace.mit.edu/handle/1721.1/39429},
  urldate = {2022-03-22},
  abstract = {In 1948, Claude Shannon, a young engineer and mathematician working at the Bell Telephone Laboratories, published "A Mathematical Theory of Communication," a seminal paper that marked the birth of information theory. In that paper, Shannon defined what the once fuzzy concept of "information" meant for communication engineers and proposed a precise way to quantify it-in his theory, the fundamental unit of information is the bit. He also showed how data could be "compressed" before transmission and how virtually error-free communication could be achieved. The concepts Shannon developed in his paper are at the heart of today's digital information technology. CDs, DVDs, cell phones, fax machines, modems, computer networks, hard drives, memory chips, encryption schemes, MP3 music, optical communication, high-definition television-all these things embody many of Shannon's ideas and others inspired by him. But despite the importance of his work and its influence on everyday life, Claude Shannon is still unknown to most people. Many papers, theses, books, and articles on information theory have been published, but none have explored in detail and in accessible language aimed at a general audience what the theory is about, how it changed the world of communication, and-most importantly-what path led Shannon to his revolutionary ideas. "The Essential Message" presents an account of the making of information theory based on papers, letters, interviews with Shannon and his colleagues, and other sources. It describes the context in which Shannon was immersed, the main ideas in his 1948 paper-and the reaction to it-and how his theory shaped the technologies that changed one of the most fundamental activities in our lives: communication.},
  langid = {english},
  keywords = {information theory},
  annotation = {Accepted: 2007-11-15T18:07:21Z},
  file = {/Users/lukakuma/Zotero/storage/UKDIMVS6/Guizzo - 2003 - The essential message  Claude Shannon and the mak.pdf;/Users/lukakuma/Zotero/storage/2JZPWSB2/39429.html}
}

@online{guoCalibrationModernNeural2017,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  date = {2017-08-03},
  number = {arXiv:1706.04599},
  eprint = {arXiv:1706.04599},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.04599},
  urldate = {2022-12-22},
  abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
  pubstate = {preprint},
  keywords = {calibration},
  file = {/Users/lukakuma/Zotero/storage/QDZA8525/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf;/Users/lukakuma/Zotero/storage/4NBR5MLJ/1706.html}
}

@online{guptaRecoveringPrivateText2022,
  title = {Recovering {{Private Text}} in {{Federated Learning}} of {{Language Models}}},
  author = {Gupta, Samyak and Huang, Yangsibo and Zhong, Zexuan and Gao, Tianyu and Li, Kai and Chen, Danqi},
  date = {2022-10-17},
  number = {arXiv:2205.08514},
  eprint = {arXiv:2205.08514},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.08514},
  urldate = {2022-11-02},
  abstract = {Federated learning allows distributed users to collaboratively train a model while keeping each user's data private. Recently, a growing body of work has demonstrated that an eavesdropping attacker can effectively recover image data from gradients transmitted during federated learning. However, little progress has been made in recovering text data. In this paper, we present a novel attack method FILM for federated learning of language models (LMs). For the first time, we show the feasibility of recovering text from large batch sizes of up to 128 sentences. Unlike image-recovery methods that are optimized to match gradients, we take a distinct approach that first identifies a set of words from gradients and then directly reconstructs sentences based on beam search and a prior-based reordering strategy. We conduct the FILM attack on several large-scale datasets and show that it can successfully reconstruct single sentences with high fidelity for large batch sizes and even multiple sentences if applied iteratively. We evaluate three defense methods: gradient pruning, DPSGD, and a simple approach to freeze word embeddings that we propose. We show that both gradient pruning and DPSGD lead to a significant drop in utility. However, if we fine-tune a public pre-trained LM on private text without updating word embeddings, it can effectively defend the attack with minimal data utility loss. Together, we hope that our results can encourage the community to rethink the privacy concerns of LM training and its standard practices in the future.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/P273JG8P/Gupta et al. - 2022 - Recovering Private Text in Federated Learning of L.pdf}
}

@online{gurAdversarialEnvironmentGeneration2021,
  title = {Adversarial {{Environment Generation}} for {{Learning}} to {{Navigate}} the {{Web}}},
  author = {Gur, Izzeddin and Jaques, Natasha and Malta, Kevin and Tiwari, Manoj and Lee, Honglak and Faust, Aleksandra},
  date = {2021-03-02},
  number = {arXiv:2103.01991},
  eprint = {arXiv:2103.01991},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.01991},
  urldate = {2023-02-20},
  abstract = {Learning to autonomously navigate the web is a difficult sequential decision making task. The state and action spaces are large and combinatorial in nature, and websites are dynamic environments consisting of several pages. One of the bottlenecks of training web navigation agents is providing a learnable curriculum of training environments that can cover the large variety of real-world websites. Therefore, we propose using Adversarial Environment Generation (AEG) to generate challenging web environments in which to train reinforcement learning (RL) agents. We provide a new benchmarking environment, gMiniWoB, which enables an RL adversary to use compositional primitives to learn to generate arbitrarily complex websites. To train the adversary, we propose a new technique for maximizing regret using the difference in the scores obtained by a pair of navigator agents. Our results show that our approach significantly outperforms prior methods for minimax regret AEG. The regret objective trains the adversary to design a curriculum of environments that are "just-the-right-challenge" for the navigator agents; our results show that over time, the adversary learns to generate increasingly complex web navigation tasks. The navigator agents trained with our technique learn to complete challenging, high-dimensional web navigation tasks, such as form filling, booking a flight etc. We show that the navigator agent trained with our proposed Flexible b-PAIRED technique significantly outperforms competitive automatic curriculum generation baselines -- including a state-of-the-art RL web navigation approach -- on a set of challenging unseen test environments, and achieves more than 80\% success rate on some tasks.},
  pubstate = {preprint},
  version = {1},
  keywords = {*general tools},
  file = {/Users/lukakuma/Zotero/storage/P5VX2XQH/Gur et al. - 2021 - Adversarial Environment Generation for Learning to.pdf;/Users/lukakuma/Zotero/storage/MUAVSS89/2103.html}
}

@online{gurUnderstandingHTMLLarge2022,
  title = {Understanding {{HTML}} with {{Large Language Models}}},
  author = {Gur, Izzeddin and Nachum, Ofir and Miao, Yingjie and Safdari, Mustafa and Huang, Austin and Chowdhery, Aakanksha and Narang, Sharan and Fiedel, Noah and Faust, Aleksandra},
  date = {2022-10-08},
  number = {arXiv:2210.03945},
  eprint = {arXiv:2210.03945},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.03945},
  urldate = {2023-02-20},
  abstract = {Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -- i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12\% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniWoB benchmark, LLMs successfully complete 50\% more tasks using 192x less data compared to the previous best supervised model. Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture. To promote further research on LLMs for HTML understanding, we create and open-source a large-scale HTML dataset distilled and auto-labeled from CommonCrawl.},
  pubstate = {preprint},
  keywords = {*general tools},
  file = {/Users/lukakuma/Zotero/storage/XTR3WLKJ/Gur et al. - 2022 - Understanding HTML with Large Language Models.pdf;/Users/lukakuma/Zotero/storage/QBP499V8/2210.html}
}

@online{gururanganDEMixLayersDisentangling2021,
  title = {{{DEMix Layers}}: {{Disentangling Domains}} for {{Modular Language Modeling}}},
  shorttitle = {{{DEMix Layers}}},
  author = {Gururangan, Suchin and Lewis, Mike and Holtzman, Ari and Smith, Noah A. and Zettlemoyer, Luke},
  date = {2021-08-20},
  number = {arXiv:2108.05036},
  eprint = {arXiv:2108.05036},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.05036},
  urldate = {2022-10-18},
  abstract = {We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer is a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity, increase training efficiency, and enable rapid adaptation with little overhead. We show that mixing experts during inference, using a parameter-free weighted ensemble, allows the model to better generalize to heterogeneous or unseen domains. We also show that experts can be added to iteratively incorporate new domains without forgetting older ones, and that experts can be removed to restrict access to unwanted domains, without additional training. Overall, these results demonstrate benefits of explicitly conditioning on textual domains during language modeling.},
  pubstate = {preprint},
  keywords = {sparsity},
  file = {/Users/lukakuma/Zotero/storage/AGUZM4C6/Gururangan et al. - 2021 - DEMix Layers Disentangling Domains for Modular La.pdf;/Users/lukakuma/Zotero/storage/84I2H7DH/2108.html}
}

@online{gururanganWhoseLanguageCounts2022,
  title = {Whose {{Language Counts}} as {{High Quality}}? {{Measuring Language Ideologies}} in {{Text Data Selection}}},
  shorttitle = {Whose {{Language Counts}} as {{High Quality}}?},
  author = {Gururangan, Suchin and Card, Dallas and Dreier, Sarah K. and Gade, Emily K. and Wang, Leroy Z. and Wang, Zeyu and Zettlemoyer, Luke and Smith, Noah A.},
  date = {2022-01-26},
  number = {arXiv:2201.10474},
  eprint = {arXiv:2201.10474},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.10474},
  urldate = {2022-11-02},
  abstract = {Language models increasingly rely on massive web dumps for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and newswire often serve as anchors for automatically selecting web text most suitable for language modeling, a process typically referred to as quality filtering. Using a new dataset of U.S. high school newspaper articles\textemdash written by students from across the country\textemdash we investigate whose language is preferred by the quality filter used for GPT-3. We find that newspapers from larger schools, located in wealthier, educated, and urban ZIP codes are more likely to be classified as high quality. We then demonstrate that the filter's measurement of quality is unaligned with other sensible metrics, such as factuality or literary acclaim. We argue that privileging any corpus as high quality entails a language ideology, and more care is needed to construct training corpora for language models, with better transparency and justification for the inclusion or exclusion of various texts.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/XRH94JB7/Gururangan et al. - 2022 - Whose Language Counts as High Quality Measuring L.pdf}
}

@article{gusakSurveyLargeScale2022,
  title = {Survey on {{Large Scale Neural Network Training}}},
  author = {Gusak, Julia and Cherniuk, Daria and Shilova, Alena and Katrutsa, Alexander and Bershatsky, Daniel and Zhao, Xunyi and Eyraud-Dubois, Lionel and Shlyazhko, Oleg and Dimitrov, Denis and Oseledets, Ivan and Beaumont, Olivier},
  date = {2022-02-21},
  url = {https://arxiv.org/abs/2202.10435v1},
  urldate = {2022-03-23},
  abstract = {Modern Deep Neural Networks (DNNs) require significant memory to store weight, activations, and other intermediate tensors during training. Hence, many models do not fit one GPU device or can be trained using only a small per-GPU batch size. This survey provides a systematic overview of the approaches that enable more efficient DNNs training. We analyze techniques that save memory and make good use of computation and communication resources on architectures with a single or several GPUs. We summarize the main categories of strategies and compare strategies within and across categories. Along with approaches proposed in the literature, we discuss available implementations.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/YG737WF5/Gusak et al. - 2022 - Survey on Large Scale Neural Network Training.pdf}
}

@online{gutmannPenPaperExercises2022,
  title = {Pen and {{Paper Exercises}} in {{Machine Learning}}},
  author = {Gutmann, Michael U.},
  date = {2022-06-27},
  number = {arXiv:2206.13446},
  eprint = {arXiv:2206.13446},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.13446},
  urldate = {2022-08-08},
  abstract = {This is a collection of (mostly) pen-and-paper exercises in machine learning. The exercises are on the following topics: linear algebra, optimisation, directed graphical models, undirected graphical models, expressive power of graphical models, factor graphs and message passing, inference for hidden Markov models, model-based learning (including ICA and unnormalised models), sampling and Monte-Carlo integration, and variational inference.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/ZFS9B9S6/Gutmann - 2022 - Pen and Paper Exercises in Machine Learning.pdf;/Users/lukakuma/Zotero/storage/FFQ2ACTM/2206.html}
}

@online{guuREALMRetrievalAugmentedLanguage2020,
  title = {{{REALM}}: {{Retrieval-Augmented Language Model Pre-Training}}},
  shorttitle = {{{REALM}}},
  author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  date = {2020-02-10},
  number = {arXiv:2002.08909},
  eprint = {arXiv:2002.08909},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.08909},
  urldate = {2022-10-03},
  abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
  pubstate = {preprint},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/LZJGZPHX/Guu et al. - 2020 - REALM Retrieval-Augmented Language Model Pre-Trai.pdf;/Users/lukakuma/Zotero/storage/C2JYPFE8/2002.html}
}

@online{gwernGPT32ndAnniversary2022,
  type = {Reddit Post},
  title = {{{GPT-3}} 2nd {{Anniversary}}},
  author = {{gwern}},
  date = {2022-05-28T13:40:34},
  url = {www.reddit.com/r/mlscaling/comments/uznkhw/gpt3_2nd_anniversary/},
  urldate = {2022-06-09},
  organization = {{r/mlscaling}},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/3FT44R2I/gpt3_2nd_anniversary.html}
}

@online{haarnojaLearningWalkDeep2019,
  title = {Learning to {{Walk}} via {{Deep Reinforcement Learning}}},
  author = {Haarnoja, Tuomas and Ha, Sehoon and Zhou, Aurick and Tan, Jie and Tucker, George and Levine, Sergey},
  date = {2019-06-19},
  number = {arXiv:1812.11103},
  eprint = {arXiv:1812.11103},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1812.11103},
  urldate = {2022-07-20},
  abstract = {Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/JD9HIQBX/Haarnoja et al. - 2019 - Learning to Walk via Deep Reinforcement Learning.pdf;/Users/lukakuma/Zotero/storage/4UUGSHLK/1812.html}
}

@online{haarnojaSoftActorCriticAlgorithms2019,
  title = {Soft {{Actor-Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  date = {2019-01-29},
  number = {arXiv:1812.05905},
  eprint = {arXiv:1812.05905},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1812.05905},
  urldate = {2022-07-20},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/F5UBW7UJ/Haarnoja et al. - 2019 - Soft Actor-Critic Algorithms and Applications.pdf;/Users/lukakuma/Zotero/storage/99KEWFBK/1812.html}
}

@online{haarnojaSoftActorCriticOffPolicy2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  date = {2018-08-08},
  number = {arXiv:1801.01290},
  eprint = {arXiv:1801.01290},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1801.01290},
  urldate = {2022-07-19},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/ILS2PJTP/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf;/Users/lukakuma/Zotero/storage/A624F5S4/1801.html}
}

@unpublished{hafnerMasteringAtariDiscrete2022,
  title = {Mastering {{Atari}} with {{Discrete World Models}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  date = {2022-02-12},
  number = {arXiv:2010.02193},
  eprint = {2010.02193},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2010.02193},
  urldate = {2022-06-08},
  abstract = {Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.},
  file = {/Users/lukakuma/Zotero/storage/IY6VKE6Q/Hafner et al. - 2022 - Mastering Atari with Discrete World Models.pdf;/Users/lukakuma/Zotero/storage/JW45QIWG/2010.html}
}

@online{hafnerMasteringDiverseDomains2023,
  title = {Mastering {{Diverse Domains}} through {{World Models}}},
  author = {Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  date = {2023-01-10},
  number = {arXiv:2301.04104},
  eprint = {arXiv:2301.04104},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.04104},
  urldate = {2023-01-13},
  abstract = {General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision-making problems.},
  pubstate = {preprint},
  version = {1},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/35V47CBM/Hafner et al. - 2023 - Mastering Diverse Domains through World Models.pdf;/Users/lukakuma/Zotero/storage/J5VAFHI9/2301.html}
}

@online{hagenDeepSpeedExtremescaleModel2020,
  title = {{{DeepSpeed}}: {{Extreme-scale}} Model Training for Everyone},
  shorttitle = {{{DeepSpeed}}},
  author = {Hagen, Alexis},
  date = {2020-09-10T20:02:08+00:00},
  url = {https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/},
  urldate = {2022-09-06},
  abstract = {DeepSpeed continues to innovate, making its tools more powerful while broadening its reach. Learn how it now powers 10x bigger model training on one GPU, 10x longer input sequences, 5x less communication volume, \& scales to train trillion-parameter models.},
  langid = {american},
  organization = {{Microsoft Research}}
}

@online{haluptzokLanguageModelsCan2022,
  title = {Language {{Models Can Teach Themselves}} to {{Program Better}}},
  author = {Haluptzok, Patrick and Bowers, Matthew and Kalai, Adam Tauman},
  date = {2022-07-29},
  number = {arXiv:2207.14502},
  eprint = {arXiv:2207.14502},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.14502},
  urldate = {2022-09-20},
  abstract = {This work shows how one can use large-scale language models (LMs) to synthesize programming problems with verified solutions, in the form of programming puzzles, which can then in turn be used to fine-tune those same models, improving their performance. This work builds on two recent developments. First, LMs have achieved breakthroughs in non-trivial reasoning and algorithm implementation, generating code that can solve some intermediate-level competitive programming problems. However, training code LMs involves curated sets of natural-language problem descriptions and source-code tests and solutions, which are limited in size. Second, a new format of programming challenge called a programming puzzle was introduced, which does not require a natural language description and is directly specified by a source-code test. In this work we show how generating synthetic programming puzzles and solutions, verified for correctness by a Python interpreter, can be used to improve performance in solving test puzzles from P3, a public benchmark set of Python Programming Puzzles. Additionally, we release a dataset of 1 million puzzles and solutions generated by the Codex model, which we show can improve smaller models through fine-tuning.},
  pubstate = {preprint},
  keywords = {code,self-play},
  file = {/Users/lukakuma/Zotero/storage/LN3UJG9X/Haluptzok et al. - 2022 - Language Models Can Teach Themselves to Program Be.pdf;/Users/lukakuma/Zotero/storage/KTQZKM64/2207.html}
}

@book{hamiltonGraphRepresentationLearning2020,
  title = {Graph {{Representation Learning}}},
  author = {Hamilton, William L.},
  date = {2020-09-15},
  volume = {14},
  url = {https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046},
  urldate = {2021-04-12},
  abstract = {Graph-structured data is ubiquitous throughout the natural and social sciences, from telecommunication networks to quantum chemistry. Building relational inductive biases into deep learning architectures is crucial if we want systems that can learn, reason, and generalize from this kind of data. Recent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalizations of convolutional neural networks to graph-structured data, and neural message-passing approaches inspired by belief propagation. These advances in graph representation learning have led to new state-of-the-art results in numerous domains, including chemical synthesis, 3D-vision, recommender systems, question answering, and social network analysis.},
  langid = {english},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/TDCJ42QD/Hamilton - 2020 - Graph Representation Learning.pdf}
}

@article{hamiltonInductiveRepresentationLearning2017,
  title = {Inductive {{Representation Learning}} on {{Large Graphs}}},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  date = {2017-06-07},
  url = {https://arxiv.org/abs/1706.02216v4},
  urldate = {2022-03-30},
  abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  langid = {english},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/66KR4HVK/Hamilton et al. - 2017 - Inductive Representation Learning on Large Graphs.pdf}
}

@unpublished{hamiltonRepresentationLearningGraphs2018,
  title = {Representation {{Learning}} on {{Graphs}}: {{Methods}} and {{Applications}}},
  shorttitle = {Representation {{Learning}} on {{Graphs}}},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  date = {2018-04-10},
  eprint = {1709.05584},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1709.05584},
  urldate = {2021-04-13},
  abstract = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
  file = {/Users/lukakuma/Zotero/storage/DUXNU9HD/Hamilton et al. - 2018 - Representation Learning on Graphs Methods and App.pdf;/Users/lukakuma/Zotero/storage/3XXEPCBA/1709.html}
}

@online{hamModulatingPretrainedDiffusion2023,
  title = {Modulating {{Pretrained Diffusion Models}} for {{Multimodal Image Synthesis}}},
  author = {Ham, Cusuh and Hays, James and Lu, Jingwan and Singh, Krishna Kumar and Zhang, Zhifei and Hinz, Tobias},
  date = {2023-02-24},
  number = {arXiv:2302.12764},
  eprint = {arXiv:2302.12764},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.12764},
  urldate = {2023-02-28},
  abstract = {We present multimodal conditioning modules (MCM) for enabling conditional image synthesis using pretrained diffusion models. Previous multimodal synthesis works rely on training networks from scratch or fine-tuning pretrained networks, both of which are computationally expensive for large, state-of-the-art diffusion models. Our method uses pretrained networks but does not require any updates to the diffusion network's parameters. MCM is a small module trained to modulate the diffusion network's predictions during sampling using 2D modalities (e.g., semantic segmentation maps, sketches) that were unseen during the original training of the diffusion model. We show that MCM enables user control over the spatial layout of the image and leads to increased control over the image generation process. Training MCM is cheap as it does not require gradients from the original diffusion net, consists of only \$\textbackslash sim\$1\$\textbackslash\%\$ of the number of parameters of the base diffusion model, and is trained using only a limited number of training examples. We evaluate our method on unconditional and text-conditional models to demonstrate the improved control over the generated images and their alignment with respect to the conditioning inputs.},
  pubstate = {preprint},
  keywords = {*diffusion,2-adapter},
  file = {/Users/lukakuma/Zotero/storage/RKX5WX6C/Ham et al. - 2023 - Modulating Pretrained Diffusion Models for Multimo.pdf;/Users/lukakuma/Zotero/storage/9S8VYEMQ/2302.html}
}

@online{hanProofArtifactCotraining2022,
  title = {Proof {{Artifact Co-training}} for {{Theorem Proving}} with {{Language Models}}},
  author = {Han, Jesse Michael and Rute, Jason and Wu, Yuhuai and Ayers, Edward W. and Polu, Stanislas},
  date = {2022-03-15},
  number = {arXiv:2102.06203},
  eprint = {arXiv:2102.06203},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.06203},
  urldate = {2022-12-22},
  abstract = {Labeled data for imitation learning of theorem proving in large libraries of formalized mathematics is scarce as such libraries require years of concentrated effort by human specialists to be built. This is particularly challenging when applying large Transformer language models to tactic prediction, because the scaling of performance with respect to model size is quickly disrupted in the data-scarce, easily-overfitted regime. We propose PACT (\{\textbackslash bf P\}roof \{\textbackslash bf A\}rtifact \{\textbackslash bf C\}o-\{\textbackslash bf T\}raining), a general methodology for extracting abundant self-supervised data from kernel-level proof terms for co-training alongside the usual tactic prediction objective. We apply this methodology to Lean, an interactive proof assistant which hosts some of the most sophisticated formalized mathematics to date. We instrument Lean with a neural theorem prover driven by a Transformer language model and show that PACT improves theorem proving success rate on a held-out suite of test theorems from 32\textbackslash\% to 48\textbackslash\%.},
  pubstate = {preprint},
  keywords = {*general tools,formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/896KLGDR/Han et al. - 2022 - Proof Artifact Co-training for Theorem Proving wit.pdf;/Users/lukakuma/Zotero/storage/9AJYNE5V/2102.html}
}

@inproceedings{hanScalableSamplingNonsymmetric2021,
  title = {Scalable {{Sampling}} for {{Nonsymmetric Determinantal Point Processes}}},
  author = {Han, Insu and Gartrell, Mike and Gillenwater, Jennifer and Dohmatob, Elvis and Karbasi, Amin},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=BB4e8Atc1eR},
  urldate = {2022-05-15},
  abstract = {A determinantal point process (DPP) on a collection of \$M\$ items is a model, parameterized by a symmetric kernel matrix, that assigns a probability to every subset of those items.  Recent work...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/LXSE2ZNZ/Han et al. - 2021 - Scalable Sampling for Nonsymmetric Determinantal P.pdf;/Users/lukakuma/Zotero/storage/HPVCDJ2G/forum.html}
}

@online{hansenMoDemAcceleratingVisual2022,
  title = {{{MoDem}}: {{Accelerating Visual Model-Based Reinforcement Learning}} with {{Demonstrations}}},
  shorttitle = {{{MoDem}}},
  author = {Hansen, Nicklas and Lin, Yixin and Su, Hao and Wang, Xiaolong and Kumar, Vikash and Rajeswaran, Aravind},
  date = {2022-12-11},
  number = {arXiv:2212.05698},
  eprint = {arXiv:2212.05698},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.05698},
  urldate = {2023-02-15},
  abstract = {Poor sample efficiency continues to be the primary challenge for deployment of deep Reinforcement Learning (RL) algorithms for real-world applications, and in particular for visuo-motor control. Model-based RL has the potential to be highly sample efficient by concurrently learning a world model and using synthetic rollouts for planning and policy improvement. However, in practice, sample-efficient learning with model-based RL is bottlenecked by the exploration challenge. In this work, we find that leveraging just a handful of demonstrations can dramatically improve the sample-efficiency of model-based RL. Simply appending demonstrations to the interaction dataset, however, does not suffice. We identify key ingredients for leveraging demonstrations in model learning -- policy pretraining, targeted exploration, and oversampling of demonstration data -- which forms the three phases of our model-based RL framework. We empirically study three complex visuo-motor control domains and find that our method is 150\%-250\% more successful in completing sparse reward tasks compared to prior approaches in the low data regime (100K interaction steps, 5 demonstrations). Code and videos are available at: https://nicklashansen.github.io/modemrl},
  pubstate = {preprint},
  keywords = {jepa},
  file = {/Users/lukakuma/Zotero/storage/JTY65CFE/Hansen et al. - 2022 - MoDem Accelerating Visual Model-Based Reinforceme.pdf;/Users/lukakuma/Zotero/storage/M7Y4N6MH/2212.html}
}

@online{haoLanguageModelsAre2022a,
  title = {Language {{Models}} Are {{General-Purpose Interfaces}}},
  author = {Hao, Yaru and Song, Haoyu and Dong, Li and Huang, Shaohan and Chi, Zewen and Wang, Wenhui and Ma, Shuming and Wei, Furu},
  date = {2022-06-13},
  number = {arXiv:2206.06336},
  eprint = {arXiv:2206.06336},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.06336},
  urldate = {2023-02-19},
  abstract = {Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/G8W6XTPZ/Hao et al. - 2022 - Language Models are General-Purpose Interfaces.pdf;/Users/lukakuma/Zotero/storage/5HRU3CWX/2206.html}
}

@article{haReinforcementLearningImproving2019,
  title = {Reinforcement {{Learning}} for {{Improving Agent Design}}},
  author = {Ha, David},
  date = {2019-11-01},
  journaltitle = {Artificial Life},
  shortjournal = {Artificial Life},
  volume = {25},
  number = {4},
  pages = {352--365},
  issn = {1064-5462},
  url = {https://doi.org/10.1162/artl_a_00301},
  urldate = {2022-06-13},
  abstract = {In many reinforcement learning tasks, the goal is to learn a policy to manipulate an agent, whose design is fixed, to maximize some notion of cumulative reward. The design of the agent's physical structure is rarely optimized for the task at hand. In this work, we explore the possibility of learning a version of the agent's design that is better suited for its task, jointly with the policy. We propose an alteration to the popular OpenAI Gym framework, where we parameterize parts of an environment, and allow an agent to jointly learn to modify these environment parameters along with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better suited for the task, but also facilitates policy learning. Joint learning of policy and structure may even uncover design principles that are useful for assisted-design applications.},
  file = {/Users/lukakuma/Zotero/storage/AY2P3DTA/Ha - 2019 - Reinforcement Learning for Improving Agent Design.pdf;/Users/lukakuma/Zotero/storage/ZQC5TZXT/Reinforcement-Learning-for-Improving-Agent-Design.html}
}

@online{hashemiGuidedTransformerLeveraging2020,
  title = {Guided {{Transformer}}: {{Leveraging Multiple External Sources}} for {{Representation Learning}} in {{Conversational Search}}},
  shorttitle = {Guided {{Transformer}}},
  author = {Hashemi, Helia and Zamani, Hamed and Croft, W. Bruce},
  date = {2020-06-12},
  number = {arXiv:2006.07548},
  eprint = {arXiv:2006.07548},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.07548},
  urldate = {2023-02-06},
  abstract = {Asking clarifying questions in response to ambiguous or faceted queries has been recognized as a useful technique for various information retrieval systems, especially conversational search systems with limited bandwidth interfaces. Analyzing and generating clarifying questions have been studied recently but the accurate utilization of user responses to clarifying questions has been relatively less explored. In this paper, we enrich the representations learned by Transformer networks using a novel attention mechanism from external information sources that weights each term in the conversation. We evaluate this Guided Transformer model in a conversational search scenario that includes clarifying questions. In our experiments, we use two separate external sources, including the top retrieved documents and a set of different possible clarifying questions for the query. We implement the proposed representation learning model for two downstream tasks in conversational search; document retrieval and next clarifying question selection. Our experiments use a public dataset for search clarification and demonstrate significant improvements compared to competitive baselines.},
  pubstate = {preprint},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/ZF87CSAI/Hashemi et al. - 2020 - Guided Transformer Leveraging Multiple External S.pdf;/Users/lukakuma/Zotero/storage/X2QB98R5/2006.html}
}

@online{hassidMoreWordsIntheWild2022,
  title = {More than {{Words}}: {{In-the-Wild Visually-Driven Prosody}} for {{Text-to-Speech}}},
  shorttitle = {More than {{Words}}},
  author = {Hassid, Michael and Ramanovich, Michelle Tadmor and Shillingford, Brendan and Wang, Miaosen and Jia, Ye and Remez, Tal},
  date = {2022-03-23},
  number = {arXiv:2111.10139},
  eprint = {arXiv:2111.10139},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.10139},
  urldate = {2023-02-07},
  abstract = {In this paper we present VDTTS, a Visually-Driven Text-to-Speech model. Motivated by dubbing, VDTTS takes advantage of video frames as an additional input alongside text, and generates speech that matches the video signal. We demonstrate how this allows VDTTS to, unlike plain TTS models, generate speech that not only has prosodic variations like natural pauses and pitch, but is also synchronized to the input video. Experimentally, we show our model produces well-synchronized outputs, approaching the video-speech synchronization quality of the ground-truth, on several challenging benchmarks including "in-the-wild" content from VoxCeleb2. Supplementary demo videos demonstrating video-speech synchronization, robustness to speaker ID swapping, and prosody, presented at the project page.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/4Y4FDENK/Hassid et al. - 2022 - More than Words In-the-Wild Visually-Driven Proso.pdf;/Users/lukakuma/Zotero/storage/KW2WWBC9/2111.html}
}

@article{hawkinsFrameworkIntelligenceCortical2019,
  title = {A {{Framework}} for {{Intelligence}} and {{Cortical Function Based}} on {{Grid Cells}} in the {{Neocortex}}},
  author = {Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai},
  date = {2019},
  journaltitle = {Frontiers in Neural Circuits},
  volume = {12},
  issn = {1662-5110},
  url = {https://www.frontiersin.org/article/10.3389/fncir.2018.00121},
  urldate = {2022-06-18},
  abstract = {How the neocortex works is a mystery. In this paper we propose a novel framework for understanding its function. Grid cells are neurons in the entorhinal cortex that represent the location of an animal in its environment. Recent evidence suggests that grid cell-like neurons may also be present in the neocortex. We propose that grid cells exist throughout the neocortex, in every region and in every cortical column. They define a location-based framework for how the neocortex functions. Whereas grid cells in the entorhinal cortex represent the location of one thing, the body relative to its environment, we propose that cortical grid cells simultaneously represent the location of many things. Cortical columns in somatosensory cortex track the location of tactile features relative to the object being touched and cortical columns in visual cortex track the location of visual features relative to the object being viewed. We propose that mechanisms in the entorhinal cortex and hippocampus that evolved for learning the structure of environments are now used by the neocortex to learn the structure of objects. Having a representation of location in each cortical column suggests mechanisms for how the neocortex represents object compositionality and object behaviors. It leads to the hypothesis that every part of the neocortex learns complete models of objects and that there are many models of each object distributed throughout the neocortex. The similarity of circuitry observed in all cortical regions is strong evidence that even high-level cognitive tasks are learned and represented in a location-based framework.},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/7WPZV5R6/Hawkins et al. - 2019 - A Framework for Intelligence and Cortical Function.pdf}
}

@article{hawkinsTheoryHowColumns2017,
  title = {A {{Theory}} of {{How Columns}} in the {{Neocortex Enable Learning}} the {{Structure}} of the {{World}}},
  author = {Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
  date = {2017},
  journaltitle = {Frontiers in Neural Circuits},
  volume = {11},
  issn = {1662-5110},
  url = {https://www.frontiersin.org/article/10.3389/fncir.2017.00081},
  urldate = {2022-06-18},
  abstract = {Neocortical regions are organized into columns and layers. Connections between layers run mostly perpendicular to the surface suggesting a columnar functional organization. Some layers have long-range excitatory lateral connections suggesting interactions between columns. Similar patterns of connectivity exist in all regions but their exact role remain a mystery. In this paper, we propose a network model composed of columns and layers that performs robust object learning and recognition. Each column integrates its changing input over time to learn complete predictive models of observed objects. Excitatory lateral connections across columns allow the network to more rapidly infer objects based on the partial knowledge of adjacent columns. Because columns integrate input over time and space, the network learns models of complex objects that extend well beyond the receptive field of individual cells. Our network model introduces a new feature to cortical columns. We propose that a representation of location relative to the object being sensed is calculated within the sub-granular layers of each column. The location signal is provided as an input to the network, where it is combined with sensory data. Our model contains two layers and one or more columns. Simulations show that using Hebbian-like learning rules small single-column networks can learn to recognize hundreds of objects, with each object containing tens of features. Multi-column networks recognize objects with significantly fewer movements of the sensory receptors. Given the ubiquity of columnar and laminar connectivity patterns throughout the neocortex, we propose that columns and regions have more powerful recognition and modeling capabilities than previously assumed.},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/ZCGHQDWN/Hawkins et al. - 2017 - A Theory of How Columns in the Neocortex Enable Le.pdf}
}

@article{hawkinsWhyNeuronsHave2016,
  title = {Why {{Neurons Have Thousands}} of {{Synapses}}, {{A Theory}} of {{Sequence Memory}} in {{Neocortex}}},
  author = {Hawkins, Jeff and Ahmad, Subutai},
  date = {2016-03-30},
  journaltitle = {Frontiers in Neural Circuits},
  shortjournal = {Front. Neural Circuits},
  volume = {10},
  eprint = {1511.00083},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  issn = {1662-5110},
  url = {http://arxiv.org/abs/1511.00083},
  urldate = {2022-06-18},
  abstract = {Neocortical neurons have thousands of excitatory synapses. It is a mystery how neurons integrate the input from so many synapses and what kind of large-scale network behavior this enables. It has been previously proposed that non-linear properties of dendrites enable neurons to recognize multiple patterns. In this paper we extend this idea by showing that a neuron with several thousand synapses arranged along active dendrites can learn to accurately and robustly recognize hundreds of unique patterns of cellular activity, even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where some of the patterns recognized by a neuron lead to action potentials and define the classic receptive field of the neuron, whereas the majority of the patterns recognized by a neuron act as predictions by slightly depolarizing the neuron without immediately generating an action potential. We then present a network model based on neurons with these properties and show that the network learns a robust model of time-based sequences. Given the similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory is a universal property of neocortical tissue. We further propose that cellular layers in the neocortex implement variations of the same sequence memory algorithm to achieve different aspects of inference and behavior. The neuron and network models we introduce are robust over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. The sequence capacity of the network scales linearly with the number of synapses on each neuron. Thus neurons need thousands of synapses to learn the many temporal patterns in sensory stimuli and motor sequences.},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/BFCM5E8A/Hawkins and Ahmad - 2016 - Why Neurons Have Thousands of Synapses, A Theory o.pdf;/Users/lukakuma/Zotero/storage/4TNAVY7M/1511.html}
}

@unpublished{haWorldModels2018,
  title = {World {{Models}}},
  author = {Ha, David and Schmidhuber, J\"urgen},
  date = {2018-03-28},
  eprint = {1803.10122},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.10122},
  urldate = {2022-06-08},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
  file = {/Users/lukakuma/Zotero/storage/D2QT4XCP/Ha and Schmidhuber - 2018 - World Models.pdf;/Users/lukakuma/Zotero/storage/J67MT7CJ/1803.html}
}

@online{hawthorneGeneralpurposeLongcontextAutoregressive2022,
  title = {General-Purpose, Long-Context Autoregressive Modeling with {{Perceiver AR}}},
  author = {Hawthorne, Curtis and Jaegle, Andrew and Cangea, C\u{a}t\u{a}lina and Borgeaud, Sebastian and Nash, Charlie and Malinowski, Mateusz and Dieleman, Sander and Vinyals, Oriol and Botvinick, Matthew and Simon, Ian and Sheahan, Hannah and Zeghidour, Neil and Alayrac, Jean-Baptiste and Carreira, Jo\~ao and Engel, Jesse},
  date = {2022-06-14},
  number = {arXiv:2202.07765},
  eprint = {arXiv:2202.07765},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.07765},
  urldate = {2022-08-02},
  abstract = {Real-world data is high-dimensional: a book, image, or musical performance can easily contain hundreds of thousands of elements even after compression. However, the most commonly used autoregressive models, Transformers, are prohibitively expensive to scale to the number of inputs and layers needed to capture this long-range structure. We develop Perceiver AR, an autoregressive, modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking. Perceiver AR can directly attend to over a hundred thousand tokens, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms. When trained on images or music, Perceiver AR generates outputs with clear long-term coherence and structure. Our architecture also obtains state-of-the-art likelihood on long-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books.},
  pubstate = {preprint},
  keywords = {DeepMind,perceiver,read},
  file = {/Users/lukakuma/Zotero/storage/EJSEB4JQ/Hawthorne et al. - 2022 - General-purpose, long-context autoregressive model.pdf;/Users/lukakuma/Zotero/storage/VJ4EL5ZX/2202.html}
}

@article{hayesMemoryReasoningCategorization2014,
  title = {Memory, Reasoning, and Categorization: Parallels and Common Mechanisms},
  shorttitle = {Memory, Reasoning, and Categorization},
  author = {Hayes, Brett K. and Heit, Evan and Rotello, Caren M.},
  date = {2014},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  volume = {5},
  eprint = {24987380},
  eprinttype = {pmid},
  pages = {529},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00529},
  abstract = {Traditionally, memory, reasoning, and categorization have been treated as separate components of human cognition. We challenge this distinction, arguing that there is broad scope for crossover between the methods and theories developed for each task. The links between memory and reasoning are illustrated in a review of two lines of research. The first takes theoretical ideas (two-process accounts) and methodological tools (signal detection analysis, receiver operating characteristic curves) from memory research and applies them to important issues in reasoning research: relations between induction and deduction, and the belief bias effect. The second line of research introduces a task in which subjects make either memory or reasoning judgments for the same set of stimuli. Other than broader generalization for reasoning than memory, the results were similar for the two tasks, across a variety of experimental stimuli and manipulations. It was possible to simultaneously explain performance on both tasks within a single cognitive architecture, based on exemplar-based comparisons of similarity. The final sections explore evidence for empirical and processing links between inductive reasoning and categorization and between categorization and recognition. An important implication is that progress in all three of these fields will be expedited by further investigation of the many commonalities between these tasks.},
  langid = {english},
  pmcid = {PMC4060413},
  file = {/Users/lukakuma/Zotero/storage/42KG8NFM/Hayes et al. - 2014 - Memory, reasoning, and categorization parallels a.pdf}
}

@article{heAutoMLSurveyStateoftheArt2021,
  title = {{{AutoML}}: {{A Survey}} of the {{State-of-the-Art}}},
  shorttitle = {{{AutoML}}},
  author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  date = {2021-01},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {212},
  eprint = {1908.00709},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {106622},
  issn = {09507051},
  url = {http://arxiv.org/abs/1908.00709},
  urldate = {2022-07-27},
  abstract = {Deep learning (DL) techniques have penetrated all aspects of our lives and brought us great convenience. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering the applications of DL to more areas. Automated machine learning (AutoML) becomes a promising solution to build a DL system without human assistance, and a growing number of researchers focus on AutoML. In this paper, we provide a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. First, we introduce AutoML methods according to the pipeline, covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS). We focus more on NAS, as it is currently very hot sub-topic of AutoML. We summarize the performance of the representative NAS algorithms on the CIFAR-10 and ImageNet datasets and further discuss several worthy studying directions of NAS methods: one/two-stage NAS, one-shot NAS, and joint hyperparameter and architecture optimization. Finally, we discuss some open problems of the existing AutoML methods for future research.},
  file = {/Users/lukakuma/Zotero/storage/YKWI4JT3/He et al. - 2021 - AutoML A Survey of the State-of-the-Art.pdf;/Users/lukakuma/Zotero/storage/3LX75SGP/1908.html}
}

@online{heContinuousNeuralAlgorithmic2022,
  title = {Continuous {{Neural Algorithmic Planners}}},
  author = {He, Yu and Veli\v{c}kovi\'c, Petar and Li\`o, Pietro and Deac, Andreea},
  date = {2022-11-28},
  number = {arXiv:2211.15839},
  eprint = {arXiv:2211.15839},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.15839},
  urldate = {2022-12-22},
  abstract = {Neural algorithmic reasoning studies the problem of learning algorithms with neural networks, especially with graph architectures. A recent proposal, XLVIN, reaps the benefits of using a graph neural network that simulates the value iteration algorithm in deep reinforcement learning agents. It allows model-free planning without access to privileged information about the environment, which is usually unavailable. However, XLVIN only supports discrete action spaces, and is hence nontrivially applicable to most tasks of real-world interest. We expand XLVIN to continuous action spaces by discretization, and evaluate several selective expansion policies to deal with the large planning graphs. Our proposal, CNAP, demonstrates how neural algorithmic reasoning can make a measurable impact in higher-dimensional continuous control settings, such as MuJoCo, bringing gains in low-data settings and outperforming model-free baselines.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/JW8FHYXB/He et al. - 2022 - Continuous Neural Algorithmic Planners.pdf;/Users/lukakuma/Zotero/storage/FSHUJ2X6/2211.html}
}

@inproceedings{heEffectivenessAdapterbasedTuning2021,
  title = {On the {{Effectiveness}} of {{Adapter-based Tuning}} for {{Pretrained Language Model Adaptation}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {He, Ruidan and Liu, Linlin and Ye, Hai and Tan, Qingyu and Ding, Bosheng and Cheng, Liying and Low, Jiawei and Bing, Lidong and Si, Luo},
  date = {2021-08},
  pages = {2208--2222},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  url = {https://aclanthology.org/2021.acl-long.172},
  urldate = {2023-03-01},
  abstract = {Adapter-based tuning has recently arisen as an alternative to fine-tuning. It works by adding light-weight adapter modules to a pretrained language model (PrLM) and only updating the parameters of adapter modules when learning on a downstream task. As such, it adds only a few trainable parameters per new task, allowing a high degree of parameter sharing. Prior studies have shown that adapter-based tuning often achieves comparable results to fine-tuning. However, existing work only focuses on the parameter-efficient aspect of adapter-based tuning while lacking further investigation on its effectiveness. In this paper, we study the latter. We first show that adapter-based tuning better mitigates forgetting issues than fine-tuning since it yields representations with less deviation from those generated by the initial PrLM. We then empirically compare the two tuning methods on several downstream NLP tasks and settings. We demonstrate that 1) adapter-based tuning outperforms fine-tuning on low-resource and cross-lingual tasks; 2) it is more robust to overfitting and less sensitive to changes in learning rates.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  keywords = {2-adapter},
  file = {/Users/lukakuma/Zotero/storage/9RMXYXZH/He et al. - 2021 - On the Effectiveness of Adapter-based Tuning for P.pdf}
}

@online{heessEmergenceLocomotionBehaviours2017,
  title = {Emergence of {{Locomotion Behaviours}} in {{Rich Environments}}},
  author = {Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, S. M. Ali and Riedmiller, Martin and Silver, David},
  date = {2017-07-10},
  number = {arXiv:1707.02286},
  eprint = {arXiv:1707.02286},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1707.02286},
  urldate = {2022-07-20},
  abstract = {The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx\_bgoTF7bs .},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/UMUXRDHK/Heess et al. - 2017 - Emergence of Locomotion Behaviours in Rich Environ.pdf;/Users/lukakuma/Zotero/storage/SYR72XT3/1707.html}
}

@unpublished{heFastMoEFastMixtureofExpert2021,
  title = {{{FastMoE}}: {{A Fast Mixture-of-Expert Training System}}},
  shorttitle = {{{FastMoE}}},
  author = {He, Jiaao and Qiu, Jiezhong and Zeng, Aohan and Yang, Zhilin and Zhai, Jidong and Tang, Jie},
  date = {2021-03-24},
  eprint = {2103.13262},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.13262},
  urldate = {2022-04-14},
  abstract = {Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of language model to trillions of parameters. However, training trillion-scale MoE requires algorithm and system co-design for a well-tuned high performance distributed training system. Unfortunately, the only existing platform that meets the requirements strongly depends on Google's hardware (TPU) and software (Mesh Tensorflow) stack, and is not open and available to the public, especially GPU and PyTorch communities. In this paper, we present FastMoE, a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and easy adaption to different applications, such as Transformer-XL and Megatron-LM. Different from direct implementation of MoE models using PyTorch, the training speed is highly optimized in FastMoE by sophisticated high-performance acceleration skills. The system supports placing different experts on multiple GPUs across multiple nodes, enabling enlarging the number of experts linearly against the number of GPUs. The source of FastMoE is available at https://github.com/laekov/fastmoe under Apache-2 license.},
  keywords = {sparsity},
  file = {/Users/lukakuma/Zotero/storage/4KYHUXL5/He et al. - 2021 - FastMoE A Fast Mixture-of-Expert Training System.pdf;/Users/lukakuma/Zotero/storage/WY53JTXY/2103.html}
}

@online{heMakingDeepLearning2022,
  title = {Making {{Deep Learning}} Go {{Brrrr From First Principles}}},
  author = {He, Horace},
  date = {2022},
  url = {https://horace.io/brrr_intro.html},
  urldate = {2022-09-07},
  file = {/Users/lukakuma/Zotero/storage/ZBGB2IRF/brrr_intro.html}
}

@article{heMomentumContrastUnsupervised2019,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  date = {2019-11-13},
  url = {https://arxiv.org/abs/1911.05722v3},
  urldate = {2022-04-25},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/LNEN5DIP/He et al. - 2019 - Momentum Contrast for Unsupervised Visual Represen.pdf;/Users/lukakuma/Zotero/storage/72UWTJ7V/1911.html}
}

@online{hendrycksGaussianErrorLinear2020,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  date = {2020-07-08},
  number = {arXiv:1606.08415},
  eprint = {arXiv:1606.08415},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.08415},
  urldate = {2022-09-19},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x\textbackslash Phi(x)\$, where \$\textbackslash Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x\textbackslash mathbf\{1\}\_\{x{$>$}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/CJ9BSLYF/Hendrycks and Gimpel - 2020 - Gaussian Error Linear Units (GELUs).pdf;/Users/lukakuma/Zotero/storage/RNC7MIUW/1606.html}
}

@online{hendrycksMeasuringMassiveMultitask2021,
  title = {Measuring {{Massive Multitask Language Understanding}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  date = {2021-01-12},
  number = {arXiv:2009.03300},
  eprint = {arXiv:2009.03300},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.03300},
  urldate = {2022-11-22},
  abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/KR8EH7UV/Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf;/Users/lukakuma/Zotero/storage/F9APQ5HL/2009.html}
}

@online{hendrycksMeasuringMathematicalProblem2021,
  title = {Measuring {{Mathematical Problem Solving With}} the {{MATH Dataset}}},
  author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  date = {2021-11-08},
  number = {arXiv:2103.03874},
  eprint = {arXiv:2103.03874},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.03874},
  urldate = {2022-11-22},
  abstract = {Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/QUHFVMK4/Hendrycks et al. - 2021 - Measuring Mathematical Problem Solving With the MA.pdf;/Users/lukakuma/Zotero/storage/R8X3553H/2103.html}
}

@online{henighanScalingLawsAutoregressive2020,
  title = {Scaling {{Laws}} for {{Autoregressive Generative Modeling}}},
  author = {Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B. and Dhariwal, Prafulla and Gray, Scott and Hallacy, Chris and Mann, Benjamin and Radford, Alec and Ramesh, Aditya and Ryder, Nick and Ziegler, Daniel M. and Schulman, John and Amodei, Dario and McCandlish, Sam},
  date = {2020-11-05},
  number = {arXiv:2010.14701},
  eprint = {arXiv:2010.14701},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.14701},
  urldate = {2022-11-02},
  abstract = {We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image{$\leftrightarrow$}text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains.},
  langid = {english},
  pubstate = {preprint},
  keywords = {read,scaling law},
  file = {/Users/lukakuma/Zotero/storage/SG84FJLX/Henighan et al. - 2020 - Scaling Laws for Autoregressive Generative Modelin.pdf;/Users/lukakuma/Zotero/storage/F4EW8F7X/2010.html}
}

@online{heParameterefficientModelAdaptation2022,
  title = {Parameter-Efficient {{Model Adaptation}} for {{Vision Transformers}}},
  author = {He, Xuehai and Li, Chunyuan and Zhang, Pengchuan and Yang, Jianwei and Wang, Xin Eric},
  date = {2022-12-04},
  number = {arXiv:2203.16329},
  eprint = {arXiv:2203.16329},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.16329},
  urldate = {2023-03-03},
  abstract = {In computer vision, it has achieved great transfer learning performance via adapting large-scale pretrained vision models (e.g., vision transformers) to downstream tasks. Common approaches for model adaptation either update all model parameters or leverage linear probes. In this paper, we aim to study parameter-efficient model adaptation strategies for vision transformers on the image classification task. We formulate efficient model adaptation as a subspace training problem and perform a comprehensive benchmarking over different efficient adaptation methods. We conduct an empirical study on each efficient model adaptation method focusing on its performance alongside parameter cost. Furthermore, we propose a parameter-efficient model adaptation framework, which first selects submodules by measuring local intrinsic dimensions and then projects them into subspace for further decomposition via a novel Kronecker Adaptation (KAdaptation) method. We analyze and compare our method with a diverse set of baseline model adaptation methods (including state-of-the-art methods for pretrained language models). Our method performs the best in terms of the tradeoff between accuracy and parameter efficiency across 20 image classification datasets under the few-shot setting and 7 image classification datasets under the full-shot setting.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/N3RNWTDJ/He et al. - 2022 - Parameter-efficient Model Adaptation for Vision Tr.pdf;/Users/lukakuma/Zotero/storage/3IPAMKP2/2203.html}
}

@online{heRethinkingRetrievalFaithful2022,
  title = {Rethinking with {{Retrieval}}: {{Faithful Large Language Model Inference}}},
  shorttitle = {Rethinking with {{Retrieval}}},
  author = {He, Hangfeng and Zhang, Hongming and Roth, Dan},
  date = {2022-12-31},
  number = {arXiv:2301.00303},
  eprint = {arXiv:2301.00303},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.00303},
  urldate = {2023-01-06},
  abstract = {Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.},
  pubstate = {preprint},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/RH2JXL5A/He et al. - 2022 - Rethinking with Retrieval Faithful Large Language.pdf;/Users/lukakuma/Zotero/storage/L8DXVKUP/2301.html}
}

@online{hernandezScalingLawsInterpretability2022,
  title = {Scaling {{Laws}} and {{Interpretability}} of {{Learning}} from {{Repeated Data}}},
  author = {Hernandez, Danny and Brown, Tom and Conerly, Tom and DasSarma, Nova and Drain, Dawn and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Henighan, Tom and Hume, Tristan and Johnston, Scott and Mann, Ben and Olah, Chris and Olsson, Catherine and Amodei, Dario and Joseph, Nicholas and Kaplan, Jared and McCandlish, Sam},
  date = {2022-05-20},
  number = {arXiv:2205.10487},
  eprint = {arXiv:2205.10487},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.10487},
  urldate = {2022-12-15},
  abstract = {Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1\% of the data 100 times, despite the other 90\% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance.},
  pubstate = {preprint},
  keywords = {scaling law},
  file = {/Users/lukakuma/Zotero/storage/7ABMWXBT/Hernandez et al. - 2022 - Scaling Laws and Interpretability of Learning from.pdf;/Users/lukakuma/Zotero/storage/GLQPGH9Z/2205.html}
}

@online{hernandezScalingLawsTransfer2021,
  title = {Scaling {{Laws}} for {{Transfer}}},
  author = {Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
  date = {2021-02-01},
  number = {arXiv:2102.01293},
  eprint = {arXiv:2102.01293},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.01293},
  urldate = {2022-08-09},
  abstract = {We study empirical scaling laws for transfer learning between distributions in an unsupervised, fine-tuning setting. When we train increasingly large neural networks from-scratch on a fixed-size dataset, they eventually become data-limited and stop improving in performance (cross-entropy loss). When we do the same for models pre-trained on a large language dataset, the slope in performance gains is merely reduced rather than going to zero. We calculate the effective data "transferred" from pre-training by determining how much data a transformer of the same size would have required to achieve the same loss when training from scratch. In other words, we focus on units of data while holding everything else fixed. We find that the effective data transferred is described well in the low data regime by a power-law of parameter count and fine-tuning dataset size. We believe the exponents in these power-laws correspond to measures of the generality of a model and proximity of distributions (in a directed rather than symmetric sense). We find that pre-training effectively multiplies the fine-tuning dataset size. Transfer, like overall performance, scales predictably in terms of parameters, data, and compute.},
  pubstate = {preprint},
  keywords = {scaling law},
  file = {/Users/lukakuma/Zotero/storage/X89J28YE/Hernandez et al. - 2021 - Scaling Laws for Transfer.pdf;/Users/lukakuma/Zotero/storage/PE46FSSM/2102.html}
}

@online{hesselRainbowCombiningImprovements2017,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Rainbow},
  author = {Hessel, Matteo and Modayil, Joseph and family=Hasselt, given=Hado, prefix=van, useprefix=true and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  date = {2017-10-06},
  number = {arXiv:1710.02298},
  eprint = {arXiv:1710.02298},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.02298},
  urldate = {2022-07-20},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/KKW3D2KP/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf;/Users/lukakuma/Zotero/storage/XWJHPG7R/1710.html}
}

@online{heUnifiedViewParameterEfficient2022a,
  title = {Towards a {{Unified View}} of {{Parameter-Efficient Transfer Learning}}},
  author = {He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  date = {2022-02-02},
  number = {arXiv:2110.04366},
  eprint = {arXiv:2110.04366},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.04366},
  urldate = {2022-12-13},
  abstract = {Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.},
  pubstate = {preprint},
  keywords = {*peft,read},
  file = {/Users/lukakuma/Zotero/storage/UDRXLUTY/He et al. - 2022 - Towards a Unified View of Parameter-Efficient Tran.pdf;/Users/lukakuma/Zotero/storage/58A8KY4C/2110.html}
}

@article{hilgetagHierarchyOrganizationBrain2020,
  title = {`{{Hierarchy}}' in the Organization of Brain Networks},
  author = {Hilgetag, Claus C. and Goulas, Alexandros},
  date = {2020-04-13},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {375},
  number = {1796},
  pages = {20190319},
  publisher = {{Royal Society}},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0319},
  urldate = {2022-06-18},
  abstract = {Concepts shape the interpretation of facts. One of the most popular concepts in systems neuroscience is that of `hierarchy'. However, this concept has been interpreted in many different ways, which are not well aligned. This observation suggests that the concept is ill defined. Using the example of the organization of the primate visual cortical system, we explore several contexts in which `hierarchy' is currently used in the description of brain networks. We distinguish at least four different uses, specifically, `hierarchy' as a topological sequence of projections, as a gradient of features, as a progression of scales, or as a sorting of laminar projection patterns. We discuss the interpretation and functional implications of the different notions of `hierarchy' in these contexts and suggest that more specific terms than `hierarchy' should be used for a deeper understanding of the different dimensions of the organization of brain networks. This article is part of the theme issue `Unifying the essential concepts of biological networks: biological insights and philosophical foundations'.},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/6KPBVLJD/Hilgetag and Goulas - 2020 - ‚ÄòHierarchy‚Äô in the organization of brain networks.pdf}
}

@article{hillsonBuildingGlobalAlliance2019,
  title = {Building a Global Alliance of Biofoundries},
  author = {Hillson, Nathan and Caddick, Mark and Cai, Yizhi and Carrasco, Jose A. and Chang, Matthew Wook and Curach, Natalie C. and Bell, David J. and Le Feuvre, Rosalind and Friedman, Douglas C. and Fu, Xiongfei and Gold, Nicholas D. and Herrg\aa rd, Markus J. and Holowko, Maciej B. and Johnson, James R. and Johnson, Richard A. and Keasling, Jay D. and Kitney, Richard I. and Kondo, Akihiko and Liu, Chenli and Martin, Vincent J. J. and Menolascina, Filippo and Ogino, Chiaki and Patron, Nicola J. and Pavan, Marilene and Poh, Chueh Loo and Pretorius, Isak S. and Rosser, Susan J. and Scrutton, Nigel S. and Storch, Marko and Tekotte, Hille and Travnik, Evelyn and Vickers, Claudia E. and Yew, Wen Shan and Yuan, Yingjin and Zhao, Huimin and Freemont, Paul S.},
  date = {2019-05-09},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {10},
  number = {1},
  pages = {2040},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  url = {https://www.nature.com/articles/s41467-019-10079-2},
  urldate = {2022-05-23},
  abstract = {Biofoundries provide an integrated infrastructure to enable the rapid design, construction, and testing of genetically reprogrammed organisms for biotechnology applications and research. Many biofoundries are being built and a Global Biofoundry Alliance has recently been established to coordinate activities worldwide.},
  issue = {1},
  langid = {english},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/LLSCSEYU/Hillson et al. - 2019 - Building a global alliance of biofoundries.pdf}
}

@online{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date = {2015-03-09},
  number = {arXiv:1503.02531},
  eprint = {arXiv:1503.02531},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2022-12-20},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  pubstate = {preprint},
  keywords = {student teacher},
  file = {/Users/lukakuma/Zotero/storage/XLMGW784/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;/Users/lukakuma/Zotero/storage/ZRLWTT76/1503.html}
}

@article{hintonForwardForwardAlgorithmPreliminary,
  title = {The {{Forward-Forward Algorithm}}: {{Some Preliminary Investigations}}},
  author = {Hinton, Geoffrey},
  date = {2022-12},
  pages = {17},
  abstract = {The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth serious investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes can be separated in time, the negative passes can be done offline, which makes the learning much simpler in the positive pass and allows video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.},
  langid = {english},
  keywords = {optimizer},
  file = {/Users/lukakuma/Zotero/storage/XZSLI942/Hinton - The Forward-Forward Algorithm Some Preliminary In.pdf}
}

@unpublished{hintonHowRepresentPartwhole2021,
  title = {How to Represent Part-Whole Hierarchies in a Neural Network},
  author = {Hinton, Geoffrey},
  date = {2021-02-24},
  number = {arXiv:2102.12627},
  eprint = {2102.12627},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2102.12627},
  urldate = {2022-06-14},
  abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language},
  file = {/Users/lukakuma/Zotero/storage/E75T682B/Hinton - 2021 - How to represent part-whole hierarchies in a neura.pdf;/Users/lukakuma/Zotero/storage/VF9HMMLB/2102.html}
}

@article{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-06-19},
  url = {https://arxiv.org/abs/2006.11239v2},
  urldate = {2023-02-24},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  langid = {english},
  keywords = {*diffusion},
  file = {/Users/lukakuma/Zotero/storage/I63VN936/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@unpublished{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and family=Driessche, given=George, prefix=van den, useprefix=false and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  date = {2022-03-29},
  eprint = {2203.15556},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.15556},
  urldate = {2022-03-30},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \textbackslash nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \textbackslash chinchilla, that uses the same compute budget as \textbackslash gopher but with 70B parameters and 4\$\textbackslash times\$ more more data. \textbackslash chinchilla uniformly and significantly outperforms \textbackslash Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \textbackslash chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \textbackslash chinchilla reaches a state-of-the-art average accuracy of 67.5\textbackslash\% on the MMLU benchmark, greater than a 7\textbackslash\% improvement over \textbackslash gopher.},
  keywords = {(ext) Flamingo,compute efficient method,DeepMind,read,scaling law},
  file = {/Users/lukakuma/Zotero/storage/5BPSLNDK/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf;/Users/lukakuma/Zotero/storage/X4H44YGK/2203.html}
}

@online{hoLargeLanguageModels2022,
  title = {Large {{Language Models Are Reasoning Teachers}}},
  author = {Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
  date = {2022-12-20},
  number = {arXiv:2212.10071},
  eprint = {arXiv:2212.10071},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.10071},
  urldate = {2022-12-28},
  abstract = {Language models (LMs) have demonstrated remarkable performance on downstream tasks, using in-context exemplars or human instructions. Recent works have shown that chain-of-thought (CoT) prompting can elicit models to solve complex reasoning tasks, step-by-step. However, the efficacy of prompt-based CoT methods is restricted to very large LMs such as GPT-3 (175B), thus limiting deployability. In this paper, we revisit the fine-tuning approach to enable complex reasoning in smaller LMs, optimized to efficiently perform a specific task. We propose Fine-tune-CoT, a method that leverages the capabilities of very large LMs to generate reasoning samples and teach smaller models via fine-tuning. We evaluate our method on publicly available LMs across a wide range of complex tasks and model sizes. We find that Fine-tune-CoT enables substantial reasoning capability in small models, whereas previous prompt-based baselines exhibit near-random performance. Student models can even outperform the teacher in some tasks while reducing model size requirements by several orders of magnitude. We conduct extensive ablations and sample studies to understand the reasoning capabilities of student models. We also identify several important nuances that have been overlooked in concurrent fine-tuning works on CoT and address them in our analysis.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/TKT4ZP2D/Ho et al. - 2022 - Large Language Models Are Reasoning Teachers.pdf;/Users/lukakuma/Zotero/storage/DZSNM6B7/2212.html}
}

@article{holeThousandBrainsBiologically2021,
  title = {A Thousand Brains: Toward Biologically Constrained {{AI}}},
  shorttitle = {A Thousand Brains},
  author = {Hole, Kjell J\o rgen and Ahmad, Subutai},
  date = {2021-07-20},
  journaltitle = {SN Applied Sciences},
  shortjournal = {SN Appl. Sci.},
  volume = {3},
  number = {8},
  pages = {743},
  issn = {2523-3971},
  url = {https://doi.org/10.1007/s42452-021-04715-0},
  urldate = {2022-06-13},
  abstract = {This paper reviews the state of artificial intelligence (AI) and the quest to create general AI with human-like cognitive capabilities. Although existing AI methods have produced powerful applications that outperform humans in specific bounded domains, these techniques have fundamental limitations that hinder the creation of general intelligent systems. In parallel, over the last few decades, an explosion of experimental techniques in neuroscience has significantly increased our understanding of the human brain. This review argues that improvements in current AI using mathematical or logical techniques are unlikely to lead to general AI. Instead, the AI community should incorporate neuroscience discoveries about the neocortex, the human brain's center of intelligence. The article explains the limitations of current AI techniques. It then focuses on the biologically constrained Thousand Brains Theory describing the neocortex's computational principles. Future AI systems can incorporate these principles to overcome the stated limitations of current systems. Finally, the article concludes that AI researchers and neuroscientists should work together on specified topics to achieve biologically constrained AI with human-like capabilities.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/YXPL9BAX/Hole and Ahmad - 2021 - A thousand brains toward biologically constrained.pdf}
}

@article{holowkoBuildingBiofoundry2020,
  title = {Building a Biofoundry},
  author = {Holowko, Maciej B and Frow, Emma K and Reid, Janet C and Rourke, Michelle and Vickers, Claudia E},
  date = {2020-12-16},
  journaltitle = {Synthetic Biology},
  shortjournal = {Synth Biol (Oxf)},
  volume = {6},
  number = {1},
  eprint = {33817343},
  eprinttype = {pmid},
  pages = {ysaa026},
  issn = {2397-7000},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7998708/},
  urldate = {2022-05-24},
  abstract = {A biofoundry provides automation and analytics infrastructure to support the engineering of biological systems. It allows scientists to perform synthetic biology and aligned experimentation on a high-throughput scale, massively increasing the solution space that can be examined for any given problem or question. However, establishing a biofoundry is a challenging undertaking, with numerous technical and operational considerations that must be addressed. Using collated learnings, here we outline several considerations that should be addressed prior to and during establishment. These include drivers for establishment, institutional models, funding and revenue models, personnel, hardware and software, data management, interoperability, client engagement and biosecurity issues. The high cost of establishment and operation means that developing a long-term business model for biofoundry sustainability in the context of funding frameworks, actual and potential client base, and costing structure is critical. Moreover, since biofoundries are leading a conceptual shift in experimental design for bioengineering, sustained outreach and engagement with the research community are needed to grow the client base. Recognition of the significant, long-term financial investment required and an understanding of the complexities of operationalization is critical for a sustainable biofoundry venture. To ensure state-of-the-art technology is integrated into planning, extensive engagement with existing facilities and community groups, such as the Global Biofoundries Alliance, is recommended.},
  pmcid = {PMC7998708},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/XN6KSXUW/Holowko et al. - 2020 - Building a biofoundry.pdf}
}

@online{holtzmanCuriousCaseNeural2020,
  title = {The {{Curious Case}} of {{Neural Text Degeneration}}},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  date = {2020-02-14},
  number = {arXiv:1904.09751},
  eprint = {arXiv:1904.09751},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.09751},
  urldate = {2022-12-27},
  abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/8D53ETJC/Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf;/Users/lukakuma/Zotero/storage/39NR7LAP/1904.html}
}

@online{holtzmanSurfaceFormCompetition2022,
  title = {Surface {{Form Competition}}: {{Why}} the {{Highest Probability Answer Isn}}'t {{Always Right}}},
  shorttitle = {Surface {{Form Competition}}},
  author = {Holtzman, Ari and West, Peter and Shwartz, Vered and Choi, Yejin and Zettlemoyer, Luke},
  date = {2022-10-31},
  number = {arXiv:2104.08315},
  eprint = {arXiv:2104.08315},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.08315},
  urldate = {2022-11-02},
  abstract = {Large language models have shown promising results in zero-shot settings (Brown et al., 2020; Radford et al., 2019). For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/PQC9HDSJ/Holtzman et al. - 2022 - Surface Form Competition Why the Highest Probabil.pdf}
}

@inproceedings{honovichEvaluatingFactualConsistency2021,
  title = {Q2: {{Evaluating Factual Consistency}} in {{Knowledge-Grounded Dialogues}} via {{Question Generation}} and {{Question Answering}}},
  shorttitle = {\textsuperscript{2\$}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Honovich, Or and Choshen, Leshem and Aharoni, Roee and Neeman, Ella and Szpektor, Idan and Abend, Omri},
  date = {2021-11},
  pages = {7856--7870},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online and Punta Cana, Dominican Republic}},
  url = {https://aclanthology.org/2021.emnlp-main.619},
  urldate = {2022-12-05},
  abstract = {Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and question answering. Our metric, denoted \$Q\^2\$, compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of \$Q\^2\$ against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements.},
  eventtitle = {{{EMNLP}} 2021},
  keywords = {QA},
  file = {/Users/lukakuma/Zotero/storage/QD2VE6ZN/Honovich et al. - 2021 - 2$ Evaluating Factual Consistency in K.pdf}
}

@online{honovichUnnaturalInstructionsTuning2022,
  title = {Unnatural {{Instructions}}: {{Tuning Language Models}} with ({{Almost}}) {{No Human Labor}}},
  shorttitle = {Unnatural {{Instructions}}},
  author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  date = {2022-12-19},
  number = {arXiv:2212.09689},
  eprint = {arXiv:2212.09689},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.09689},
  urldate = {2022-12-27},
  abstract = {Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/HSZCH7R5/Honovich et al. - 2022 - Unnatural Instructions Tuning Language Models wit.pdf;/Users/lukakuma/Zotero/storage/XWV34AI8/2212.html}
}

@unpublished{hopeComputationalInflectionScientific2022,
  title = {A {{Computational Inflection}} for {{Scientific Discovery}}},
  author = {Hope, Tom and Downey, Doug and Etzioni, Oren and Weld, Daniel S. and Horvitz, Eric},
  date = {2022-05-04},
  eprint = {2205.02007},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.02007},
  urldate = {2022-05-10},
  abstract = {We stand at the foot of a significant inflection in the trajectory of scientific discovery. As society continues on its fast-paced digital transformation, so does humankind's collective scientific knowledge and discourse. We now read and write papers in digitized form, and a great deal of the formal and informal processes of science are captured digitally -- including papers, preprints and books, code and datasets, conference presentations, and interactions in social networks and communication platforms. The transition has led to the growth of a tremendous amount of information, opening exciting opportunities for computational models and systems that analyze and harness it. In parallel, exponential growth in data processing power has fueled remarkable advances in AI, including self-supervised neural models capable of learning powerful representations from large-scale unstructured text without costly human supervision. The confluence of societal and computational trends suggests that computer science is poised to ignite a revolution in the scientific process itself. However, the explosion of scientific data, results and publications stands in stark contrast to the constancy of human cognitive capacity. While scientific knowledge is expanding with rapidity, our minds have remained static, with severe limitations on the capacity for finding, assimilating and manipulating information. We propose a research agenda of task-guided knowledge retrieval, in which systems counter humans' bounded capacity by ingesting corpora of scientific knowledge and retrieving inspirations, explanations, solutions and evidence synthesized to directly augment human performance on salient tasks in scientific endeavors. We present initial progress on methods and prototypes, and lay out important opportunities and challenges ahead with computational approaches that have the potential to revolutionize science.},
  file = {/Users/lukakuma/Zotero/storage/SERXUZA3/Hope et al. - 2022 - A Computational Inflection for Scientific Discover.pdf;/Users/lukakuma/Zotero/storage/4AUXMRNN/2205.html}
}

@article{hoPeopleConstructSimplified2022,
  title = {People Construct Simplified Mental Representations to Plan},
  author = {Ho, Mark K. and Abel, David and Correa, Carlos G. and Littman, Michael L. and Cohen, Jonathan D. and Griffiths, Thomas L.},
  date = {2022-05-19},
  journaltitle = {Nature},
  pages = {1--8},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/s41586-022-04743-9},
  urldate = {2022-05-21},
  abstract = {One of the most striking features of human cognition is the ability to plan. Two aspects of human planning stand out\textemdash its efficiency and flexibility. Efficiency is especially impressive because plans must often be made in complex environments, and yet people successfully plan solutions to many everyday problems despite having limited cognitive resources1\textendash 3. Standard accounts in psychology, economics and artificial intelligence have suggested that human planning succeeds because people have a complete representation of a task and then use heuristics to plan future actions in that representation4\textendash 11. However, this approach generally assumes that task representations are fixed. Here we propose that task representations can be controlled and that such control provides opportunities to quickly simplify problems and more easily reason about them. We propose a computational account of this simplification process and, in a series of preregistered behavioural experiments, show that it is subject to online cognitive control12\textendash 14 and that people optimally balance the complexity of a task representation and its utility for planning and acting. These results demonstrate how strategically perceiving and conceiving problems facilitates the effective use of limited cognitive resources.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/MMQZNPEL/Ho et al. - 2022 - People construct simplified mental representations.pdf;/Users/lukakuma/Zotero/storage/QGE9SQVB/s41586-022-04743-9.html}
}

@online{hospedalesMetaLearningNeuralNetworks2020,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  date = {2020-11-07},
  number = {arXiv:2004.05439},
  eprint = {arXiv:2004.05439},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.05439},
  urldate = {2022-07-20},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/3C8T69JP/Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf;/Users/lukakuma/Zotero/storage/6CMPZDU7/2004.html}
}

@online{houlsbyParameterEfficientTransferLearning2019a,
  title = {Parameter-{{Efficient Transfer Learning}} for {{NLP}}},
  author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and family=Laroussilhe, given=Quentin, prefix=de, useprefix=true and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  date = {2019-06-13},
  number = {arXiv:1902.00751},
  eprint = {arXiv:1902.00751},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1902.00751},
  urldate = {2023-02-17},
  abstract = {Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4\% of the performance of full fine-tuning, adding only 3.6\% parameters per task. By contrast, fine-tuning trains 100\% of the parameters per task.},
  pubstate = {preprint},
  keywords = {2-adapter},
  file = {/Users/lukakuma/Zotero/storage/RAKTB3JI/Houlsby et al. - 2019 - Parameter-Efficient Transfer Learning for NLP.pdf;/Users/lukakuma/Zotero/storage/RQLJ4KS8/1902.html}
}

@online{houthooftEvolvedPolicyGradients2018,
  title = {Evolved {{Policy Gradients}}},
  author = {Houthooft, Rein and Chen, Richard Y. and Isola, Phillip and Stadie, Bradly C. and Wolski, Filip and Ho, Jonathan and Abbeel, Pieter},
  date = {2018-04-29},
  number = {arXiv:1802.04821},
  eprint = {arXiv:1802.04821},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.04821},
  urldate = {2022-07-23},
  abstract = {We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/2WTEQFWY/Houthooft et al. - 2018 - Evolved Policy Gradients.pdf;/Users/lukakuma/Zotero/storage/5FY9AWYX/1802.html}
}

@online{hsuReVISESelfSupervisedSpeech2022,
  title = {{{ReVISE}}: {{Self-Supervised Speech Resynthesis}} with {{Visual Input}} for {{Universal}} and {{Generalized Speech Enhancement}}},
  shorttitle = {{{ReVISE}}},
  author = {Hsu, Wei-Ning and Remez, Tal and Shi, Bowen and Donley, Jacob and Adi, Yossi},
  date = {2022-12-21},
  number = {arXiv:2212.11377},
  eprint = {arXiv:2212.11377},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.11377},
  urldate = {2023-02-15},
  abstract = {Prior works on improving speech quality with visual input typically study each type of auditory distortion separately (e.g., separation, inpainting, video-to-speech) and present tailored algorithms. This paper proposes to unify these subjects and study Generalized Speech Enhancement, where the goal is not to reconstruct the exact reference clean signal, but to focus on improving certain aspects of speech. In particular, this paper concerns intelligibility, quality, and video synchronization. We cast the problem as audio-visual speech resynthesis, which is composed of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and P-TTS are connected by discrete units derived from a self-supervised speech model. Moreover, we utilize self-supervised audio-visual speech model to initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first high-quality model for in-the-wild video-to-speech synthesis and achieves superior performance on all LRS3 audio-visual enhancement tasks with a single model. To demonstrates its applicability in the real world, ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE greatly suppresses noise and improves quality. Project page: https://wnhsu.github.io/ReVISE.},
  pubstate = {preprint},
  keywords = {jepa},
  file = {/Users/lukakuma/Zotero/storage/47DDZAWB/Hsu et al. - 2022 - ReVISE Self-Supervised Speech Resynthesis with Vi.pdf;/Users/lukakuma/Zotero/storage/9MJP35JW/2212.html}
}

@online{huangComposerCreativeControllable2023,
  title = {Composer: {{Creative}} and {{Controllable Image Synthesis}} with {{Composable Conditions}}},
  shorttitle = {Composer},
  author = {Huang, Lianghua and Chen, Di and Liu, Yu and Shen, Yujun and Zhao, Deli and Zhou, Jingren},
  date = {2023-02-21},
  number = {arXiv:2302.09778},
  eprint = {arXiv:2302.09778},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.09778},
  urldate = {2023-02-28},
  abstract = {Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.},
  pubstate = {preprint},
  keywords = {*diffusion,functa,read},
  file = {/Users/lukakuma/Zotero/storage/BYTH4Y9S/Huang et al. - 2023 - Composer Creative and Controllable Image Synthesi.pdf;/Users/lukakuma/Zotero/storage/QYR5D77P/2302.html}
}

@article{huangGPipeEfficientTraining2018,
  title = {{{GPipe}}: {{Efficient Training}} of {{Giant Neural Networks}} Using {{Pipeline Parallelism}}},
  shorttitle = {{{GPipe}}},
  author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
  date = {2018-11-16},
  url = {https://arxiv.org/abs/1811.06965v5},
  urldate = {2022-10-05},
  abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4\% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
  langid = {english},
  keywords = {distributed training,read},
  file = {/Users/lukakuma/Zotero/storage/VFRL3SJQ/Huang et al. - 2018 - GPipe Efficient Training of Giant Neural Networks.pdf;/Users/lukakuma/Zotero/storage/CI66JEUD/1811.html}
}

@online{huangInnerMonologueEmbodied2022,
  title = {Inner {{Monologue}}: {{Embodied Reasoning}} through {{Planning}} with {{Language Models}}},
  shorttitle = {Inner {{Monologue}}},
  author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian},
  date = {2022-07-12},
  number = {arXiv:2207.05608},
  eprint = {arXiv:2207.05608},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.05608},
  urldate = {2022-07-21},
  abstract = {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
  pubstate = {preprint},
  keywords = {action/physical},
  file = {/Users/lukakuma/Zotero/storage/MPL2Y337/Huang et al. - 2022 - Inner Monologue Embodied Reasoning through Planni.pdf;/Users/lukakuma/Zotero/storage/3F99XFZ4/2207.html}
}

@online{huangLanguageModelsZeroShot2022,
  title = {Language {{Models}} as {{Zero-Shot Planners}}: {{Extracting Actionable Knowledge}} for {{Embodied Agents}}},
  shorttitle = {Language {{Models}} as {{Zero-Shot Planners}}},
  author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  date = {2022-03-08},
  number = {arXiv:2201.07207},
  eprint = {arXiv:2201.07207},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.07207},
  urldate = {2023-02-20},
  abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner},
  pubstate = {preprint},
  keywords = {6-planning,action/digital},
  file = {/Users/lukakuma/Zotero/storage/K7ISHJSS/Huang et al. - 2022 - Language Models as Zero-Shot Planners Extracting .pdf;/Users/lukakuma/Zotero/storage/P5ZPHJHF/2201.html}
}

@online{huangLanguageNotAll2023,
  title = {Language {{Is Not All You Need}}: {{Aligning Perception}} with {{Language Models}}},
  shorttitle = {Language {{Is Not All You Need}}},
  author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
  date = {2023-02-27},
  number = {arXiv:2302.14045},
  eprint = {arXiv:2302.14045},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.14045},
  urldate = {2023-02-28},
  abstract = {A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/LM8EUISI/Huang et al. - 2023 - Language Is Not All You Need Aligning Perception .pdf;/Users/lukakuma/Zotero/storage/T7V7UT9Q/2302.html}
}

@article{huangLargeLanguageModels2022,
  title = {Large {{Language Models Can Self-Improve}}},
  author = {Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  date = {2022-10-20},
  url = {https://arxiv.org/abs/2210.11610v1},
  urldate = {2022-10-25},
  abstract = {Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate "high-confidence" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4\%-{$>$}82.1\% on GSM8K, 78.2\%-{$>$}83.0\% on DROP, 90.0\%-{$>$}94.4\% on OpenBookQA, and 63.4\%-{$>$}67.9\% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.},
  langid = {english},
  keywords = {read,self-play},
  file = {/Users/lukakuma/Zotero/storage/6N5FDJMC/Huang et al. - 2022 - Large Language Models Can Self-Improve.pdf;/Users/lukakuma/Zotero/storage/G5YVJ8P7/2210.html}
}

@online{huangOnePolicyControl2020,
  title = {One {{Policy}} to {{Control Them All}}: {{Shared Modular Policies}} for {{Agent-Agnostic Control}}},
  shorttitle = {One {{Policy}} to {{Control Them All}}},
  author = {Huang, Wenlong and Mordatch, Igor and Pathak, Deepak},
  date = {2020-07-09},
  number = {arXiv:2007.04976},
  eprint = {arXiv:2007.04976},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2007.04976},
  urldate = {2022-07-25},
  abstract = {Reinforcement learning is typically concerned with learning control policies tailored to a particular agent. We investigate whether there exists a single global policy that can generalize to control a wide variety of agent morphologies -- ones in which even dimensionality of state and action spaces changes. We propose to express this global policy as a collection of identical modular neural networks, dubbed as Shared Modular Policies (SMP), that correspond to each of the agent's actuators. Every module is only responsible for controlling its corresponding actuator and receives information from only its local sensors. In addition, messages are passed between modules, propagating information between distant modules. We show that a single modular policy can successfully generate locomotion behaviors for several planar agents with different skeletal structures such as monopod hoppers, quadrupeds, bipeds, and generalize to variants not seen during training -- a process that would normally require training and manual hyperparameter tuning for each morphology. We observe that a wide variety of drastically diverse locomotion styles across morphologies as well as centralized coordination emerges via message passing between decentralized modules purely from the reinforcement learning objective. Videos and code at https://huangwl18.github.io/modular-rl/},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/YYXC8V5R/Huang et al. - 2020 - One Policy to Control Them All Shared Modular Pol.pdf;/Users/lukakuma/Zotero/storage/R39RQY3S/2007.html}
}

@online{huangReasoningLargeLanguage2022a,
  title = {Towards {{Reasoning}} in {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Towards {{Reasoning}} in {{Large Language Models}}},
  author = {Huang, Jie and Chang, Kevin Chen-Chuan},
  date = {2022-12-20},
  number = {arXiv:2212.10403},
  eprint = {arXiv:2212.10403},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.10403},
  urldate = {2023-02-20},
  abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
  pubstate = {preprint},
  keywords = {5-reasoning,read},
  file = {/Users/lukakuma/Zotero/storage/HGZYWTFW/Huang and Chang - 2022 - Towards Reasoning in Large Language Models A Surv.pdf;/Users/lukakuma/Zotero/storage/VBDYN6ZM/2212.html}
}

@online{huLoRALowRankAdaptation2021a,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-16},
  number = {arXiv:2106.09685},
  eprint = {arXiv:2106.09685},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2023-02-15},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  pubstate = {preprint},
  keywords = {*peft,1-param composition,read},
  file = {/Users/lukakuma/Zotero/storage/7EBEG26P/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/Users/lukakuma/Zotero/storage/QCR6KT8F/2106.html}
}

@online{humphreysDatadrivenApproachLearning2022,
  title = {A Data-Driven Approach for Learning to Control Computers},
  author = {Humphreys, Peter C. and Raposo, David and Pohlen, Toby and Thornton, Gregory and Chhaparia, Rachita and Muldal, Alistair and Abramson, Josh and Georgiev, Petko and Goldin, Alex and Santoro, Adam and Lillicrap, Timothy},
  date = {2022-02-16},
  number = {arXiv:2202.08137},
  eprint = {arXiv:2202.08137},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.08137},
  urldate = {2023-02-20},
  abstract = {It would be useful for machines to use computers as humans do so that they can aid us in everyday tasks. This is a setting in which there is also the potential to leverage large-scale expert demonstrations and human judgements of interactive behaviour, which are two ingredients that have driven much recent success in AI. Here we investigate the setting of computer control using keyboard and mouse, with goals specified via natural language. Instead of focusing on hand-designed curricula and specialized action spaces, we focus on developing a scalable method centered on reinforcement learning combined with behavioural priors informed by actual human-computer interactions. We achieve state-of-the-art and human-level mean performance across all tasks within the MiniWob++ benchmark, a challenging suite of computer control problems, and find strong evidence of cross-task transfer. These results demonstrate the usefulness of a unified human-agent interface when training machines to use computers. Altogether our results suggest a formula for achieving competency beyond MiniWob++ and towards controlling computers, in general, as a human would.},
  pubstate = {preprint},
  version = {1},
  keywords = {*general tools},
  file = {/Users/lukakuma/Zotero/storage/ZL73MJ8E/Humphreys et al. - 2022 - A data-driven approach for learning to control com.pdf;/Users/lukakuma/Zotero/storage/ZAURYUX2/2202.html}
}

@unpublished{humphreysLargeScaleRetrievalReinforcement2022,
  title = {Large-{{Scale Retrieval}} for {{Reinforcement Learning}}},
  author = {Humphreys, Peter C. and Guez, Arthur and Tieleman, Olivier and Sifre, Laurent and Weber, Th\'eophane and Lillicrap, Timothy},
  date = {2022-06-10},
  number = {arXiv:2206.05314},
  eprint = {2206.05314},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.05314},
  urldate = {2022-06-15},
  abstract = {Effective decision making involves flexibly relating past experiences and relevant contextual information to a novel situation. In deep reinforcement learning, the dominant paradigm is for an agent to amortise information that helps decision-making into its network weights via gradient descent on training losses. Here, we pursue an alternative approach in which agents can utilise large-scale context-sensitive database lookups to support their parametric computations. This allows agents to directly learn in an end-to-end manner to utilise relevant information to inform their outputs. In addition, new information can be attended to by the agent, without retraining, by simply augmenting the retrieval dataset. We study this approach in Go, a challenging game for which the vast combinatorial state space privileges generalisation over direct matching to past experiences. We leverage fast, approximate nearest neighbor techniques in order to retrieve relevant data from a set of tens of millions of expert demonstration states. Attending to this information provides a significant boost to prediction accuracy and game-play performance over simply using these demonstrations as training trajectories, providing a compelling demonstration of the value of large-scale retrieval in reinforcement learning agents.},
  file = {/Users/lukakuma/Zotero/storage/GAUJ3NWH/Humphreys et al. - 2022 - Large-Scale Retrieval for Reinforcement Learning.pdf;/Users/lukakuma/Zotero/storage/3A54X3BI/2206.html}
}

@unpublished{hunterTwoSparsitiesAre2021,
  title = {Two {{Sparsities Are Better Than One}}: {{Unlocking}} the {{Performance Benefits}} of {{Sparse-Sparse Networks}}},
  shorttitle = {Two {{Sparsities Are Better Than One}}},
  author = {Hunter, Kevin Lee and Spracklen, Lawrence and Ahmad, Subutai},
  date = {2021-12-27},
  number = {arXiv:2112.13896},
  eprint = {2112.13896},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.13896},
  urldate = {2022-06-13},
  abstract = {In principle, sparse neural networks should be significantly more efficient than traditional dense networks. Neurons in the brain exhibit two types of sparsity; they are sparsely interconnected and sparsely active. These two types of sparsity, called weight sparsity and activation sparsity, when combined, offer the potential to reduce the computational cost of neural networks by two orders of magnitude. Despite this potential, today's neural networks deliver only modest performance benefits using just weight sparsity, because traditional computing hardware cannot efficiently process sparse networks. In this article we introduce Complementary Sparsity, a novel technique that significantly improves the performance of dual sparse networks on existing hardware. We demonstrate that we can achieve high performance running weight-sparse networks, and we can multiply those speedups by incorporating activation sparsity. Using Complementary Sparsity, we show up to 100X improvement in throughput and energy efficiency performing inference on FPGAs. We analyze scalability and resource tradeoffs for a variety of kernels typical of commercial convolutional networks such as ResNet-50 and MobileNetV2. Our results with Complementary Sparsity suggest that weight plus activation sparsity can be a potent combination for efficiently scaling future AI models.},
  file = {/Users/lukakuma/Zotero/storage/36S57VHS/Hunter et al. - 2021 - Two Sparsities Are Better Than One Unlocking the .pdf;/Users/lukakuma/Zotero/storage/GAGDPEEG/2112.html}
}

@unpublished{huoWenLanBridgingVision2021,
  title = {{{WenLan}}: {{Bridging Vision}} and {{Language}} by {{Large-Scale Multi-Modal Pre-Training}}},
  shorttitle = {{{WenLan}}},
  author = {Huo, Yuqi and Zhang, Manli and Liu, Guangzhen and Lu, Haoyu and Gao, Yizhao and Yang, Guoxing and Wen, Jingyuan and Zhang, Heng and Xu, Baogui and Zheng, Weihao and Xi, Zongzheng and Yang, Yueqian and Hu, Anwen and Zhao, Jinming and Li, Ruichen and Zhao, Yida and Zhang, Liang and Song, Yuqing and Hong, Xin and Cui, Wanqing and Hou, Danyang and Li, Yingyan and Li, Junyi and Liu, Peiyu and Gong, Zheng and Jin, Chuhao and Sun, Yuchong and Chen, Shizhe and Lu, Zhiwu and Dou, Zhicheng and Jin, Qin and Lan, Yanyan and Zhao, Wayne Xin and Song, Ruihua and Wen, Ji-Rong},
  date = {2021-07-08},
  eprint = {2103.06561},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.06561},
  urldate = {2022-04-13},
  abstract = {Multi-modal pre-training models have been intensively explored to bridge vision and language in recent years. However, most of them explicitly model the cross-modal interaction between image-text pairs, by assuming that there exists strong semantic correlation between the text and image modalities. Since this strong assumption is often invalid in real-world scenarios, we choose to implicitly model the cross-modal correlation for large-scale multi-modal pre-training, which is the focus of the Chinese project `WenLan' led by our team. Specifically, with the weak correlation assumption over image-text pairs, we propose a two-tower pre-training model called BriVL within the cross-modal contrastive learning framework. Unlike OpenAI CLIP that adopts a simple contrastive learning method, we devise a more advanced algorithm by adapting the latest method MoCo into the cross-modal scenario. By building a large queue-based dictionary, our BriVL can incorporate more negative samples in limited GPU resources. We further construct a large Chinese multi-source image-text dataset called RUC-CAS-WenLan for pre-training our BriVL model. Extensive experiments demonstrate that the pre-trained BriVL model outperforms both UNITER and OpenAI CLIP on various downstream tasks.},
  file = {/Users/lukakuma/Zotero/storage/NQ4MKBNK/Huo et al. - 2021 - WenLan Bridging Vision and Language by Large-Scal.pdf;/Users/lukakuma/Zotero/storage/96TV94SF/2103.html}
}

@online{hupkesCompositionalityDecomposedHow2020,
  title = {Compositionality Decomposed: How Do Neural Networks Generalise?},
  shorttitle = {Compositionality Decomposed},
  author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  date = {2020-02-23},
  number = {arXiv:1908.08351},
  eprint = {arXiv:1908.08351},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1908.08351},
  urldate = {2022-12-15},
  abstract = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/DYETANMH/Hupkes et al. - 2020 - Compositionality decomposed how do neural network.pdf;/Users/lukakuma/Zotero/storage/5M9YX4SU/1908.html}
}

@online{hutterUniversalAlgorithmicIntelligence2007,
  title = {Universal {{Algorithmic Intelligence}}: {{A}} Mathematical Top-{$>$}down Approach},
  shorttitle = {Universal {{Algorithmic Intelligence}}},
  author = {Hutter, Marcus},
  date = {2007-01-19},
  number = {arXiv:cs/0701125},
  eprint = {arXiv:cs/0701125},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/cs/0701125},
  urldate = {2022-07-05},
  abstract = {Sequential decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameter-free theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline how the AIXI model can formally solve a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXItl that is still effectively more intelligent than any other time t and length l bounded agent. The computation time of AIXItl is of the order t x 2\^l. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/CU4USKQG/Hutter - 2007 - Universal Algorithmic Intelligence A mathematical.pdf;/Users/lukakuma/Zotero/storage/FE2ZRPU3/0701125.html}
}

@unpublished{hwangCOMETATOMIC2020Symbolic2021,
  title = {{{COMET-ATOMIC}} 2020: {{On Symbolic}} and {{Neural Commonsense Knowledge Graphs}}},
  shorttitle = {{{COMET-ATOMIC}} 2020},
  author = {Hwang, Jena D. and Bhagavatula, Chandra and Bras, Ronan Le and Da, Jeff and Sakaguchi, Keisuke and Bosselut, Antoine and Choi, Yejin},
  date = {2021-12-16},
  eprint = {2010.05953},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.05953},
  urldate = {2022-03-08},
  abstract = {Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs (CSKG) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge. In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them. With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that ATOMIC 2020 is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 (175B parameters), while impressive, remains \textasciitilde 12 absolute points lower than a BART-based knowledge model trained on ATOMIC 2020 despite using over 430x fewer parameters.},
  file = {/Users/lukakuma/Zotero/storage/BX5PJ3AH/Hwang et al. - 2021 - COMET-ATOMIC 2020 On Symbolic and Neural Commonse.pdf;/Users/lukakuma/Zotero/storage/SZKT5X3I/2010.html}
}

@online{ibarzGeneralistNeuralAlgorithmic2022a,
  title = {A {{Generalist Neural Algorithmic Learner}}},
  author = {Ibarz, Borja and Kurin, Vitaly and Papamakarios, George and Nikiforou, Kyriacos and Bennani, Mehdi and Csord\'as, R\'obert and Dudzik, Andrew and Bo\v{s}njak, Matko and Vitvitskyi, Alex and Rubanova, Yulia and Deac, Andreea and Bevilacqua, Beatrice and Ganin, Yaroslav and Blundell, Charles and Veli\v{c}kovi\'c, Petar},
  date = {2022-12-03},
  number = {arXiv:2209.11142},
  eprint = {arXiv:2209.11142},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.11142},
  urldate = {2022-12-08},
  abstract = {The cornerstone of neural algorithmic reasoning is the ability to solve algorithmic tasks, especially in a way that generalises out of distribution. While recent years have seen a surge in methodological improvements in this area, they mostly focused on building specialist models. Specialist models are capable of learning to neurally execute either only one algorithm or a collection of algorithms with identical control-flow backbone. Here, instead, we focus on constructing a generalist neural algorithmic learner -- a single graph neural network processor capable of learning to execute a wide range of algorithms, such as sorting, searching, dynamic programming, path-finding and geometry. We leverage the CLRS benchmark to empirically show that, much like recent successes in the domain of perception, generalist algorithmic learners can be built by "incorporating" knowledge. That is, it is possible to effectively learn algorithms in a multi-task manner, so long as we can learn to execute them well in a single-task regime. Motivated by this, we present a series of improvements to the input representation, training regime and processor architecture over CLRS, improving average single-task performance by over 20\% from prior art. We then conduct a thorough ablation of multi-task learners leveraging these improvements. Our results demonstrate a generalist learner that effectively incorporates knowledge captured by specialist models.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/GBG6N8BD/Ibarz et al. - 2022 - A Generalist Neural Algorithmic Learner.pdf;/Users/lukakuma/Zotero/storage/WH447WPQ/Ibarz et al. - 2022 - A Generalist Neural Algorithmic Learner.pdf;/Users/lukakuma/Zotero/storage/PICSCUGR/2209.html}
}

@online{ilharcoPatchingOpenvocabularyModels2022,
  title = {Patching Open-Vocabulary Models by Interpolating Weights},
  author = {Ilharco, Gabriel and Wortsman, Mitchell and Gadre, Samir Yitzhak and Song, Shuran and Hajishirzi, Hannaneh and Kornblith, Simon and Farhadi, Ali and Schmidt, Ludwig},
  date = {2022-10-11},
  number = {arXiv:2208.05592},
  eprint = {arXiv:2208.05592},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.05592},
  urldate = {2022-11-29},
  abstract = {Open-vocabulary models like CLIP achieve high accuracy across many image classification tasks. However, there are still settings where their zero-shot performance is far from optimal. We study model patching, where the goal is to improve accuracy on specific tasks without degrading accuracy on tasks where performance is already adequate. Towards this goal, we introduce PAINT, a patching method that uses interpolations between the weights of a model before fine-tuning and the weights after fine-tuning on a task to be patched. On nine tasks where zero-shot CLIP performs poorly, PAINT increases accuracy by 15 to 60 percentage points while preserving accuracy on ImageNet within one percentage point of the zero-shot model. PAINT also allows a single model to be patched on multiple tasks and improves with model scale. Furthermore, we identify cases of broad transfer, where patching on one task increases accuracy on other tasks even when the tasks have disjoint classes. Finally, we investigate applications beyond common benchmarks such as counting or reducing the impact of typographic attacks on CLIP. Our findings demonstrate that it is possible to expand the set of tasks on which open-vocabulary models achieve high accuracy without re-training them from scratch.},
  pubstate = {preprint},
  keywords = {finetuning},
  file = {/Users/lukakuma/Zotero/storage/AQP39CX9/Ilharco et al. - 2022 - Patching open-vocabulary models by interpolating w.pdf;/Users/lukakuma/Zotero/storage/98QVZMQ5/2208.html}
}

@online{ingrossoDatadrivenEmergenceConvolutional2022,
  title = {Data-Driven Emergence of Convolutional Structure in Neural Networks},
  author = {Ingrosso, Alessandro and Goldt, Sebastian},
  date = {2022-02-01},
  number = {arXiv:2202.00565},
  eprint = {arXiv:2202.00565},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.00565},
  urldate = {2022-06-09},
  abstract = {Exploiting data invariances is crucial for efficient learning in both artificial and biological neural circuits. Understanding how neural networks can discover appropriate representations capable of harnessing the underlying symmetries of their inputs is thus crucial in machine learning and neuroscience. Convolutional neural networks, for example, were designed to exploit translation symmetry and their capabilities triggered the first wave of deep learning successes. However, learning convolutions directly from translation-invariant data with a fully-connected network has so far proven elusive. Here, we show how initially fully-connected neural networks solving a discrimination task can learn a convolutional structure directly from their inputs, resulting in localised, space-tiling receptive fields. These receptive fields match the filters of a convolutional network trained on the same task. By carefully designing data models for the visual scene, we show that the emergence of this pattern is triggered by the non-Gaussian, higher-order local structure of the inputs, which has long been recognised as the hallmark of natural images. We provide an analytical and numerical characterisation of the pattern-formation mechanism responsible for this phenomenon in a simple model, which results in an unexpected link between receptive field formation and the tensor decomposition of higher-order input correlations. These results provide a new perspective on the development of low-level feature detectors in various sensory modalities, and pave the way for studying the impact of higher-order statistics on learning in neural networks.},
  langid = {english},
  pubstate = {preprint},
  keywords = {emergence},
  file = {/Users/lukakuma/Zotero/storage/4BVB62UY/Ingrosso and Goldt - 2022 - Data-driven emergence of convolutional structure i.pdf}
}

@video{innovationendeavorsEmergingResearchApplications2022a,
  title = {Emerging {{Research}} \& {{Applications}} of {{Large Language Models}} (w/ {{Google Brain}}, {{Replit}}, \& {{HuggingFace}})},
  editor = {{Innovation Endeavors}},
  date = {2022-11-12},
  url = {https://www.youtube.com/watch?v=r7UfYlFj2xw},
  urldate = {2022-11-15},
  editortype = {director}
}

@online{IntroducingPathwaysNextgeneration2021,
  title = {Introducing {{Pathways}}: {{A}} next-Generation {{AI}} Architecture},
  shorttitle = {Introducing {{Pathways}}},
  date = {2021-10-28},
  url = {https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/},
  urldate = {2022-03-11},
  abstract = {When I reflect on the past two decades of computer science research, few things inspire me more than the remarkable progress we've seen in the field of artificial intelligence.},
  langid = {american},
  organization = {{Google}},
  file = {/Users/lukakuma/Zotero/storage/MGJ32GPT/introducing-pathways-next-generation-ai-architecture.html}
}

@unpublished{irieModernSelfReferentialWeight2022,
  title = {A {{Modern Self-Referential Weight Matrix That Learns}} to {{Modify Itself}}},
  author = {Irie, Kazuki and Schlag, Imanol and Csord\'as, R\'obert and Schmidhuber, J\"urgen},
  date = {2022-02-11},
  number = {arXiv:2202.05780},
  eprint = {2202.05780},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.05780},
  urldate = {2022-05-25},
  abstract = {The weight matrix (WM) of a neural network (NN) is its program. The programs of many traditional NNs are learned through gradient descent in some error function, then remain fixed. The WM of a self-referential NN, however, can keep rapidly modifying all of itself during runtime. In principle, such NNs can meta-learn to learn, and meta-meta-learn to meta-learn to learn, and so on, in the sense of recursive self-improvement. While NN architectures potentially capable of implementing such behavior have been proposed since the '90s, there have been few if any practical studies. Here we revisit such NNs, building upon recent successes of fast weight programmers and closely related linear Transformers. We propose a scalable self-referential WM (SRWM) that uses outer products and the delta update rule to modify itself. We evaluate our SRWM in supervised few-shot learning and in multi-task reinforcement learning with procedurally generated game environments. Our experiments demonstrate both practical applicability and competitive performance of the proposed SRWM. Our code is public.},
  file = {/Users/lukakuma/Zotero/storage/HACDXC5F/Irie et al. - 2022 - A Modern Self-Referential Weight Matrix That Learn.pdf;/Users/lukakuma/Zotero/storage/R35WRI5L/2202.html}
}

@online{irvingAISafetyDebate2018,
  title = {{{AI}} Safety via Debate},
  author = {Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
  date = {2018-10-22},
  number = {arXiv:1805.00899},
  eprint = {arXiv:1805.00899},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1805.00899},
  urldate = {2022-12-27},
  abstract = {To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4\% to 88.9\% given 6 pixels and from 48.2\% to 85.2\% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/UBDJZ96D/Irving et al. - 2018 - AI safety via debate.pdf;/Users/lukakuma/Zotero/storage/923E4H6M/1805.html}
}

@article{islamGlobalPoolingMore2021,
  title = {Global {{Pooling}}, {{More}} than {{Meets}} the {{Eye}}: {{Position Information}} Is {{Encoded Channel-Wise}} in {{CNNs}}},
  shorttitle = {Global {{Pooling}}, {{More}} than {{Meets}} the {{Eye}}},
  author = {Islam, Md Amirul and Kowal, Matthew and Jia, Sen and Derpanis, Konstantinos G. and Bruce, Neil D. B.},
  date = {2021-08-17},
  url = {https://arxiv.org/abs/2108.07884v1},
  urldate = {2022-08-08},
  abstract = {In this paper, we challenge the common assumption that collapsing the spatial dimensions of a 3D (spatial-channel) tensor in a convolutional neural network (CNN) into a vector via global pooling removes all spatial information. Specifically, we demonstrate that positional information is encoded based on the ordering of the channel dimensions, while semantic information is largely not. Following this demonstration, we show the real world impact of these findings by applying them to two applications. First, we propose a simple yet effective data augmentation strategy and loss function which improves the translation invariance of a CNN's output. Second, we propose a method to efficiently determine which channels in the latent representation are responsible for (i) encoding overall position information or (ii) region-specific positions. We first show that semantic segmentation has a significant reliance on the overall position channels to make predictions. We then show for the first time that it is possible to perform a `region-specific' attack, and degrade a network's performance in a particular part of the input. We believe our findings and demonstrated applications will benefit research areas concerned with understanding the characteristics of CNNs.},
  langid = {english},
  keywords = {(ext) Flamingo},
  file = {/Users/lukakuma/Zotero/storage/M3XYBS42/Islam et al. - 2021 - Global Pooling, More than Meets the Eye Position .pdf;/Users/lukakuma/Zotero/storage/QSFSY5H3/2108.html}
}

@article{iyerAvoidingCatastropheActive2022,
  title = {Avoiding {{Catastrophe}}: {{Active Dendrites Enable Multi-Task Learning}} in {{Dynamic Environments}}},
  shorttitle = {Avoiding {{Catastrophe}}},
  author = {Iyer, Abhiram and Grewal, Karan and Velu, Akash and Souza, Lucas Oliveira and Forest, Jeremy and Ahmad, Subutai},
  date = {2022},
  journaltitle = {Frontiers in Neurorobotics},
  volume = {16},
  issn = {1662-5218},
  url = {https://www.frontiersin.org/article/10.3389/fnbot.2022.846219},
  urldate = {2022-06-13},
  abstract = {A key challenge for AI is to build embodied systems that operate in dynamically changing environments. Such systems must adapt to changing task contexts and learn continuously. Although standard deep learning systems achieve state of the art results on static benchmarks, they often struggle in dynamic scenarios. In these settings, error signals from multiple contexts can interfere with one another, ultimately leading to a phenomenon known as catastrophic forgetting. In this article we investigate biologically inspired architectures as solutions to these problems. Specifically, we show that the biophysical properties of dendrites and local inhibitory systems enable networks to dynamically restrict and route information in a context-specific manner. Our key contributions are as follows: first, we propose a novel artificial neural network architecture that incorporates active dendrites and sparse representations into the standard deep learning framework. Next, we study the performance of this architecture on two separate benchmarks requiring task-based adaptation: Meta-World, a multi-task reinforcement learning environment where a robotic agent must learn to solve a variety of manipulation tasks simultaneously; and a continual learning benchmark in which the model's prediction task changes throughout training. Analysis on both benchmarks demonstrates the emergence of overlapping but distinct and sparse subnetworks, allowing the system to fluidly learn multiple tasks with minimal forgetting. Our neural implementation marks the first time a single architecture has achieved competitive results in both multi-task and continual learning settings. Our research sheds light on how biological properties of neurons can inform deep learning systems to address dynamic scenarios that are typically impossible for traditional ANNs to solve.},
  file = {/Users/lukakuma/Zotero/storage/SACG59NE/Iyer et al. - 2022 - Avoiding Catastrophe Active Dendrites Enable Mult.pdf}
}

@article{iyerOPTIMLScalingLanguage,
  title = {{{OPT-IML}} : {{Scaling Language Model Instruction Meta Learning}} through the {{Lens}} of {{Generalization}}},
  author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, D\'aniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and Li, Xian and O'Horo, Brian and Pereyra, Gabriel and Wang, Jeff and Dewan, Christopher and Celikyilmaz, Asli and Zettlemoyer, Luke and Stoyanov, Ves},
  date = {2022-12-23},
  abstract = {Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction MetaLearning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instructiontuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats \textendash{} PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on the specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/U3QF8287/Iyer et al. - OPT-IML  Scaling Language Model Instruction Meta .pdf}
}

@online{izacardAtlasFewshotLearning2022,
  title = {Atlas: {{Few-shot Learning}} with {{Retrieval Augmented Language Models}}},
  shorttitle = {Atlas},
  author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  date = {2022-11-16},
  number = {arXiv:2208.03299},
  eprint = {arXiv:2208.03299},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.03299},
  urldate = {2022-11-23},
  abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42\% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3\% despite having 50x fewer parameters.},
  pubstate = {preprint},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/PGVHWC2H/Izacard et al. - 2022 - Atlas Few-shot Learning with Retrieval Augmented .pdf;/Users/lukakuma/Zotero/storage/T82VHTZR/2208.html}
}

@online{izacardLeveragingPassageRetrieval2021,
  title = {Leveraging {{Passage Retrieval}} with {{Generative Models}} for {{Open Domain Question Answering}}},
  author = {Izacard, Gautier and Grave, Edouard},
  date = {2021-02-03},
  number = {arXiv:2007.01282},
  eprint = {arXiv:2007.01282},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2007.01282},
  urldate = {2022-11-22},
  abstract = {Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.},
  pubstate = {preprint},
  keywords = {4-document retrieval,QA},
  file = {/Users/lukakuma/Zotero/storage/38BLZGG5/Izacard and Grave - 2021 - Leveraging Passage Retrieval with Generative Model.pdf;/Users/lukakuma/Zotero/storage/X9AI56MA/2007.html}
}

@online{jacobModelingStrongHumanLike2022,
  title = {Modeling {{Strong}} and {{Human-Like Gameplay}} with {{KL-Regularized Search}}},
  author = {Jacob, Athul Paul and Wu, David J. and Farina, Gabriele and Lerer, Adam and Hu, Hengyuan and Bakhtin, Anton and Andreas, Jacob and Brown, Noam},
  date = {2022-02-16},
  number = {arXiv:2112.07544},
  eprint = {arXiv:2112.07544},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.07544},
  urldate = {2022-11-24},
  abstract = {We consider the task of building strong but human-like policies in multi-agent decision-making problems, given examples of human behavior. Imitation learning is effective at predicting human actions but may not match the strength of expert humans, while self-play learning and search techniques (e.g. AlphaZero) lead to strong performance but may produce policies that are difficult for humans to understand and coordinate with. We show in chess and Go that regularizing search based on the KL divergence from an imitation-learned policy results in higher human prediction accuracy and stronger performance than imitation learning alone. We then introduce a novel regret minimization algorithm that is regularized based on the KL divergence from an imitation-learned policy, and show that using this algorithm for search in no-press Diplomacy yields a policy that matches the human prediction accuracy of imitation learning while being substantially stronger.},
  pubstate = {preprint},
  keywords = {(ext) CICERO},
  file = {/Users/lukakuma/Zotero/storage/57HSGKLJ/Jacob et al. - 2022 - Modeling Strong and Human-Like Gameplay with KL-Re.pdf;/Users/lukakuma/Zotero/storage/VGPHD8M2/2112.html}
}

@article{jacobsDirectRecordingsGridlike2013,
  title = {Direct Recordings of Grid-like Neuronal Activity in Human Spatial Navigation},
  author = {Jacobs, Joshua and Weidemann, Christoph T. and Miller, Jonathan F. and Solway, Alec and Burke, John F. and Wei, Xue-Xin and Suthana, Nanthia and Sperling, Michael R. and Sharan, Ashwini D. and Fried, Itzhak and Kahana, Michael J.},
  date = {2013-09},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {16},
  number = {9},
  pages = {1188--1190},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  url = {https://www.nature.com/articles/nn.3466},
  urldate = {2022-06-18},
  abstract = {Grid cell activity in the rodent and non-human primate entorhinal cortex is thought to provide spatial location information to the hippocampus for navigation and spatial processing. Here, Jacobs et al. examined single neuron spiking activities from human subjects performing a virtual spatial navigation task and show the presence of grid-like firing activity.},
  issue = {9},
  langid = {english},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/VX4XBF5Q/Jacobs et al. - 2013 - Direct recordings of grid-like neuronal activity i.pdf;/Users/lukakuma/Zotero/storage/A6HECZD4/nn.html}
}

@unpublished{jaeglePerceiverGeneralPerception2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  date = {2021-06-22},
  eprint = {2103.03206},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2103.03206},
  urldate = {2022-04-29},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  keywords = {(ext) Flamingo,perceiver,read},
  file = {/Users/lukakuma/Zotero/storage/K9QI2MVF/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf;/Users/lukakuma/Zotero/storage/2S48B2SS/2103.html}
}

@unpublished{jaeglePerceiverIOGeneral2022,
  title = {Perceiver {{IO}}: {{A General Architecture}} for {{Structured Inputs}} \& {{Outputs}}},
  shorttitle = {Perceiver {{IO}}},
  author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and H\'enaff, Olivier and Botvinick, Matthew M. and Zisserman, Andrew and Vinyals, Oriol and Carreira, Jo\=ao},
  date = {2022-03-15},
  number = {arXiv:2107.14795},
  eprint = {2107.14795},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2107.14795},
  urldate = {2022-05-13},
  abstract = {A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain \& task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.},
  keywords = {(ext) Gato,perceiver,read},
  file = {/Users/lukakuma/Zotero/storage/LPPQPSVN/Jaegle et al. - 2022 - Perceiver IO A General Architecture for Structure.pdf;/Users/lukakuma/Zotero/storage/3RB6URPV/2107.html}
}

@unpublished{jainBiologicalSequenceDesign2022,
  title = {Biological {{Sequence Design}} with {{GFlowNets}}},
  author = {Jain, Moksh and Bengio, Emmanuel and Garcia, Alex-Hernandez and Rector-Brooks, Jarrid and Dossou, Bonaventure F. P. and Ekbote, Chanakya and Fu, Jie and Zhang, Tianyu and Kilgour, Micheal and Zhang, Dinghuai and Simine, Lena and Das, Payel and Bengio, Yoshua},
  date = {2022-03-02},
  eprint = {2203.04115},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2203.04115},
  urldate = {2022-05-09},
  abstract = {Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed GFlowNets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in GFlowNets. We present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/VLF2JRJU/Jain et al. - 2022 - Biological Sequence Design with GFlowNets.pdf;/Users/lukakuma/Zotero/storage/BVCF2L5X/2203.html}
}

@inproceedings{jangBCZZeroShotTask2021,
  title = {{{BC-Z}}: {{Zero-Shot Task Generalization}} with {{Robotic Imitation Learning}}},
  shorttitle = {{{BC-Z}}},
  author = {Jang, Eric and Irpan, Alex and Khansari, Mohi and Kappler, Daniel and Ebert, Frederik and Lynch, Corey and Levine, Sergey and Finn, Chelsea},
  date = {2021-11-04},
  url = {https://openreview.net/forum?id=8kbp23tSGYv},
  urldate = {2022-07-21},
  abstract = {In this paper, we study the problem of enabling a vision-based robotic manipulation system to generalize to novel tasks, a long-standing challenge in robot learning. We approach the challenge from...},
  eventtitle = {5th {{Annual Conference}} on {{Robot Learning}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/FLH4V9UU/Jang et al. - 2021 - BC-Z Zero-Shot Task Generalization with Robotic I.pdf}
}

@online{jangExploringBenefitsTraining2023,
  title = {Exploring the {{Benefits}} of {{Training Expert Language Models}} over {{Instruction Tuning}}},
  author = {Jang, Joel and Kim, Seungone and Ye, Seonghyeon and Kim, Doyoung and Logeswaran, Lajanugen and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
  date = {2023-02-08},
  number = {arXiv:2302.03202},
  eprint = {arXiv:2302.03202},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.03202},
  urldate = {2023-02-23},
  abstract = {Recently, Language Models (LMs) instruction-tuned on multiple tasks, also known as multitask-prompted fine-tuning (MT), have shown the capability to generalize to unseen tasks. Previous work has shown that scaling the number of training tasks is the key component in making stronger MT LMs. In this work, we report an unexpected finding that an expert LM fine-tuned on just a single task can outperform an MT LM trained with 300+ different tasks on 11 different unseen datasets and on 13 datasets of the BIG-bench benchmark by a mean accuracy of 3.20\% and 1.29\%, respectively. This finding casts doubt on the previously held belief that simply scaling the number of tasks makes stronger MT LMs. Leveraging this finding, we further show that this distributed approach of training a separate expert LM per training task instead of a single MT LM for zero-shot inference possesses many benefits including (1) avoiding negative task transfer that often occurs during instruction tuning, (2) being able to continually learn new tasks without having to re-train on previous tasks to avoid catastrophic forgetting, and (3) showing compositional capabilities when merging individual experts together. The code is available at https://github.com/joeljang/ELM.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/XJJSSXIR/Jang et al. - 2023 - Exploring the Benefits of Training Expert Language.pdf;/Users/lukakuma/Zotero/storage/VLVCGLW9/2302.html}
}

@online{jannerOfflineReinforcementLearning2021,
  title = {Offline {{Reinforcement Learning}} as {{One Big Sequence Modeling Problem}}},
  author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
  date = {2021-11-28},
  number = {arXiv:2106.02039},
  eprint = {arXiv:2106.02039},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.02039},
  urldate = {2022-07-23},
  abstract = {Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/852XWS7N/Janner et al. - 2021 - Offline Reinforcement Learning as One Big Sequence.pdf;/Users/lukakuma/Zotero/storage/GUADT4TA/2106.html}
}

@inproceedings{jannyFilteredCoPhyUnsupervisedLearning2021,
  title = {Filtered-{{CoPhy}}: {{Unsupervised Learning}} of {{Counterfactual Physics}} in {{Pixel Space}}},
  shorttitle = {Filtered-{{CoPhy}}},
  author = {Janny, Steeven and Baradel, Fabien and Neverova, Natalia and Nadri, Madiha and Mori, Greg and Wolf, Christian},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=1L0C5ROtFp},
  urldate = {2022-05-15},
  abstract = {Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/RAWP75R3/Janny et al. - 2021 - Filtered-CoPhy Unsupervised Learning of Counterfa.pdf;/Users/lukakuma/Zotero/storage/RKNT9KVU/forum.html}
}

@online{jawaharAutoMoENeuralArchitecture2022,
  title = {{{AutoMoE}}: {{Neural Architecture Search}} for {{Efficient Sparsely Activated Transformers}}},
  shorttitle = {{{AutoMoE}}},
  author = {Jawahar, Ganesh and Mukherjee, Subhabrata and Liu, Xiaodong and Kim, Young Jin and Abdul-Mageed, Muhammad and Lakshmanan, Laks V. S. and Awadallah, Ahmed Hassan and Bubeck, Sebastien and Gao, Jianfeng},
  date = {2022-10-14},
  number = {arXiv:2210.07535},
  eprint = {arXiv:2210.07535},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.07535},
  urldate = {2022-10-18},
  abstract = {Neural architecture search (NAS) has demonstrated promising results on identifying efficient Transformer architectures which outperform manually designed ones for natural language tasks like neural machine translation (NMT). Existing NAS methods operate on a space of dense architectures, where all of the sub-architecture weights are activated for every input. Motivated by the recent advances in sparsely activated models like the Mixture-of-Experts (MoE) model, we introduce sparse architectures with conditional computation into the NAS search space. Given this expressive search space which subsumes prior densely activated architectures, we develop a new framework AutoMoE to search for efficient sparsely activated sub-Transformers. AutoMoE-generated sparse models obtain (i) 3x FLOPs reduction over manually designed dense Transformers and (ii) 23\% FLOPs reduction over state-of-the-art NAS-generated dense sub-Transformers with parity in BLEU score on benchmark datasets for NMT. AutoMoE consists of three training phases: (a) Heterogeneous search space design with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?); (b) SuperNet training that jointly trains several subnetworks sampled from the large search space by weight-sharing; (c) Evolutionary search for the architecture with the optimal trade-off between task performance and computational constraint like FLOPs and latency. AutoMoE code, data and trained models are available at https://github.com/microsoft/AutoMoE.},
  pubstate = {preprint},
  keywords = {sparsity},
  file = {/Users/lukakuma/Zotero/storage/Q5NZ39P8/Jawahar et al. - 2022 - AutoMoE Neural Architecture Search for Efficient .pdf;/Users/lukakuma/Zotero/storage/EUTXRCDP/2210.html}
}

@unpublished{jegelkaTheoryGraphNeural2022,
  title = {Theory of {{Graph Neural Networks}}: {{Representation}} and {{Learning}}},
  shorttitle = {Theory of {{Graph Neural Networks}}},
  author = {Jegelka, Stefanie},
  date = {2022-04-15},
  eprint = {2204.07697},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2204.07697},
  urldate = {2022-06-03},
  abstract = {Graph Neural Networks (GNNs), neural network architectures targeted to learning representations of graphs, have become a popular learning model for prediction tasks on nodes, graphs and configurations of points, with wide success in practice. This article summarizes a selection of the emerging theoretical results on approximation and learning properties of widely used message passing GNNs and higher-order GNNs, focusing on representation, generalization and extrapolation. Along the way, it summarizes mathematical connections.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/8GBDN2WI/Jegelka - 2022 - Theory of Graph Neural Networks Representation and Learning.pdf}
}

@online{jiangGeneralIntelligenceRequires2022,
  title = {General {{Intelligence Requires Rethinking Exploration}}},
  author = {Jiang, Minqi and Rockt\"aschel, Tim and Grefenstette, Edward},
  date = {2022-11-14},
  number = {arXiv:2211.07819},
  eprint = {arXiv:2211.07819},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.07819},
  urldate = {2022-11-17},
  abstract = {We are at the cusp of a transition from "learning from data" to "learning what data to learn from" as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train our models to how to effectively acquire and use task-relevant data. This problem, which we frame as exploration, is a universal aspect of learning in open-ended domains, such as the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of generalized exploration to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration serves as a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence.},
  pubstate = {preprint},
  keywords = {action/physical,read},
  file = {/Users/lukakuma/Zotero/storage/8DASKXLR/Jiang et al. - 2022 - General Intelligence Requires Rethinking Explorati.pdf;/Users/lukakuma/Zotero/storage/ZAJATX3L/2211.html}
}

@online{jiangHowCanWe2021,
  title = {How {{Can We Know When Language Models Know}}? {{On}} the {{Calibration}} of {{Language Models}} for {{Question Answering}}},
  shorttitle = {How {{Can We Know When Language Models Know}}?},
  author = {Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
  date = {2021-05-20},
  number = {arXiv:2012.00955},
  eprint = {arXiv:2012.00955},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.00955},
  urldate = {2022-11-02},
  abstract = {Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question ``how can we know when language models know, with confidence, the answer to a particular query?'' We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models \textendash{} T5, BART, and GPT-2 \textendash and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github. com/jzbjyb/lm-calibration.},
  langid = {english},
  pubstate = {preprint},
  keywords = {QA},
  file = {/Users/lukakuma/Zotero/storage/THDVETSH/Jiang et al. - 2021 - How Can We Know When Language Models Know On the .pdf}
}

@online{jiangPrioritizedLevelReplay2021,
  title = {Prioritized {{Level Replay}}},
  author = {Jiang, Minqi and Grefenstette, Edward and Rockt\"aschel, Tim},
  date = {2021-06-12},
  number = {arXiv:2010.03934},
  eprint = {arXiv:2010.03934},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.03934},
  urldate = {2022-11-18},
  abstract = {Environments with procedurally generated content serve as important benchmarks for testing systematic generalization in deep reinforcement learning. In this setting, each level is an algorithmically created environment instance with a unique configuration of its factors of variation. Training on a prespecified subset of levels allows for testing generalization to unseen levels. What can be learned from a level depends on the current policy, yet prior work defaults to uniform sampling of training levels independently of the policy. We introduce Prioritized Level Replay (PLR), a general framework for selectively sampling the next training level by prioritizing those with higher estimated learning potential when revisited in the future. We show TD-errors effectively estimate a level's future learning potential and, when used to guide the sampling procedure, induce an emergent curriculum of increasingly difficult levels. By adapting the sampling of training levels, PLR significantly improves sample efficiency and generalization on Procgen Benchmark--matching the previous state-of-the-art in test return--and readily combines with other methods. Combined with the previous leading method, PLR raises the state-of-the-art to over 76\% improvement in test return relative to standard RL baselines.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/L8UINSHE/Jiang et al. - 2021 - Prioritized Level Replay.pdf;/Users/lukakuma/Zotero/storage/X7NZ7AFZ/2010.html}
}

@online{jiangReplayGuidedAdversarialEnvironment2022,
  title = {Replay-{{Guided Adversarial Environment Design}}},
  author = {Jiang, Minqi and Dennis, Michael and Parker-Holder, Jack and Foerster, Jakob and Grefenstette, Edward and Rockt\"aschel, Tim},
  date = {2022-01-13},
  number = {arXiv:2110.02439},
  eprint = {arXiv:2110.02439},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.02439},
  urldate = {2022-11-18},
  abstract = {Deep reinforcement learning (RL) agents may successfully generalize to new settings if trained on an appropriately diverse set of environment and task configurations. Unsupervised Environment Design (UED) is a promising self-supervised RL paradigm, wherein the free parameters of an underspecified environment are automatically adapted during training to the agent's capabilities, leading to the emergence of diverse training environments. Here, we cast Prioritized Level Replay (PLR), an empirically successful but theoretically unmotivated method that selectively samples randomly-generated training levels, as UED. We argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). Crucially, DCD includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows us to develop novel theory for PLR, providing a version with a robustness guarantee at Nash equilibria. Furthermore, our theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibria. Indeed, our experiments confirm that our new method, PLR\$\^\{\textbackslash perp\}\$, obtains better results on a suite of out-of-distribution, zero-shot transfer tasks, in addition to demonstrating that PLR\$\^\{\textbackslash perp\}\$ improves the performance of PAIRED, from which it inherited its theoretical framework.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/5TGLEE3H/Jiang et al. - 2022 - Replay-Guided Adversarial Environment Design.pdf;/Users/lukakuma/Zotero/storage/EDC2HJLW/2110.html}
}

@online{jiangSoftware2022,
  title = {{{Software}}{$^{2}$}},
  author = {Jiang, Minqi},
  date = {2022-11-15},
  url = {https://blog.minch.co/2022/11/15/software-squared.html},
  urldate = {2022-11-17},
  abstract = {We are currently at the cusp of transitioning from ``learning from data'' to ``learning what data to learn from'' as the central focus of AI research. State-of-the-art deep learning models, like GPT-[X] and Stable Diffusion, have been described as data sponges,1 capable of modeling immense amounts of data.2 These large generative models, many based on the transformer architecture, can model massive datasets, learning to produce images, video, audio, code, and data in many other domains at a quality that begins to rival that of samples authored by human experts. Growing evidence suggests the generality of such large models is largely limited by the quality of the training data. Yet, mainstream training practices are not inherently data-seeking. Instead, they ignore the specific quality of information within the training data in favor of maximizing data quantity. This discrepancy hints at a likely, major shift in research focus in the coming years, toward innovating directly on data collection and generation as a principal way to improve model performance. To our knowledge, the term ``data sponge'' was first coined in Eric Jang's excellent article, ``Just Ask for Generalization.'' https://evjang.com/2021/10/23/generalization.html.~{$\hookleftarrow$} The recent Stable Diffusion model effectively compresses approximately 100GB of training data into a mere 2GB of model weights.~{$\hookleftarrow$}},
  langid = {english},
  organization = {{Minqi Jiang}},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/Q4L5RFPJ/software-squared.html}
}

@unpublished{jiaScalingVisualVisionLanguage2021,
  title = {Scaling {{Up Visual}} and {{Vision-Language Representation Learning With Noisy Text Supervision}}},
  author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
  date = {2021-06-11},
  eprint = {2102.05918},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.05918},
  urldate = {2022-04-28},
  abstract = {Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.},
  file = {/Users/lukakuma/Zotero/storage/YA2AJ2W6/Jia et al. - 2021 - Scaling Up Visual and Vision-Language Representati.pdf;/Users/lukakuma/Zotero/storage/7TR8QE94/2102.html}
}

@online{jinPubMedQADatasetBiomedical2019,
  title = {{{PubMedQA}}: {{A Dataset}} for {{Biomedical Research Question Answering}}},
  shorttitle = {{{PubMedQA}}},
  author = {Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William W. and Lu, Xinghua},
  date = {2019-09-13},
  number = {arXiv:1909.06146},
  eprint = {arXiv:1909.06146},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.06146},
  urldate = {2022-11-23},
  abstract = {We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1\% accuracy, compared to single human performance of 78.0\% accuracy and majority-baseline of 55.2\% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.},
  pubstate = {preprint},
  keywords = {QA},
  file = {/Users/lukakuma/Zotero/storage/N7NRTX8D/Jin et al. - 2019 - PubMedQA A Dataset for Biomedical Research Questi.pdf;/Users/lukakuma/Zotero/storage/W43G2BMJ/1909.html}
}

@article{johansonEmergentBarteringBehaviour2022,
  title = {Emergent {{Bartering Behaviour}} in {{Multi-Agent Reinforcement Learning}}},
  author = {Johanson, Michael Bradley and Hughes, Edward and Timbers, Finbarr and Leibo, Joel Z.},
  date = {2022-05-13},
  url = {https://arxiv.org/abs/2205.06760v1},
  urldate = {2022-06-10},
  abstract = {Advances in artificial intelligence often stem from the development of new environments that abstract real-world situations into a form where research can be done conveniently. This paper contributes such an environment based on ideas inspired by elementary Microeconomics. Agents learn to produce resources in a spatially complex world, trade them with one another, and consume those that they prefer. We show that the emergent production, consumption, and pricing behaviors respond to environmental conditions in the directions predicted by supply and demand shifts in Microeconomics. We also demonstrate settings where the agents' emergent prices for goods vary over space, reflecting the local abundance of goods. After the price disparities emerge, some agents then discover a niche of transporting goods between regions with different prevailing prices -- a profitable strategy because they can buy goods where they are cheap and sell them where they are expensive. Finally, in a series of ablation experiments, we investigate how choices in the environmental rewards, bartering actions, agent architecture, and ability to consume tradable goods can either aid or inhibit the emergence of this economic behavior. This work is part of the environment development branch of a research program that aims to build human-like artificial general intelligence through multi-agent interactions in simulated societies. By exploring which environment features are needed for the basic phenomena of elementary microeconomics to emerge automatically from learning, we arrive at an environment that differs from those studied in prior multi-agent reinforcement learning work along several dimensions. For example, the model incorporates heterogeneous tastes and physical abilities, and agents negotiate with one another as a grounded form of communication.},
  langid = {english},
  keywords = {DeepMind,emergence,multiagent},
  file = {/Users/lukakuma/Zotero/storage/MVW35CBP/Johanson et al. - 2022 - Emergent Bartering Behaviour in Multi-Agent Reinfo.pdf;/Users/lukakuma/Zotero/storage/KVID65AH/2205.html}
}

@article{jonesImpactAlphaFold2One2022,
  title = {The Impact of {{AlphaFold2}} One Year On},
  author = {Jones, David T. and Thornton, Janet M.},
  date = {2022-01},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {19},
  number = {1},
  pages = {15--20},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  url = {https://www.nature.com/articles/s41592-021-01365-3},
  urldate = {2022-05-26},
  abstract = {The greatly improved prediction of protein 3D structure from sequence achieved by the second version of AlphaFold in 2020 has already had a huge impact on biological research, but challenges remain; the protein folding problem cannot be considered solved. We expect fierce competition to improve the method even further and new applications of machine learning to help illuminate proteomes and their many interactions.},
  issue = {1},
  langid = {english},
  keywords = {DeepMind,read},
  file = {/Users/lukakuma/Zotero/storage/KMWV4PH5/Jones and Thornton - 2022 - The impact of AlphaFold2 one year on.pdf;/Users/lukakuma/Zotero/storage/4IBUIXVB/s41592-021-01365-3.html}
}

@online{joshiTriviaQALargeScale2017,
  title = {{{TriviaQA}}: {{A Large Scale Distantly Supervised Challenge Dataset}} for {{Reading Comprehension}}},
  shorttitle = {{{TriviaQA}}},
  author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
  date = {2017-05-13},
  number = {arXiv:1705.03551},
  eprint = {arXiv:1705.03551},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.03551},
  urldate = {2023-01-06},
  abstract = {We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23\% and 40\% vs. 80\%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/RJE7XGFT/Joshi et al. - 2017 - TriviaQA A Large Scale Distantly Supervised Chall.pdf;/Users/lukakuma/Zotero/storage/GMNKZTMY/1705.html}
}

@online{juLearningDataMixed2022,
  title = {Learning from Data in the Mixed Adversarial Non-Adversarial Case: {{Finding}} the Helpers and Ignoring the Trolls},
  shorttitle = {Learning from Data in the Mixed Adversarial Non-Adversarial Case},
  author = {Ju, Da and Xu, Jing and Boureau, Y.-Lan and Weston, Jason},
  date = {2022-08-05},
  number = {arXiv:2208.03295},
  eprint = {arXiv:2208.03295},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.03295},
  urldate = {2022-11-28},
  abstract = {The promise of interaction between intelligent conversational agents and humans is that models can learn from such feedback in order to improve. Unfortunately, such exchanges in the wild will not always involve human utterances that are benign or of high quality, and will include a mixture of engaged (helpers) and unengaged or even malicious users (trolls). In this work we study how to perform robust learning in such an environment. We introduce a benchmark evaluation, SafetyMix, which can evaluate methods that learn safe vs. toxic language in a variety of adversarial settings to test their robustness. We propose and analyze several mitigating learning algorithms that identify trolls either at the example or at the user level. Our main finding is that user-based methods, that take into account that troll users will exhibit adversarial behavior across multiple examples, work best in a variety of settings on our benchmark. We then test these methods in a further real-life setting of conversations collected during deployment, with similar results.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/EN25HHGN/Ju et al. - 2022 - Learning from data in the mixed adversarial non-ad.pdf;/Users/lukakuma/Zotero/storage/35JZIRY3/2208.html}
}

@article{jumperHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and \v{Z}\'idek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  date = {2021-08},
  journaltitle = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/s41586-021-03819-2},
  urldate = {2022-05-12},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1\textendash 4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence\textemdash the structure prediction component of the `protein folding problem'8\textemdash has been an important open research problem for more than 50~years9. Despite recent progress10\textendash 14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  issue = {7873},
  langid = {english},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/T8EGR52S/Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf;/Users/lukakuma/Zotero/storage/ZHG4QUQH/s41586-021-03819-2.html}
}

@online{kadavathLanguageModelsMostly2022a,
  title = {Language {{Models}} ({{Mostly}}) {{Know What They Know}}},
  author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Dodds, Zac Hatfield and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  date = {2022-07-16},
  number = {arXiv:2207.05221},
  eprint = {arXiv:2207.05221},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.05221},
  urldate = {2022-11-02},
  abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
  langid = {english},
  pubstate = {preprint},
  keywords = {knowledge,OpenAI},
  file = {/Users/lukakuma/Zotero/storage/RIXU5BW6/Kadavath et al. - 2022 - Language Models (Mostly) Know What They Know.pdf;/Users/lukakuma/Zotero/storage/FI9638RM/2207.html}
}

@online{kajicLearningCooperateEmergent2020,
  title = {Learning to Cooperate: {{Emergent}} Communication in Multi-Agent Navigation},
  shorttitle = {Learning to Cooperate},
  author = {Kaji\'c, Ivana and Ayg\"un, Eser and Precup, Doina},
  date = {2020-06-30},
  number = {arXiv:2004.01097},
  eprint = {arXiv:2004.01097},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.01097},
  urldate = {2023-02-20},
  abstract = {Emergent communication in artificial agents has been studied to understand language evolution, as well as to develop artificial systems that learn to communicate with humans. We show that agents performing a cooperative navigation task in various gridworld environments learn an interpretable communication protocol that enables them to efficiently, and in many cases, optimally, solve the task. An analysis of the agents' policies reveals that emergent signals spatially cluster the state space, with signals referring to specific locations and spatial directions such as "left", "up", or "upper left room". Using populations of agents, we show that the emergent protocol has basic compositional structure, thus exhibiting a core property of natural language.},
  pubstate = {preprint},
  keywords = {multiagent},
  file = {/Users/lukakuma/Zotero/storage/KS4PRYAE/Kajiƒá et al. - 2020 - Learning to cooperate Emergent communication in m.pdf;/Users/lukakuma/Zotero/storage/GWS2GSGJ/2004.html}
}

@online{kandpalDeduplicatingTrainingData2022,
  title = {Deduplicating {{Training Data Mitigates Privacy Risks}} in {{Language Models}}},
  author = {Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  date = {2022-02-16},
  number = {arXiv:2202.06539},
  eprint = {arXiv:2202.06539},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.06539},
  urldate = {2022-11-02},
  abstract = {Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set. In this work, we show that the success of these attacks is largely due to duplication in commonly used web-scraped training sets. We first show that the rate at which language models regenerate training sequences is superlinearly related to a sequence's count in the training set. For instance, a sequence that is present 10 times in the training data is on average generated {$\sim$}1000\texttimes{} more often than a sequence that is present only once. We next show that existing methods for detecting memorized sequences have nearchance accuracy on non-duplicated training sequences. Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks. Taken together, our results motivate an increased focus on deduplication in privacysensitive applications and a reevaluation of the practicality of existing privacy attacks.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/WQJUT74E/Kandpal et al. - 2022 - Deduplicating Training Data Mitigates Privacy Risk.pdf}
}

@online{kandpalLargeLanguageModels2022,
  title = {Large {{Language Models Struggle}} to {{Learn Long-Tail Knowledge}}},
  author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
  date = {2022-11-15},
  number = {arXiv:2211.08411},
  eprint = {arXiv:2211.08411},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.08411},
  urldate = {2022-11-17},
  abstract = {The internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, there is a huge variability in the number of times a given piece of information appears on the web. In this paper, we study the relationship between the knowledge memorized by large language models and the information in their pre-training datasets. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, we find that while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant document count, presenting a promising approach for capturing the long-tail.},
  pubstate = {preprint},
  keywords = {knowledge,read},
  file = {/Users/lukakuma/Zotero/storage/GA39PNXQ/Kandpal et al. - 2022 - Large Language Models Struggle to Learn Long-Tail .pdf;/Users/lukakuma/Zotero/storage/UKS3QYFK/2211.html}
}

@unpublished{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  date = {2020-01-22},
  number = {arXiv:2001.08361},
  eprint = {arXiv:2001.08361},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2001.08361},
  urldate = {2022-05-13},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  pubstate = {preprint},
  keywords = {(ext) Gato,OpenAI,read,scaling law},
  file = {/Users/lukakuma/Zotero/storage/2A3DDR36/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf;/Users/lukakuma/Zotero/storage/JDUHEHJG/2001.html}
}

@online{kapturowskiHumanlevelAtari200x2022,
  title = {Human-Level {{Atari}} 200x Faster},
  author = {Kapturowski, Steven and Campos, V\'ictor and Jiang, Ray and Raki\'cevi\'c, Nemanja and family=Hasselt, given=Hado, prefix=van, useprefix=true and Blundell, Charles and Badia, Adri\`a Puigdom\`enech},
  date = {2022-09-15},
  number = {arXiv:2209.07550},
  eprint = {arXiv:2209.07550},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.07550},
  urldate = {2022-09-19},
  abstract = {The task of building general agents that perform well over a wide range of tasks has been an importantgoal in reinforcement learning since its inception. The problem has been subject of research of alarge body of work, with performance frequently measured by observing scores over the wide rangeof environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass thehuman benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80billion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set ofstrategies to achieve a 200-fold reduction of experience needed to outperform the human baseline. Weinvestigate a range of instabilities and bottlenecks we encountered while reducing the data regime, andpropose effective solutions to build a more robust and efficient agent. We also demonstrate competitiveperformance with high-performing methods such as Muesli and MuZero. The four key components toour approach are (1) an approximate trust region method which enables stable bootstrapping from theonline network, (2) a normalisation scheme for the loss and priorities which improves robustness whenlearning a set of value functions with a wide range of scales, (3) an improved architecture employingtechniques from NFNets in order to leverage deeper networks without the need for normalization layers,and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy overtime.},
  pubstate = {preprint},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/NXDDE365/Kapturowski et al. - 2022 - Human-level Atari 200x faster.pdf;/Users/lukakuma/Zotero/storage/H6EFR864/2209.html}
}

@online{karagiannakosVisionLanguageModels2022,
  title = {Vision {{Language}} Models: Towards Multi-Modal Deep Learning},
  shorttitle = {Vision {{Language}} Models},
  author = {Karagiannakos, Sergios},
  date = {2022-03-03},
  url = {https://theaisummer.com/vision-language-models/},
  urldate = {2022-04-28},
  abstract = {A review of state of the art vision-language models such as CLIP, DALLE, ALIGN and SimVL},
  langid = {english},
  organization = {{AI Summer}},
  file = {/Users/lukakuma/Zotero/storage/YIZM97V9/vision-language-models.html}
}

@online{karamchetiLanguageDrivenRepresentationLearning2023,
  title = {Language-{{Driven Representation Learning}} for {{Robotics}}},
  author = {Karamcheti, Siddharth and Nair, Suraj and Chen, Annie S. and Kollar, Thomas and Finn, Chelsea and Sadigh, Dorsa and Liang, Percy},
  date = {2023-02-24},
  number = {arXiv:2302.12766},
  eprint = {arXiv:2302.12766},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.12766},
  urldate = {2023-02-28},
  abstract = {Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite. We then introduce Voltron, a framework for language-driven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems \$\textbackslash unicode\{x2013\}\$ a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find that Voltron's language-driven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.},
  pubstate = {preprint},
  keywords = {7-multimodal,action/physical},
  file = {/Users/lukakuma/Zotero/storage/JD52PPMN/Karamcheti et al. - 2023 - Language-Driven Representation Learning for Roboti.pdf;/Users/lukakuma/Zotero/storage/NPTFGBQG/2302.html}
}

@article{karmiloff-smithAlternativeDomaingeneralDomainspecific2015,
  title = {An Alternative to Domain-General or Domain-Specific Frameworks for Theorizing about Human Evolution and Ontogenesis},
  author = {Karmiloff-Smith, Annette},
  date = {2015-06-19},
  journaltitle = {AIMS neuroscience},
  shortjournal = {AIMS Neurosci},
  volume = {2},
  number = {2},
  eprint = {26682283},
  eprinttype = {pmid},
  pages = {91--104},
  issn = {2373-7972},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4678597/},
  urldate = {2022-07-26},
  abstract = {This paper maintains that neither a domain-general nor a domain-specific framework is appropriate for furthering our understanding of human evolution and ontogenesis. Rather, as we learn increasingly more about the dynamics of gene-environment interaction and gene expression, theorists should consider a third alternative: a domain-relevant approach, which argues that the infant brain comes equipped with biases that are relevant to, but not initially specific to, processing different kinds of input. The hypothesis developed here is that domain-specific core knowledge/specialized functions do not constitute the start state; rather, functional specialization emerges progressively through neuronal competition over developmental time. Thus, the existence of category-specific deficits in brain-damaged adults cannot be used to bolster claims that category-specific or domain-specific modules underpin early development, because neural specificity in the adult brain is likely to have been the emergent property over time of a developing, self-structuring system in interaction with the environment.},
  pmcid = {PMC4678597},
  file = {/Users/lukakuma/Zotero/storage/EFVP35A7/Karmiloff-Smith - 2015 - An alternative to domain-general or domain-specifi.pdf}
}

@online{karpathyYesYouShould2016,
  title = {Yes You Should Understand Backprop},
  author = {Karpathy, Andrej},
  date = {2016-12-19T19:50:08},
  url = {https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b},
  urldate = {2021-05-07},
  abstract = {When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit\ldots},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/lukakuma/Zotero/storage/UGEMNYZR/yes-you-should-understand-backprop-e2f06eab496b.html}
}

@inproceedings{karpukhinDensePassageRetrieval2020,
  title = {Dense {{Passage Retrieval}} for {{Open-Domain Question Answering}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  date = {2020-11},
  pages = {6769--6781},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  url = {https://aclanthology.org/2020.emnlp-main.550},
  urldate = {2022-12-05},
  abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
  eventtitle = {{{EMNLP}} 2020},
  keywords = {4-document retrieval,QA},
  file = {/Users/lukakuma/Zotero/storage/ZW2UYN6H/Karpukhin et al. - 2020 - Dense Passage Retrieval for Open-Domain Question A.pdf}
}

@online{kasirzadehReasonsValuesStakeholders2021,
  title = {Reasons, {{Values}}, {{Stakeholders}}: {{A Philosophical Framework}} for {{Explainable Artificial Intelligence}}},
  shorttitle = {Reasons, {{Values}}, {{Stakeholders}}},
  author = {Kasirzadeh, Atoosa},
  date = {2021-02-28},
  number = {arXiv:2103.00752},
  eprint = {arXiv:2103.00752},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.00752},
  urldate = {2022-12-23},
  abstract = {The societal and ethical implications of the use of opaque artificial intelligence systems for consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholder groups, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by (1) identifying the types of explanations that are most pertinent to artificial intelligence predictions, (2) recognizing the relevance and importance of social and ethical values for the evaluation of these explanations, and (3) demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.},
  pubstate = {preprint},
  keywords = {self-explain},
  file = {/Users/lukakuma/Zotero/storage/B83EWG9C/Kasirzadeh - 2021 - Reasons, Values, Stakeholders A Philosophical Fra.pdf;/Users/lukakuma/Zotero/storage/C2344ESK/2103.html}
}

@online{kazemiLAMBADABackwardChaining2022,
  title = {{{LAMBADA}}: {{Backward Chaining}} for {{Automated Reasoning}} in {{Natural Language}}},
  shorttitle = {{{LAMBADA}}},
  author = {Kazemi, Seyed Mehran and Kim, Najoung and Bhatia, Deepti and Xu, Xin and Ramachandran, Deepak},
  date = {2022-12-20},
  number = {arXiv:2212.13894},
  eprint = {arXiv:2212.13894},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.13894},
  urldate = {2022-12-30},
  abstract = {Remarkable progress has been made on automated reasoning with knowledge specified as unstructured, natural text, by using the power of large language models (LMs) coupled with methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from the intended conclusion to the set of axioms that support it) is significantly more efficient at proof-finding problems. We import this intuition into the LM setting and develop a Backward Chaining algorithm, which we call LAMBADA, that decomposes reasoning into four sub-modules, each of which can be simply implemented by few-shot prompted LM inference. We show that LAMBADA achieves massive accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/CXQUYA68/Kazemi et al. - 2022 - LAMBADA Backward Chaining for Automated Reasoning.pdf;/Users/lukakuma/Zotero/storage/NL98SQIE/2212.html}
}

@unpublished{kaziDifferentiableGraphModule2020,
  title = {Differentiable {{Graph Module}} ({{DGM}}) for {{Graph Convolutional Networks}}},
  author = {Kazi, Anees and Cosmo, Luca and Navab, Nassir and Bronstein, Michael},
  date = {2020-06-17},
  eprint = {2002.04999},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.04999},
  urldate = {2022-03-24},
  abstract = {Graph deep learning has recently emerged as a powerful ML concept allowing to generalize successful deep neural architectures to non-Euclidean structured data. Such methods have shown promising results on a broad spectrum of applications ranging from social science, biomedicine, and particle physics to computer vision, graphics, and chemistry. One of the limitations of the majority of the current graph neural network architectures is that they are often restricted to the transductive setting and rely on the assumption that the underlying graph is known and fixed. In many settings, such as those arising in medical and healthcare applications, this assumption is not necessarily true since the graph may be noisy, partially- or even completely unknown, and one is thus interested in inferring it from the data. This is especially important in inductive settings when dealing with nodes not present in the graph at training time. Furthermore, sometimes such a graph itself may convey insights that are even more important than the downstream task. In this paper, we introduce Differentiable Graph Module (DGM), a learnable function predicting the edge probability in the graph relevant for the task, that can be combined with convolutional graph neural network layers and trained in an end-to-end fashion. We provide an extensive evaluation of applications from the domains of healthcare (disease prediction), brain imaging (gender and age prediction), computer graphics (3D point cloud segmentation), and computer vision (zero-shot learning). We show that our model provides a significant improvement over baselines both in transductive and inductive settings and achieves state-of-the-art results.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/YVT3DLKW/Kazi et al. - 2020 - Differentiable Graph Module (DGM) for Graph Convol.pdf;/Users/lukakuma/Zotero/storage/J4MVMX6P/2002.html}
}

@article{kendallLearningDriveDay2019,
  title = {Learning to {{Drive}} in a {{Day}}},
  author = {Kendall, Alex and Hawke, Jeffrey and Janz, David and Mazur, Przemyslaw and Reda, Daniele and Allen, John M. and Lam, Vinh-Dieu and Bewley, A. and Shah, Amar},
  date = {2019},
  journaltitle = {2019 International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2019.8793742},
  abstract = {This work demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision and provides a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We demonstrate the first application of deep reinforcement learning to autonomous driving. From randomly initialised parameters, our model is able to learn a policy for lane following in a handful of training episodes using a single monocular image as input. We provide a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We use a continuous, model-free deep reinforcement learning algorithm, with all exploration and optimisation performed on-vehicle. This demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision. We discuss the challenges and opportunities to scale this approach to a broader range of autonomous driving tasks.}
}

@inproceedings{khandelwalGeneralizationMemorizationNearest2020,
  title = {Generalization through {{Memorization}}: {{Nearest Neighbor Language Models}}},
  shorttitle = {Generalization through {{Memorization}}},
  author = {Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  date = {2020-03-11},
  url = {https://openreview.net/forum?id=HklBjCEKvH},
  urldate = {2022-10-03},
  abstract = {We extend a pre-trained neural language model by linearly interpolating it with a k-nearest neighbors model, achieving new state-of-the-art results on Wikitext-103 with no additional training.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {knowledge},
  file = {/Users/lukakuma/Zotero/storage/27YTYP9M/Khandelwal et al. - 2020 - Generalization through Memorization Nearest Neigh.pdf;/Users/lukakuma/Zotero/storage/3RDZTD75/forum.html}
}

@article{khanTransformersVisionSurvey2022,
  title = {Transformers in {{Vision}}: {{A Survey}}},
  shorttitle = {Transformers in {{Vision}}},
  author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  date = {2022-01-06},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  eprint = {2101.01169},
  eprinttype = {arxiv},
  pages = {3505244},
  issn = {0360-0300, 1557-7341},
  url = {http://arxiv.org/abs/2101.01169},
  urldate = {2022-04-20},
  abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.},
  file = {/Users/lukakuma/Zotero/storage/A9WBLP92/Khan et al. - 2022 - Transformers in Vision A Survey.pdf;/Users/lukakuma/Zotero/storage/C4ZZ6XKY/2101.html}
}

@online{khattabDemonstrateSearchPredictComposingRetrieval2023,
  title = {Demonstrate-{{Search-Predict}}: {{Composing}} Retrieval and Language Models for Knowledge-Intensive {{NLP}}},
  shorttitle = {Demonstrate-{{Search-Predict}}},
  author = {Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  date = {2023-01-23},
  number = {arXiv:2212.14024},
  eprint = {arXiv:2212.14024},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.14024},
  urldate = {2023-02-02},
  abstract = {Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple "retrieve-then-read" pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120\%, 8-39\%, and 80-290\% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp},
  pubstate = {preprint},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/4HM6Q7EF/Khattab et al. - 2023 - Demonstrate-Search-Predict Composing retrieval an.pdf;/Users/lukakuma/Zotero/storage/8JZ4TD2A/2212.html}
}

@online{khotDecomposedPromptingModular2022a,
  title = {Decomposed {{Prompting}}: {{A Modular Approach}} for {{Solving Complex Tasks}}},
  shorttitle = {Decomposed {{Prompting}}},
  author = {Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
  date = {2022-10-05},
  number = {arXiv:2210.02406},
  eprint = {arXiv:2210.02406},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.02406},
  urldate = {2023-02-20},
  abstract = {Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/TGC5B4QD/Khot et al. - 2022 - Decomposed Prompting A Modular Approach for Solvi.pdf;/Users/lukakuma/Zotero/storage/L5SBRH9W/2210.html}
}

@online{kimScalableEfficientMoE2021,
  title = {Scalable and {{Efficient MoE Training}} for {{Multitask Multilingual Models}}},
  author = {Kim, Young Jin and Awan, Ammar Ahmad and Muzio, Alexandre and Salinas, Andres Felipe Cruz and Lu, Liyang and Hendy, Amr and Rajbhandari, Samyam and He, Yuxiong and Awadalla, Hany Hassan},
  date = {2021-09-21},
  number = {arXiv:2109.10465},
  eprint = {arXiv:2109.10465},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.10465},
  urldate = {2022-07-29},
  abstract = {The Mixture of Experts (MoE) models are an emerging class of sparsely activated deep learning models that have sublinear compute costs with respect to their parameters. In contrast with dense models, the sparse architecture of MoE offers opportunities for drastically growing model size with significant accuracy gain while consuming much lower compute budget. However, supporting large scale MoE training also has its own set of system and modeling challenges. To overcome the challenges and embrace the opportunities of MoE, we first develop a system capable of scaling MoE models efficiently to trillions of parameters. It combines multi-dimensional parallelism and heterogeneous memory technologies harmoniously with MoE to empower 8x larger models on the same hardware compared with existing work. Besides boosting system efficiency, we also present new training methods to improve MoE sample efficiency and leverage expert pruning strategy to improve inference time efficiency. By combining the efficient system and training methods, we are able to significantly scale up large multitask multilingual models for language generation which results in a great improvement in model accuracy. A model trained with 10 billion parameters on 50 languages can achieve state-of-the-art performance in Machine Translation (MT) and multilingual natural language generation tasks. The system support of efficient MoE training has been implemented and open-sourced with the DeepSpeed library.},
  pubstate = {preprint},
  keywords = {sparsity},
  file = {/Users/lukakuma/Zotero/storage/TUHF9F6G/Kim et al. - 2021 - Scalable and Efficient MoE Training for Multitask .pdf;/Users/lukakuma/Zotero/storage/93CEGELG/2109.html}
}

@online{kimSODAMillionscaleDialogue2022,
  title = {{{SODA}}: {{Million-scale Dialogue Distillation}} with {{Social Commonsense Contextualization}}},
  shorttitle = {{{SODA}}},
  author = {Kim, Hyunwoo and Hessel, Jack and Jiang, Liwei and Lu, Ximing and Yu, Youngjae and Zhou, Pei and Bras, Ronan Le and Alikhani, Malihe and Kim, Gunhee and Sap, Maarten and Choi, Yejin},
  date = {2022-12-20},
  number = {arXiv:2212.10465},
  eprint = {arXiv:2212.10465},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.10465},
  urldate = {2022-12-23},
  abstract = {We present SODA: the first publicly available, million-scale high-quality social dialogue dataset. Using SODA, we train COSMO: a generalizable conversation agent outperforming previous best-performing agents on both in- and out-of-domain datasets. In contrast to most existing crowdsourced, small-scale dialogue corpora, we distill 1.5M socially-grounded dialogues from a pre-trained language model (InstructGPT; Ouyang et al., 2022). Dialogues are distilled by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x; West et al., 2022). Human evaluation shows that dialogues in SODA are more consistent, specific, and (surprisingly) natural than prior human-authored datasets - e.g., DailyDialog (Li et al., 2017), BlendedSkillTalk (Smith et al., 2020). In addition, extensive evaluations show that COSMO is significantly more natural and consistent on unseen datasets than best-performing dialogue models - e.g., GODEL (Peng et al., 2022), BlenderBot (Roller et al., 2021), DialoGPT (Zhang et al., 2020). Furthermore, it is sometimes even preferred to the original human-written gold responses. We make our data, models, and code public.},
  pubstate = {preprint},
  keywords = {8-chat},
  file = {/Users/lukakuma/Zotero/storage/QRWC5EE3/Kim et al. - 2022 - SODA Million-scale Dialogue Distillation with Soc.pdf;/Users/lukakuma/Zotero/storage/SZEQTTQ4/2212.html}
}

@article{kipfSemiSupervisedClassificationGraph2016,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2016-09-09},
  url = {https://arxiv.org/abs/1609.02907v4},
  urldate = {2022-03-30},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  langid = {english},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/KE8H5YXW/Kipf and Welling - 2016 - Semi-Supervised Classification with Graph Convolut.pdf}
}

@unpublished{kipfVariationalGraphAutoEncoders2016,
  title = {Variational {{Graph Auto-Encoders}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2016-11-21},
  eprint = {1611.07308},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.07308},
  urldate = {2022-04-08},
  abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/NLXR9D2J/Kipf and Welling - 2016 - Variational Graph Auto-Encoders.pdf;/Users/lukakuma/Zotero/storage/GZEW4XM3/1611.html}
}

@article{kirbyZeroingOutZeroCOVID2022,
  title = {Zeroing out on Zero-{{COVID}}},
  author = {Kirby, William C.},
  date = {2022-06-03},
  journaltitle = {Science},
  volume = {376},
  number = {6597},
  pages = {1026--1026},
  publisher = {{American Association for the Advancement of Science}},
  url = {https://www.science.org/doi/10.1126/science.add1891},
  urldate = {2022-06-10},
  file = {/Users/lukakuma/Zotero/storage/D93NWYPP/Kirby - 2022 - Zeroing out on zero-COVID.pdf}
}

@article{kirkpatrickPushingFrontiersDensity2021,
  title = {Pushing the Frontiers of Density Functionals by Solving the Fractional Electron Problem},
  author = {Kirkpatrick, James and McMorrow, Brendan and Turban, David H. P. and Gaunt, Alexander L. and Spencer, James S. and Matthews, Alexander G. D. G. and Obika, Annette and Thiry, Louis and Fortunato, Meire and Pfau, David and Castellanos, Lara Rom\'an and Petersen, Stig and Nelson, Alexander W. R. and Kohli, Pushmeet and Mori-S\'anchez, Paula and Hassabis, Demis and Cohen, Aron J.},
  date = {2021-12-10},
  journaltitle = {Science},
  volume = {374},
  number = {6573},
  pages = {1385--1389},
  publisher = {{American Association for the Advancement of Science}},
  url = {https://www.science.org/doi/10.1126/science.abj6511},
  urldate = {2022-07-08},
  file = {/Users/lukakuma/Zotero/storage/CW9A24BV/Kirkpatrick et al. - 2021 - Pushing the frontiers of density functionals by so.pdf}
}

@online{kirkSurveyGeneralisationDeep2022,
  title = {A {{Survey}} of {{Generalisation}} in {{Deep Reinforcement Learning}}},
  author = {Kirk, Robert and Zhang, Amy and Grefenstette, Edward and Rockt\"aschel, Tim},
  date = {2022-01-30},
  number = {arXiv:2111.09794},
  eprint = {arXiv:2111.09794},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.09794},
  urldate = {2022-07-21},
  abstract = {The study of generalisation in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We provide a unifying formalism and terminology for discussing different generalisation problems, building upon previous works. We go on to categorise existing benchmarks for generalisation, as well as current methods for tackling the generalisation problem. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in generalisation, we suggest fast online adaptation and tackling RL-specific problems as some areas for future work on methods for generalisation, and we recommend building benchmarks in underexplored problem settings such as offline RL generalisation and reward-function variation.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/PAMIJ6SL/Kirk et al. - 2022 - A Survey of Generalisation in Deep Reinforcement L.pdf}
}

@unpublished{kirschImprovingGeneralizationMeta2020,
  title = {Improving {{Generalization}} in {{Meta Reinforcement Learning}} Using {{Learned Objectives}}},
  author = {Kirsch, Louis and family=Steenkiste, given=Sjoerd, prefix=van, useprefix=true and Schmidhuber, J\"urgen},
  date = {2020-02-14},
  number = {arXiv:1910.04098},
  eprint = {1910.04098},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1910.04098},
  urldate = {2022-06-08},
  abstract = {Biological evolution has distilled the experiences of many learners into the general learning algorithms of humans. Our novel meta reinforcement learning algorithm MetaGenRL is inspired by this process. MetaGenRL distills the experiences of many complex agents to meta-learn a low-complexity neural objective function that decides how future individuals will learn. Unlike recent meta-RL algorithms, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training. In some cases, it even outperforms human-engineered RL algorithms. MetaGenRL uses off-policy second-order gradients during meta-training that greatly increase its sample efficiency.},
  file = {/Users/lukakuma/Zotero/storage/L7NT29UB/Kirsch et al. - 2020 - Improving Generalization in Meta Reinforcement Lea.pdf;/Users/lukakuma/Zotero/storage/NPM94EAU/1910.html}
}

@inproceedings{kitaevReformerEfficientTransformer2020,
  title = {Reformer: {{The Efficient Transformer}}},
  shorttitle = {Reformer},
  author = {Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  date = {2020-03-11},
  url = {https://openreview.net/forum?id=rkgNKkHtvB},
  urldate = {2022-08-03},
  abstract = {Efficient Transformer with locality-sensitive hashing and reversible layers},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/J4EREQQU/Kitaev et al. - 2020 - Reformer The Efficient Transformer.pdf;/Users/lukakuma/Zotero/storage/G4KFVEM4/forum.html}
}

@article{klukasEfficientFlexibleRepresentation2020,
  title = {Efficient and Flexible Representation of Higher-Dimensional Cognitive Variables with Grid Cells},
  author = {Klukas, Mirko and Lewis, Marcus and Fiete, Ila},
  date = {2020-04-28},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {16},
  number = {4},
  pages = {e1007796},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007796},
  urldate = {2022-06-13},
  abstract = {We shed light on the potential of entorhinal grid cells to efficiently encode variables of dimension greater than two, while remaining faithful to empirical data on their low-dimensional structure. Our model constructs representations of high-dimensional inputs through a combination of low-dimensional random projections and ``classical'' low-dimensional hexagonal grid cell responses. Without reconfiguration of the recurrent circuit, the same system can flexibly encode multiple variables of different dimensions while maximizing the coding range (per dimension) by automatically trading-off dimension with an exponentially large coding range. It achieves high efficiency and flexibility by combining two powerful concepts, modularity and mixed selectivity, in what we call ``mixed modular coding''. In contrast to previously proposed schemes, the model does not require the formation of higher-dimensional grid responses, a cell-inefficient and rigid mechanism. The firing fields observed in flying bats or climbing rats can be generated by neurons that combine activity from multiple grid modules, each representing higher-dimensional spaces according to our model. The idea expands our understanding of grid cells, suggesting that they could implement a general circuit that generates on-demand coding and memory states for variables in high-dimensional vector spaces.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/JN4UHNWS/Klukas et al. - 2020 - Efficient and flexible representation of higher-di.pdf;/Users/lukakuma/Zotero/storage/9DTB72IU/article.html}
}

@online{kocetkovStackTBPermissively2022,
  title = {The {{Stack}}: 3 {{TB}} of Permissively Licensed Source Code},
  shorttitle = {The {{Stack}}},
  author = {Kocetkov, Denis and Li, Raymond and Allal, Loubna Ben and Li, Jia and Mou, Chenghao and Ferrandis, Carlos Mu\~noz and Jernite, Yacine and Mitchell, Margaret and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and family=Werra, given=Leandro, prefix=von, useprefix=true and family=Vries, given=Harm, prefix=de, useprefix=true},
  date = {2022-11-20},
  number = {arXiv:2211.15533},
  eprint = {arXiv:2211.15533},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.15533},
  urldate = {2022-11-29},
  abstract = {Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called "Am I in The Stack" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.},
  pubstate = {preprint},
  keywords = {pretraining},
  file = {/Users/lukakuma/Zotero/storage/ULBRRLNW/Kocetkov et al. - 2022 - The Stack 3 TB of permissively licensed source co.pdf;/Users/lukakuma/Zotero/storage/7QX2H265/2211.html}
}

@online{kohGroundingLanguageModels2023,
  title = {Grounding {{Language Models}} to {{Images}} for {{Multimodal Generation}}},
  author = {Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel},
  date = {2023-01-31},
  number = {arXiv:2301.13823},
  eprint = {arXiv:2301.13823},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.13823},
  urldate = {2023-02-25},
  abstract = {We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/UZPM5PGE/Koh et al. - 2023 - Grounding Language Models to Images for Multimodal.pdf;/Users/lukakuma/Zotero/storage/NRMSQQ3C/2301.html}
}

@online{kojimaLargeLanguageModels2022a,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  date = {2022-10-02},
  number = {arXiv:2205.11916},
  eprint = {arXiv:2205.11916},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.11916},
  urldate = {2022-11-02},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-bystep answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding ``Let's think step by step'' before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted fewshot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with 175B parameter InstructGPT model, as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  langid = {english},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/FBX384QR/Kojima et al. - 2022 - Large Language Models are Zero-Shot Reasoners.pdf;/Users/lukakuma/Zotero/storage/PFMZCH75/Kojima et al. - 2022 - Large Language Models are Zero-Shot Reasoners.pdf;/Users/lukakuma/Zotero/storage/MUP7VTEI/2205.html}
}

@online{komeiliInternetAugmentedDialogueGeneration2021,
  title = {Internet-{{Augmented Dialogue Generation}}},
  author = {Komeili, Mojtaba and Shuster, Kurt and Weston, Jason},
  date = {2021-07-15},
  number = {arXiv:2107.07566},
  eprint = {arXiv:2107.07566},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.07566},
  urldate = {2023-02-20},
  abstract = {The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020).},
  pubstate = {preprint},
  keywords = {*general tools},
  file = {/Users/lukakuma/Zotero/storage/8L68EDLA/Komeili et al. - 2021 - Internet-Augmented Dialogue Generation.pdf;/Users/lukakuma/Zotero/storage/RI9ERSZE/2107.html}
}

@unpublished{konecnyFederatedLearningStrategies2017,
  title = {Federated {{Learning}}: {{Strategies}} for {{Improving Communication Efficiency}}},
  shorttitle = {Federated {{Learning}}},
  author = {Kone\v{c}n\'y, Jakub and McMahan, H. Brendan and Yu, Felix X. and Richt\'arik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  date = {2017-10-30},
  eprint = {1610.05492},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1610.05492},
  urldate = {2022-03-11},
  abstract = {Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.},
  file = {/Users/lukakuma/Zotero/storage/IXSS4FYH/Koneƒçn√Ω et al. - 2017 - Federated Learning Strategies for Improving Commu.pdf;/Users/lukakuma/Zotero/storage/E4ZV8NM3/1610.html}
}

@online{kopparapuHiddenAgendaSocial2022,
  title = {Hidden {{Agenda}}: A {{Social Deduction Game}} with {{Diverse Learned Equilibria}}},
  shorttitle = {Hidden {{Agenda}}},
  author = {Kopparapu, Kavya and Du\'e\~nez-Guzm\'an, Edgar A. and Matyas, Jayd and Vezhnevets, Alexander Sasha and Agapiou, John P. and McKee, Kevin R. and Everett, Richard and Marecki, Janusz and Leibo, Joel Z. and Graepel, Thore},
  date = {2022-01-05},
  number = {arXiv:2201.01816},
  eprint = {arXiv:2201.01816},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.01816},
  urldate = {2023-02-20},
  abstract = {A key challenge in the study of multiagent cooperation is the need for individual agents not only to cooperate effectively, but to decide with whom to cooperate. This is particularly critical in situations when other agents have hidden, possibly misaligned motivations and goals. Social deduction games offer an avenue to study how individuals might learn to synthesize potentially unreliable information about others, and elucidate their true motivations. In this work, we present Hidden Agenda, a two-team social deduction game that provides a 2D environment for studying learning agents in scenarios of unknown team alignment. The environment admits a rich set of strategies for both teams. Reinforcement learning agents trained in Hidden Agenda show that agents can learn a variety of behaviors, including partnering and voting without need for communication in natural language.},
  pubstate = {preprint},
  keywords = {multiagent},
  file = {/Users/lukakuma/Zotero/storage/TQJ822H4/Kopparapu et al. - 2022 - Hidden Agenda a Social Deduction Game with Divers.pdf;/Users/lukakuma/Zotero/storage/23ZZYRF9/2201.html}
}

@online{korbakPretrainingLanguageModels2023,
  title = {Pretraining {{Language Models}} with {{Human Preferences}}},
  author = {Korbak, Tomasz and Shi, Kejian and Chen, Angelica and Bhalerao, Rasika and Buckley, Christopher L. and Phang, Jason and Bowman, Samuel R. and Perez, Ethan},
  date = {2023-02-16},
  number = {arXiv:2302.08582},
  eprint = {arXiv:2302.08582},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.08582},
  urldate = {2023-02-25},
  abstract = {Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training.},
  pubstate = {preprint},
  keywords = {pretraining},
  file = {/Users/lukakuma/Zotero/storage/5FAVXFCF/Korbak et al. - 2023 - Pretraining Language Models with Human Preferences.pdf;/Users/lukakuma/Zotero/storage/X6XYNYS3/2302.html}
}

@online{korinekLanguageModelsCognitive2023,
  type = {Working Paper},
  title = {Language {{Models}} and {{Cognitive Automation}} for {{Economic Research}}},
  author = {Korinek, Anton},
  date = {2023-02},
  series = {Working {{Paper Series}}},
  number = {30957},
  eprint = {30957},
  eprinttype = {National Bureau of Economic Research},
  url = {https://www.nber.org/papers/w30957},
  urldate = {2023-02-14},
  abstract = {Large language models (LLMs) such as ChatGPT have the potential to revolutionize research in economics and other disciplines. I describe 25 use cases along six domains in which LLMs are starting to become useful as both research assistants and tutors: ideation, writing, background research, data analysis, coding, and mathematical derivations. I provide general instructions and demonstrate specific examples for how to take advantage of each of these, classifying the LLM capabilities from experimental to highly useful. I hypothesize that ongoing advances will improve the performance of LLMs across all of these domains, and that economic researchers who take advantage of LLMs to automate micro tasks will become significantly more productive. Finally, I speculate on the longer-term implications of cognitive automation via LLMs for economic research.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/XB8B8JYL/Korinek - 2023 - Language Models and Cognitive Automation for Econo.pdf}
}

@article{kosoyExploringExplorationComparing2020,
  title = {Exploring {{Exploration}}: {{Comparing Children}} with {{RL Agents}} in {{Unified Environments}}},
  shorttitle = {Exploring {{Exploration}}},
  author = {Kosoy, Eliza and Collins, Jasmine and Chan, David M. and Huang, Sandy and Pathak, Deepak and Agrawal, Pulkit and Canny, John and Gopnik, Alison and Hamrick, Jessica B.},
  date = {2020-05-06},
  url = {https://arxiv.org/abs/2005.02880v2},
  urldate = {2022-07-26},
  abstract = {Research in developmental psychology consistently shows that children explore the world thoroughly and efficiently and that this exploration allows them to learn. In turn, this early learning supports more robust generalization and intelligent behavior later in life. While much work has gone into developing methods for exploration in machine learning, artificial agents have not yet reached the high standard set by their human counterparts. In this work we propose using DeepMind Lab (Beattie et al., 2016) as a platform to directly compare child and agent behaviors and to develop new exploration techniques. We outline two ongoing experiments to demonstrate the effectiveness of a direct comparison, and outline a number of open research questions that we believe can be tested using this methodology.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/PEWLGJTE/Kosoy et al. - 2020 - Exploring Exploration Comparing Children with RL .pdf;/Users/lukakuma/Zotero/storage/EAKFCI9M/2005.html}
}

@article{kosterHumancentredMechanismDesign2022,
  title = {Human-Centred Mechanism Design with {{Democratic AI}}},
  author = {Koster, Raphael and Jan, Balaguer and Tacchetti, Andrea and Weinstein, Ari and Zhu, Tina and Hauser, Oliver and Williams, Duncan and Campbell-Gillingham, Lucy and Thacker, Phoebe and Botvinick, Matthew and Summerfield, Christopher},
  date = {2022-07-04},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  pages = {1--10},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  url = {https://www.nature.com/articles/s41562-022-01383-x},
  urldate = {2022-07-08},
  abstract = {Building artificial intelligence (AI) that aligns with human values is an unsolved problem. Here we developed a human-in-the-loop research pipeline called Democratic AI, in which reinforcement learning is used to design a social mechanism that humans prefer by majority. A large group of humans played an online investment game that involved deciding whether to keep a monetary endowment or to share it with others for collective benefit. Shared revenue was returned to players under two different redistribution mechanisms, one designed by the AI and the other by humans. The AI discovered a mechanism that redressed initial wealth imbalance, sanctioned free riders and successfully won the majority vote. By optimizing for human preferences, Democratic AI offers a proof of concept for value-aligned policy innovation.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/262LVSEV/Koster et al. - 2022 - Human-centred mechanism design with Democratic AI.pdf;/Users/lukakuma/Zotero/storage/56RK5FKK/s41562-022-01383-x.html}
}

@online{kotonyaPolicyComplianceDetection2022,
  title = {Policy {{Compliance Detection}} via {{Expression Tree Inference}}},
  author = {Kotonya, Neema and Vlachos, Andreas and Yazdani, Majid and Mathias, Lambert and Saeidi, Marzieh},
  date = {2022-05-24},
  number = {arXiv:2205.12259},
  eprint = {arXiv:2205.12259},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.12259},
  urldate = {2022-12-28},
  abstract = {Policy Compliance Detection (PCD) is a task we encounter when reasoning over texts, e.g. legal frameworks. Previous work to address PCD relies heavily on modeling the task as a special case of Recognizing Textual Entailment. Entailment is applicable to the problem of PCD, however viewing the policy as a single proposition, as opposed to multiple interlinked propositions, yields poor performance and lacks explainability. To address this challenge, more recent proposals for PCD have argued for decomposing policies into expression trees consisting of questions connected with logic operators. Question answering is used to obtain answers to these questions with respect to a scenario. Finally, the expression tree is evaluated in order to arrive at an overall solution. However, this work assumes expression trees are provided by experts, thus limiting its applicability to new policies. In this work, we learn how to infer expression trees automatically from policy texts. We ensure the validity of the inferred trees by introducing constrained decoding using a finite state automaton to ensure the generation of valid trees. We determine through automatic evaluation that 63\% of the expression trees generated by our constrained generation model are logically equivalent to gold trees. Human evaluation shows that 88\% of trees generated by our model are correct.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/Z3EINYT3/Kotonya et al. - 2022 - Policy Compliance Detection via Expression Tree In.pdf;/Users/lukakuma/Zotero/storage/N6KANWNP/2205.html}
}

@article{kramarNegotiationHonestyArtificial2022,
  title = {Negotiation and Honesty in Artificial Intelligence Methods for the Board Game of {{Diplomacy}}},
  author = {Kram\'ar, J\'anos and Eccles, Tom and Gemp, Ian and Tacchetti, Andrea and McKee, Kevin R. and Malinowski, Mateusz and Graepel, Thore and Bachrach, Yoram},
  date = {2022-12-06},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {7214},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  url = {https://www.nature.com/articles/s41467-022-34473-5},
  urldate = {2022-12-08},
  abstract = {The success of human civilization is rooted in our ability to cooperate by communicating and making joint plans. We study how artificial agents may use communication to better cooperate in Diplomacy, a long-standing AI challenge. We propose negotiation algorithms allowing agents to agree on contracts regarding joint plans, and show they outperform agents lacking this ability. For humans, misleading others about our intentions forms a barrier to cooperation. Diplomacy requires reasoning about our opponents' future plans, enabling us to study broken commitments between agents and the conditions for honest cooperation. We find that artificial agents face a similar problem as humans: communities of communicating agents are susceptible to peers who deviate from agreements. To defend against this, we show that the inclination to sanction peers who break contracts dramatically reduces the advantage of such deviators. Hence, sanctioning helps foster mostly truthful communication, despite conditions that initially favor deviations from agreements.},
  issue = {1},
  langid = {english},
  keywords = {8-chat,read},
  file = {/Users/lukakuma/Zotero/storage/Q2D9TJFS/Kram√°r et al. - 2022 - Negotiation and honesty in artificial intelligence.pdf}
}

@online{krauseGeDiGenerativeDiscriminator2020,
  title = {{{GeDi}}: {{Generative Discriminator Guided Sequence Generation}}},
  shorttitle = {{{GeDi}}},
  author = {Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
  date = {2020-10-22},
  number = {arXiv:2009.06367},
  eprint = {arXiv:2009.06367},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.06367},
  urldate = {2022-11-02},
  abstract = {While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/UHC7TQSU/Krause et al. - 2020 - GeDi Generative Discriminator Guided Sequence Gen.pdf}
}

@unpublished{krishnaVisualGenomeConnecting2016,
  title = {Visual {{Genome}}: {{Connecting Language}} and {{Vision Using Crowdsourced Dense Image Annotations}}},
  shorttitle = {Visual {{Genome}}},
  author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Li, Fei-Fei},
  date = {2016-02-23},
  eprint = {1602.07332},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1602.07332},
  urldate = {2022-04-21},
  abstract = {Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that "the person is riding a horse-drawn carriage". In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.},
  file = {/Users/lukakuma/Zotero/storage/22HQ5NUK/Krishna et al. - 2016 - Visual Genome Connecting Language and Vision Usin.pdf;/Users/lukakuma/Zotero/storage/TCI35MAS/1602.html}
}

@inproceedings{kuduguntaDistillationTasklevelMixtureofExperts2021,
  title = {Beyond {{Distillation}}: {{Task-level Mixture-of-Experts}} for {{Efficient Inference}}},
  shorttitle = {Beyond {{Distillation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Kudugunta, Sneha and Huang, Yanping and Bapna, Ankur and Krikun, Maxim and Lepikhin, Dmitry and Luong, Minh-Thang and Firat, Orhan},
  date = {2021-11},
  pages = {3577--3599},
  publisher = {{Association for Computational Linguistics}},
  location = {{Punta Cana, Dominican Republic}},
  url = {https://aclanthology.org/2021.findings-emnlp.304},
  urldate = {2023-03-03},
  abstract = {Sparse Mixture-of-Experts (MoE) has been a successful approach for scaling multilingual translation models to billions of parameters without a proportional increase in training computation. However, MoE models are prohibitively large and practitioners often resort to methods such as distillation for serving. In this work, we investigate routing strategies at different granularity (token, sentence, task) in MoE models to bypass distillation. Experiments on WMT and a web-scale dataset suggest that task-level routing (task-MoE) enables us to extract smaller, ready-to-deploy sub-networks from large sparse models. On WMT, our task-MoE with 32 experts (533M parameters) outperforms the best performing token-level MoE model (token-MoE) by +1.0 BLEU on average across 30 language pairs. The peak inference throughput is also improved by a factor of 1.9x when we route by tasks instead of tokens. While distilling a token-MoE to a smaller dense model preserves only 32\% of the BLEU gains, our sub-network task-MoE, by design, preserves all the gains with the same inference cost as the distilled student model. Finally, when scaling up to 200 language pairs, our 128-expert task-MoE (13B parameters) performs competitively with a token-level counterpart, while improving the peak inference throughput by a factor of 2.6x.},
  eventtitle = {Findings 2021},
  file = {/Users/lukakuma/Zotero/storage/QPVC66AH/Kudugunta et al. - 2021 - Beyond Distillation Task-level Mixture-of-Experts.pdf}
}

@online{kuhnSemanticUncertaintyLinguistic2023,
  title = {Semantic {{Uncertainty}}: {{Linguistic Invariances}} for {{Uncertainty Estimation}} in {{Natural Language Generation}}},
  shorttitle = {Semantic {{Uncertainty}}},
  author = {Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
  date = {2023-02-21},
  number = {arXiv:2302.09664},
  eprint = {arXiv:2302.09664},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.09664},
  urldate = {2023-02-25},
  abstract = {We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of "semantic equivalence" -- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.},
  pubstate = {preprint},
  keywords = {calibration},
  file = {/Users/lukakuma/Zotero/storage/MB69TQ6D/Kuhn et al. - 2023 - Semantic Uncertainty Linguistic Invariances for U.pdf;/Users/lukakuma/Zotero/storage/HNUYHDIG/2302.html}
}

@unpublished{kumarDataAugmentationUsing2021,
  title = {Data {{Augmentation}} Using {{Pre-trained Transformer Models}}},
  author = {Kumar, Varun and Choudhary, Ashutosh and Cho, Eunah},
  date = {2021-01-31},
  eprint = {2003.02245},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.02245},
  urldate = {2022-04-13},
  abstract = {Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of transformer based pre-trained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classification benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information.},
  file = {/Users/lukakuma/Zotero/storage/JRDZF8N2/Kumar et al. - 2021 - Data Augmentation using Pre-trained Transformer Mo.pdf;/Users/lukakuma/Zotero/storage/NPQC3PZN/2003.html}
}

@online{kumarWhenShouldWe2022,
  title = {When {{Should We Prefer Offline Reinforcement Learning Over Behavioral Cloning}}?},
  author = {Kumar, Aviral and Hong, Joey and Singh, Anikait and Levine, Sergey},
  date = {2022-04-12},
  number = {arXiv:2204.05618},
  eprint = {arXiv:2204.05618},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.05618},
  urldate = {2022-07-24},
  abstract = {Offline reinforcement learning (RL) algorithms can acquire effective policies by utilizing previously collected experience, without any online interaction. It is widely understood that offline RL is able to extract good policies even from highly suboptimal data, a scenario where imitation learning finds suboptimal solutions that do not improve over the demonstrator that generated the dataset. However, another common use case for practitioners is to learn from data that resembles demonstrations. In this case, one can choose to apply offline RL, but can also use behavioral cloning (BC) algorithms, which mimic a subset of the dataset via supervised learning. Therefore, it seems natural to ask: when can an offline RL method outperform BC with an equal amount of expert data, even when BC is a natural choice? To answer this question, we characterize the properties of environments that allow offline RL methods to perform better than BC methods, even when only provided with expert data. Additionally, we show that policies trained on sufficiently noisy suboptimal data can attain better performance than even BC algorithms with expert data, especially on long-horizon problems. We validate our theoretical results via extensive experiments on both diagnostic and high-dimensional domains including robotic manipulation, maze navigation, and Atari games, with a variety of data distributions. We observe that, under specific but common conditions such as sparse rewards or noisy data sources, modern offline RL methods can significantly outperform BC.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/UM9Z34IM/Kumar et al. - 2022 - When Should We Prefer Offline Reinforcement Learni.pdf;/Users/lukakuma/Zotero/storage/Z4C7EIPC/2204.html}
}

@article{kundaCaseMotivatedReasoning1990,
  title = {The Case for Motivated Reasoning},
  author = {Kunda, Z.},
  date = {1990-11},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychol Bull},
  volume = {108},
  number = {3},
  eprint = {2270237},
  eprinttype = {pmid},
  pages = {480--498},
  issn = {0033-2909},
  doi = {10.1037/0033-2909.108.3.480},
  abstract = {It is proposed that motivation may affect reasoning through reliance on a biased set of cognitive processes--that is, strategies for accessing, constructing, and evaluating beliefs. The motivation to be accurate enhances use of those beliefs and strategies that are considered most appropriate, whereas the motivation to arrive at particular conclusions enhances use of those that are considered most likely to yield the desired conclusion. There is considerable evidence that people are more likely to arrive at conclusions that they want to arrive at, but their ability to do so is constrained by their ability to construct seemingly reasonable justifications for these conclusions. These ideas can account for a wide variety of research concerned with motivated reasoning.},
  langid = {english},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/FF5DRX42/Kunda - 1990 - The case for motivated reasoning.pdf}
}

@online{kuoFindItGeneralizedLocalization2022,
  title = {{{FindIt}}: {{Generalized Localization}} with {{Natural Language Queries}}},
  shorttitle = {{{FindIt}}},
  author = {Kuo, Weicheng and Bertsch, Fred and Li, Wei and Piergiovanni, A. J. and Saffar, Mohammad and Angelova, Anelia},
  date = {2022-08-08},
  number = {arXiv:2203.17273},
  eprint = {arXiv:2203.17273},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.17273},
  urldate = {2023-02-07},
  abstract = {We propose FindIt, a simple and versatile framework that unifies a variety of visual grounding and localization tasks including referring expression comprehension, text-based localization, and object detection. Key to our architecture is an efficient multi-scale fusion module that unifies the disparate localization requirements across the tasks. In addition, we discover that a standard object detector is surprisingly effective in unifying these tasks without a need for task-specific design, losses, or pre-computed detections. Our end-to-end trainable framework responds flexibly and accurately to a wide range of referring expression, localization or detection queries for zero, one, or multiple objects. Jointly trained on these tasks, FindIt outperforms the state of the art on both referring expression and text-based localization, and shows competitive performance on object detection. Finally, FindIt generalizes better to out-of-distribution data and novel categories compared to strong single-task baselines. All of these are accomplished by a single, unified and efficient model. The code will be released.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/Y428GTRB/Kuo et al. - 2022 - FindIt Generalized Localization with Natural Lang.pdf;/Users/lukakuma/Zotero/storage/KJG547ZV/2203.html}
}

@unpublished{kurinMyBodyCage2021,
  title = {My {{Body}} Is a {{Cage}}: The {{Role}} of {{Morphology}} in {{Graph-Based Incompatible Control}}},
  shorttitle = {My {{Body}} Is a {{Cage}}},
  author = {Kurin, Vitaly and Igl, Maximilian and Rockt\"aschel, Tim and Boehmer, Wendelin and Whiteson, Shimon},
  date = {2021-04-14},
  number = {arXiv:2010.01856},
  eprint = {2010.01856},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2010.01856},
  urldate = {2022-05-13},
  abstract = {Multitask Reinforcement Learning is a promising way to obtain models with better performance, generalisation, data efficiency, and robustness. Most existing work is limited to compatible settings, where the state and action space dimensions are the same across tasks. Graph Neural Networks (GNN) are one way to address incompatible environments, because they can process graphs of arbitrary size. They also allow practitioners to inject biases encoded in the structure of the input graph. Existing work in graph-based continuous control uses the physical morphology of the agent to construct the input graph, i.e., encoding limb features as node labels and using edges to connect the nodes if their corresponded limbs are physically connected. In this work, we present a series of ablations on existing methods that show that morphological information encoded in the graph does not improve their performance. Motivated by the hypothesis that any benefits GNNs extract from the graph structure are outweighed by difficulties they create for message passing, we also propose Amorpheus, a transformer-based approach. Further results show that, while Amorpheus ignores the morphological information that GNNs encode, it nonetheless substantially outperforms GNN-based methods that use the morphological information to define the message-passing scheme.},
  keywords = {(ext) Gato},
  file = {/Users/lukakuma/Zotero/storage/LIQ6ZHN5/Kurin et al. - 2021 - My Body is a Cage the Role of Morphology in Graph.pdf;/Users/lukakuma/Zotero/storage/YKEL4ETU/2010.html}
}

@online{kurutachModelEnsembleTrustRegionPolicy2018,
  title = {Model-{{Ensemble Trust-Region Policy Optimization}}},
  author = {Kurutach, Thanard and Clavera, Ignasi and Duan, Yan and Tamar, Aviv and Abbeel, Pieter},
  date = {2018-10-05},
  number = {arXiv:1802.10592},
  eprint = {arXiv:1802.10592},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.10592},
  urldate = {2022-07-20},
  abstract = {Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/YRLL73RR/Kurutach et al. - 2018 - Model-Ensemble Trust-Region Policy Optimization.pdf;/Users/lukakuma/Zotero/storage/LX9TT6ZK/1802.html}
}

@online{kuttlerNetHackLearningEnvironment2020,
  title = {The {{NetHack Learning Environment}}},
  author = {K\"uttler, Heinrich and Nardelli, Nantas and Miller, Alexander H. and Raileanu, Roberta and Selvatici, Marco and Grefenstette, Edward and Rockt\"aschel, Tim},
  date = {2020-12-01},
  number = {arXiv:2006.13760},
  eprint = {arXiv:2006.13760},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.13760},
  urldate = {2022-06-25},
  abstract = {Progress in Reinforcement Learning (RL) algorithms goes hand-in-hand with the development of challenging environments that test the limits of current methods. While existing RL environments are either sufficiently complex or based on fast simulation, they are rarely both. Here, we present the NetHack Learning Environment (NLE), a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular single-player terminal-based roguelike game, NetHack. We argue that NetHack is sufficiently complex to drive long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience. We compare NLE and its task suite to existing alternatives, and discuss why it is an ideal medium for testing the robustness and systematic generalization of RL agents. We demonstrate empirical success for early stages of the game using a distributed Deep RL baseline and Random Network Distillation exploration, alongside qualitative analysis of various agents trained in the environment. NLE is open source at https://github.com/facebookresearch/nle.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/EEL84FGQ/K√ºttler et al. - 2020 - The NetHack Learning Environment.pdf;/Users/lukakuma/Zotero/storage/BF46XHJZ/2006.html}
}

@article{kwiatkowskiNaturalQuestionsBenchmark2019,
  title = {Natural {{Questions}}: {{A Benchmark}} for {{Question Answering Research}}},
  shorttitle = {Natural {{Questions}}},
  author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
  date = {2019},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {452--466},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA}},
  url = {https://aclanthology.org/Q19-1026},
  urldate = {2023-01-06},
  abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
  file = {/Users/lukakuma/Zotero/storage/U6NRTDKN/Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answer.pdf}
}

@article{ladoszExplorationDeepReinforcement2022,
  title = {Exploration in {{Deep Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Exploration in {{Deep Reinforcement Learning}}},
  author = {Ladosz, Pawel and Weng, Lilian and Kim, Minwoo and Oh, Hyondong},
  date = {2022-05},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {85},
  eprint = {2205.00824},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--22},
  issn = {15662535},
  url = {http://arxiv.org/abs/2205.00824},
  urldate = {2022-07-20},
  abstract = {This paper reviews exploration techniques in deep reinforcement learning. Exploration techniques are of primary importance when solving sparse reward problems. In sparse reward problems, the reward is rare, which means that the agent will not find the reward often by acting randomly. In such a scenario, it is challenging for reinforcement learning to learn rewards and actions association. Thus more sophisticated exploration methods need to be devised. This review provides a comprehensive overview of existing exploration approaches, which are categorized based on the key contributions as follows reward novel states, reward diverse behaviours, goal-based methods, probabilistic methods, imitation-based methods, safe exploration and random-based methods. Then, the unsolved challenges are discussed to provide valuable future research directions. Finally, the approaches of different categories are compared in terms of complexity, computational effort and overall performance.},
  file = {/Users/lukakuma/Zotero/storage/UEAAIGUV/Ladosz et al. - 2022 - Exploration in Deep Reinforcement Learning A Surv.pdf;/Users/lukakuma/Zotero/storage/VJ8Z3MBS/2205.html}
}

@online{lahoud3DVisionTransformers2022,
  title = {{{3D Vision}} with {{Transformers}}: {{A Survey}}},
  shorttitle = {{{3D Vision}} with {{Transformers}}},
  author = {Lahoud, Jean and Cao, Jiale and Khan, Fahad Shahbaz and Cholakkal, Hisham and Anwer, Rao Muhammad and Khan, Salman and Yang, Ming-Hsuan},
  date = {2022-08-08},
  number = {arXiv:2208.04309},
  eprint = {arXiv:2208.04309},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.04309},
  urldate = {2022-08-10},
  abstract = {The success of the transformer architecture in natural language processing has recently triggered attention in the computer vision field. The transformer has been used as a replacement for the widely used convolution operators, due to its ability to learn long-range dependencies. This replacement was proven to be successful in numerous tasks, in which several state-of-the-art methods rely on transformers for better learning. In computer vision, the 3D field has also witnessed an increase in employing the transformer for 3D convolution neural networks and multi-layer perceptron networks. Although a number of surveys have focused on transformers in vision in general, 3D vision requires special attention due to the difference in data representation and processing when compared to 2D vision. In this work, we present a systematic and thorough review of more than 100 transformers methods for different 3D vision tasks, including classification, segmentation, detection, completion, pose estimation, and others. We discuss transformer design in 3D vision, which allows it to process data with various 3D representations. For each application, we highlight key properties and contributions of proposed transformer-based methods. To assess the competitiveness of these methods, we compare their performance to common non-transformer methods on 12 3D benchmarks. We conclude the survey by discussing different open directions and challenges for transformers in 3D vision. In addition to the presented papers, we aim to frequently update the latest relevant papers along with their corresponding implementations at: https://github.com/lahoud/3d-vision-transformers.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/UXFSZ6D5/Lahoud et al. - 2022 - 3D Vision with Transformers A Survey.pdf;/Users/lukakuma/Zotero/storage/PU8QRF97/2208.html}
}

@unpublished{lampleDeepLearningSymbolic2019,
  title = {Deep {{Learning}} for {{Symbolic Mathematics}}},
  author = {Lample, Guillaume and Charton, Fran\c{c}ois},
  date = {2019-12-02},
  eprint = {1912.01412},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.01412},
  urldate = {2022-04-23},
  abstract = {Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.},
  file = {/Users/lukakuma/Zotero/storage/A8R3J5L9/Lample and Charton - 2019 - Deep Learning for Symbolic Mathematics.pdf;/Users/lukakuma/Zotero/storage/Q564PX2S/1912.html}
}

@online{lampleHyperTreeProofSearch2022,
  title = {{{HyperTree Proof Search}} for {{Neural Theorem Proving}}},
  author = {Lample, Guillaume and Lachaux, Marie-Anne and Lavril, Thibaut and Martinet, Xavier and Hayat, Amaury and Ebner, Gabriel and Rodriguez, Aur\'elien and Lacroix, Timoth\'ee},
  date = {2022-05-23},
  number = {arXiv:2205.11491},
  eprint = {arXiv:2205.11491},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.11491},
  urldate = {2023-02-20},
  abstract = {We propose an online training procedure for a transformer-based automated theorem prover. Our approach leverages a new search algorithm, HyperTree Proof Search (HTPS), inspired by the recent success of AlphaZero. Our model learns from previous proof searches through online training, allowing it to generalize to domains far from the training distribution. We report detailed ablations of our pipeline's main components by studying performance on three environments of increasing complexity. In particular, we show that with HTPS alone, a model trained on annotated proofs manages to prove 65.4\% of a held-out set of Metamath theorems, significantly outperforming the previous state of the art of 56.5\% by GPT-f. Online training on these unproved theorems increases accuracy to 82.6\%. With a similar computational budget, we improve the state of the art on the Lean-based miniF2F-curriculum dataset from 31\% to 42\% proving accuracy.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/BVWLYZRF/Lample et al. - 2022 - HyperTree Proof Search for Neural Theorem Proving.pdf;/Users/lukakuma/Zotero/storage/L8IGABXN/2205.html}
}

@unpublished{lansdellLearningtolearn2019,
  title = {Towards Learning-to-Learn},
  author = {Lansdell, Benjamin James and Kording, Konrad Paul},
  date = {2019-01-09},
  number = {arXiv:1811.00231},
  eprint = {1811.00231},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1811.00231},
  urldate = {2022-06-08},
  abstract = {In good old-fashioned artificial intelligence (GOFAI), humans specified systems that solved problems. Much of the recent progress in AI has come from replacing human insights by learning. However, learning itself is still usually built by humans -- specifically the choice that parameter updates should follow the gradient of a cost function. Yet, in analogy with GOFAI, there is no reason to believe that humans are particularly good at defining such learning systems: we may expect learning itself to be better if we learn it. Recent research in machine learning has started to realize the benefits of that strategy. We should thus expect this to be relevant for neuroscience: how could the correct learning rules be acquired? Indeed, cognitive science has long shown that humans learn-to-learn, which is potentially responsible for their impressive learning abilities. Here we discuss ideas across machine learning, neuroscience, and cognitive science that matter for the principle of learning-to-learn.},
  file = {/Users/lukakuma/Zotero/storage/89PLY7TS/Lansdell and Kording - 2019 - Towards learning-to-learn.pdf;/Users/lukakuma/Zotero/storage/W3879UR9/1811.html}
}

@online{laskinIncontextReinforcementLearning2022,
  title = {In-Context {{Reinforcement Learning}} with {{Algorithm Distillation}}},
  author = {Laskin, Michael and Wang, Luyu and Oh, Junhyuk and Parisotto, Emilio and Spencer, Stephen and Steigerwald, Richie and Strouse, D. J. and Hansen, Steven and Filos, Angelos and Brooks, Ethan and Gazeau, Maxime and Sahni, Himanshu and Singh, Satinder and Mnih, Volodymyr},
  date = {2022-10-25},
  number = {arXiv:2210.14215},
  eprint = {arXiv:2210.14215},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.14215},
  urldate = {2022-11-17},
  abstract = {We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data.},
  pubstate = {preprint},
  keywords = {self-play},
  file = {/Users/lukakuma/Zotero/storage/VKSTRUJL/Laskin et al. - 2022 - In-context Reinforcement Learning with Algorithm D.pdf;/Users/lukakuma/Zotero/storage/R8X9Y59U/2210.html}
}

@inproceedings{laurenconBigScienceROOTSCorpus2022,
  title = {The {{BigScience ROOTS Corpus}}: {{A}} 1.{{6TB Composite Multilingual Dataset}}},
  shorttitle = {The {{BigScience ROOTS Corpus}}},
  author = {Lauren\c{c}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and family=Moral, given=Albert Villanova, prefix=del, useprefix=false and Scao, Teven Le and Werra, Leandro Von and Mou, Chenghao and Ponferrada, Eduardo Gonz\'alez and Nguyen, Huu and Frohberg, J\"org and \v{S}a\v{s}ko, Mario and Lhoest, Quentin and McMillan-Major, Angelina and Dupont, G\'erard and Biderman, Stella and Rogers, Anna and Allal, Loubna Ben and Toni, Francesco De and Pistilli, Giada and Nguyen, Olivier and Nikpoor, Somaieh and Masoud, Maraim and Colombo, Pierre and family=Rosa, given=Javier, prefix=de la, useprefix=false and Villegas, Paulo and Thrush, Tristan and Longpre, Shayne and Nagel, Sebastian and Weber, Leon and Mu\~noz, Manuel Romero and Zhu, Jian and Strien, Daniel Van and Alyafeai, Zaid and Almubarak, Khalid and Chien, Vu Minh and Gonzalez-Dios, Itziar and Soroa, Aitor and Lo, Kyle and Dey, Manan and Suarez, Pedro Ortiz and Gokaslan, Aaron and Bose, Shamik and Adelani, David Ifeoluwa and Phan, Long and Tran, Hieu and Yu, Ian and Pai, Suhas and Chim, Jenny and Lepercq, Violette and Ilic, Suzana and Mitchell, Margaret and Luccioni, Sasha and Jernite, Yacine},
  date = {2022-10-12},
  url = {https://openreview.net/forum?id=UoEw6KigkUn},
  urldate = {2022-10-13},
  abstract = {As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.},
  eventtitle = {Thirty-Sixth {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  langid = {english},
  keywords = {pretraining},
  file = {/Users/lukakuma/Zotero/storage/CTKIYIK7/Lauren√ßon et al. - 2022 - The BigScience ROOTS Corpus A 1.6TB Composite Mul.pdf;/Users/lukakuma/Zotero/storage/QMUEC85M/forum.html}
}

@online{lazaridouEmergentMultiAgentCommunication2020,
  title = {Emergent {{Multi-Agent Communication}} in the {{Deep Learning Era}}},
  author = {Lazaridou, Angeliki and Baroni, Marco},
  date = {2020-07-14},
  number = {arXiv:2006.02419},
  eprint = {arXiv:2006.02419},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.02419},
  urldate = {2023-02-20},
  abstract = {The ability to cooperate through language is a defining feature of humans. As the perceptual, motory and planning capabilities of deep artificial networks increase, researchers are studying whether they also can develop a shared language to interact. From a scientific perspective, understanding the conditions under which language evolves in communities of deep agents and its emergent features can shed light on human language evolution. From an applied perspective, endowing deep networks with the ability to solve problems interactively by communicating with each other and with us should make them more flexible and useful in everyday life. This article surveys representative recent language emergence studies from both of these two angles.},
  pubstate = {preprint},
  keywords = {multiagent},
  file = {/Users/lukakuma/Zotero/storage/8S45ZJGH/Lazaridou and Baroni - 2020 - Emergent Multi-Agent Communication in the Deep Lea.pdf;/Users/lukakuma/Zotero/storage/K9FFSU28/2006.html}
}

@online{lazaridouMultiagentCommunicationMeets2020,
  title = {Multi-Agent {{Communication}} Meets {{Natural Language}}: {{Synergies}} between {{Functional}} and {{Structural Language Learning}}},
  shorttitle = {Multi-Agent {{Communication}} Meets {{Natural Language}}},
  author = {Lazaridou, Angeliki and Potapenko, Anna and Tieleman, Olivier},
  date = {2020-05-14},
  number = {arXiv:2005.07064},
  eprint = {arXiv:2005.07064},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.07064},
  urldate = {2023-02-20},
  abstract = {We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language. Our starting point is a language model that has been trained on generic, not task-specific language data. We then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model, turning it into a task-conditional language model. We introduce a new way for combining the two types of learning based on the idea of reranking language model samples, and show that this method outperforms others in communicating with humans in a visual referential communication task. Finally, we present a taxonomy of different types of language drift that can occur alongside a set of measures to detect them.},
  pubstate = {preprint},
  keywords = {multiagent},
  file = {/Users/lukakuma/Zotero/storage/NZR4ZCSD/Lazaridou et al. - 2020 - Multi-agent Communication meets Natural Language .pdf;/Users/lukakuma/Zotero/storage/TI5K99KV/2005.html}
}

@unpublished{leadholmGridCellPath2021,
  title = {Grid {{Cell Path Integration For Movement-Based Visual Object Recognition}}},
  author = {Leadholm, Niels and Lewis, Marcus and Ahmad, Subutai},
  date = {2021-02-17},
  number = {arXiv:2102.09076},
  eprint = {2102.09076},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2102.09076},
  urldate = {2022-06-13},
  abstract = {Grid cells enable the brain to model the physical space of the world and navigate effectively via path integration, updating self-position using information from self-movement. Recent proposals suggest that the brain might use similar mechanisms to understand the structure of objects in diverse sensory modalities, including vision. In machine vision, object recognition given a sequence of sensory samples of an image, such as saccades, is a challenging problem when the sequence does not follow a consistent, fixed pattern - yet this is something humans do naturally and effortlessly. We explore how grid cell-based path integration in a cortical network can support reliable recognition of objects given an arbitrary sequence of inputs. Our network (GridCellNet) uses grid cell computations to integrate visual information and make predictions based on movements. We use local Hebbian plasticity rules to learn rapidly from a handful of examples (few-shot learning), and consider the task of recognizing MNIST digits given only a sequence of image feature patches. We compare GridCellNet to k-Nearest Neighbour (k-NN) classifiers as well as recurrent neural networks (RNNs), both of which lack explicit mechanisms for handling arbitrary sequences of input samples. We show that GridCellNet can reliably perform classification, generalizing to both unseen examples and completely novel sequence trajectories. We further show that inference is often successful after sampling a fraction of the input space, enabling the predictive GridCellNet to reconstruct the rest of the image given just a few movements. We propose that dynamically moving agents with active sensors can use grid cell representations not only for navigation, but also for efficient recognition and feature prediction of seen objects.},
  file = {/Users/lukakuma/Zotero/storage/CNETQDFB/Leadholm et al. - 2021 - Grid Cell Path Integration For Movement-Based Visu.pdf;/Users/lukakuma/Zotero/storage/AQPSJP37/2102.html}
}

@online{leahyWhatLongStrange2021,
  title = {What {{A Long}}, {{Strange Trip It}}'s {{Been}}: {{EleutherAI One Year Retrospective}}},
  shorttitle = {What {{A Long}}, {{Strange Trip It}}'s {{Been}}},
  author = {Leahy, Connor and Hallahan, Eric and Gao, Leo and Biderman, Stella},
  date = {2021-07-07T20:00:00-04:00},
  url = {https://blog.eleuther.ai/year-one/},
  urldate = {2022-08-18},
  abstract = {A look back at the first year of EleutherAI.},
  langid = {english},
  organization = {{EleutherAI Blog}},
  file = {/Users/lukakuma/Zotero/storage/XPA6RWAD/year-one.html}
}

@online{learningFullStackDeep2022,
  title = {Full {{Stack Deep Learning}}},
  author = {Learning, Full Stack Deep},
  date = {2022},
  url = {https://fullstackdeeplearning.com/},
  urldate = {2022-09-15},
  abstract = {The community for people building ML-powered products.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/GL9IZCIV/fullstackdeeplearning.com.html}
}

@online{leCodeRLMasteringCode2022,
  title = {{{CodeRL}}: {{Mastering Code Generation}} through {{Pretrained Models}} and {{Deep Reinforcement Learning}}},
  shorttitle = {{{CodeRL}}},
  author = {Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven C. H.},
  date = {2022-11-03},
  number = {arXiv:2207.01780},
  eprint = {arXiv:2207.01780},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.01780},
  urldate = {2022-11-16},
  abstract = {Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose "CodeRL", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.},
  pubstate = {preprint},
  keywords = {code},
  file = {/Users/lukakuma/Zotero/storage/45GL6E6Z/Le et al. - 2022 - CodeRL Mastering Code Generation through Pretrain.pdf;/Users/lukakuma/Zotero/storage/2D2AWGYG/2207.html}
}

@article{lecunPathAutonomousMachine2022,
  title = {A {{Path Towards Autonomous Machine Intelligence Version}} 0.9.2, 2022-06-27},
  author = {LeCun, Yann},
  date = {2022-06-27},
  pages = {62},
  abstract = {How could machines learn as efficiently as humans and animals? How could machines learn to reason and plan? How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons? This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as configurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.},
  langid = {english},
  keywords = {jepa,read},
  file = {/Users/lukakuma/Zotero/storage/MS3BFMU3/LeCun - A Path Towards Autonomous Machine Intelligence Ver.pdf}
}

@online{leeDeduplicatingTrainingData2022,
  title = {Deduplicating {{Training Data Makes Language Models Better}}},
  author = {Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  date = {2022-03-24},
  number = {arXiv:2107.06499},
  eprint = {arXiv:2107.06499},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.06499},
  urldate = {2022-11-02},
  abstract = {We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1\% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets\textemdash for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4\% of the validation set of standard datasets, thus allowing for more accurate evaluation. Code for deduplication is released at https://github.com/goog e-research/ dedup icate-text-datasets.},
  langid = {english},
  pubstate = {preprint},
  keywords = {pretraining},
  file = {/Users/lukakuma/Zotero/storage/G3NDSV8Y/Lee et al. - 2022 - Deduplicating Training Data Makes Language Models .pdf}
}

@online{leeMultiGameDecisionTransformers2022,
  title = {Multi-{{Game Decision Transformers}}},
  author = {Lee, Kuang-Huei and Nachum, Ofir and Yang, Mengjiao and Lee, Lisa and Freeman, Daniel and Xu, Winnie and Guadarrama, Sergio and Fischer, Ian and Jang, Eric and Michalewski, Henryk and Mordatch, Igor},
  date = {2022-05-30},
  number = {arXiv:2205.15241},
  eprint = {arXiv:2205.15241},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.15241},
  urldate = {2022-07-23},
  abstract = {A longstanding goal of the field of AI is a strategy for compiling diverse experience into a highly capable, generalist agent. In the subfields of vision and language, this was largely achieved by scaling up transformer-based models and training them on large, diverse datasets. Motivated by this progress, we investigate whether the same strategy can be used to produce generalist reinforcement learning agents. Specifically, we show that a single transformer-based model - with a single set of weights - trained purely offline can play a suite of up to 46 Atari games simultaneously at close-to-human performance. When trained and evaluated appropriately, we find that the same trends observed in language and vision hold, including scaling of performance with model size and rapid adaptation to new games via fine-tuning. We compare several approaches in this multi-game setting, such as online and offline RL methods and behavioral cloning, and find that our Multi-Game Decision Transformer models offer the best scalability and performance. We release the pre-trained models and code to encourage further research in this direction. Additional information, videos and code can be seen at: sites.google.com/view/multi-game-transformers},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/2RNDF974/Lee et al. - 2022 - Multi-Game Decision Transformers.pdf;/Users/lukakuma/Zotero/storage/FDLA62DZ/2205.html}
}

@unpublished{leeSetTransformerFramework2019,
  title = {Set {{Transformer}}: {{A Framework}} for {{Attention-based Permutation-Invariant Neural Networks}}},
  shorttitle = {Set {{Transformer}}},
  author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R. and Choi, Seungjin and Teh, Yee Whye},
  date = {2019-05-26},
  eprint = {1810.00825},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.00825},
  urldate = {2022-05-06},
  abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.},
  file = {/Users/lukakuma/Zotero/storage/LKCHDZTD/Lee et al. - 2019 - Set Transformer A Framework for Attention-based P.pdf;/Users/lukakuma/Zotero/storage/YH7N55FG/1810.html}
}

@unpublished{leggCollectionDefinitionsIntelligence2007,
  title = {A {{Collection}} of {{Definitions}} of {{Intelligence}}},
  author = {Legg, Shane and Hutter, Marcus},
  date = {2007-06-25},
  number = {arXiv:0706.3639},
  eprint = {0706.3639},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/0706.3639},
  urldate = {2022-06-07},
  abstract = {This paper is a survey of a large number of informal definitions of ``intelligence'' that the authors have collected over the years. Naturally, compiling a complete list would be impossible as many definitions of intelligence are buried deep inside articles and books. Nevertheless, the 70-odd definitions presented here are, to the authors' knowledge, the largest and most well referenced collection there is.},
  file = {/Users/lukakuma/Zotero/storage/LVSZPLCP/Legg and Hutter - 2007 - A Collection of Definitions of Intelligence.pdf;/Users/lukakuma/Zotero/storage/IPBZ7QZS/0706.html}
}

@unpublished{leggUniversalIntelligenceDefinition2007,
  title = {Universal {{Intelligence}}: {{A Definition}} of {{Machine Intelligence}}},
  shorttitle = {Universal {{Intelligence}}},
  author = {Legg, Shane and Hutter, Marcus},
  date = {2007-12-20},
  number = {arXiv:0712.3329},
  eprint = {0712.3329},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/0712.3329},
  urldate = {2022-06-07},
  abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/3A8X77JQ/Legg and Hutter - 2007 - Universal Intelligence A Definition of Machine In.pdf;/Users/lukakuma/Zotero/storage/2XYK7XRZ/0712.html}
}

@online{leikeScalableAgentAlignment2018a,
  title = {Scalable Agent Alignment via Reward Modeling: A Research Direction},
  shorttitle = {Scalable Agent Alignment via Reward Modeling},
  author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  date = {2018-11-19},
  number = {arXiv:1811.07871},
  eprint = {arXiv:1811.07871},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1811.07871},
  urldate = {2022-12-28},
  abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow,alignment},
  file = {/Users/lukakuma/Zotero/storage/FHRIPH8R/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf;/Users/lukakuma/Zotero/storage/JXT6AJ93/1811.html}
}

@unpublished{lepikhinGShardScalingGiant2020,
  title = {{{GShard}}: {{Scaling Giant Models}} with {{Conditional Computation}} and {{Automatic Sharding}}},
  shorttitle = {{{GShard}}},
  author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  date = {2020-06-30},
  eprint = {2006.16668},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.16668},
  urldate = {2022-04-14},
  abstract = {Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.},
  keywords = {distributed training,sparsity},
  file = {/Users/lukakuma/Zotero/storage/QCJS32MZ/Lepikhin et al. - 2020 - GShard Scaling Giant Models with Conditional Comp.pdf;/Users/lukakuma/Zotero/storage/9ARXZRB2/2006.html}
}

@online{lesterPowerScaleParameterEfficient2021,
  title = {The {{Power}} of {{Scale}} for {{Parameter-Efficient Prompt Tuning}}},
  author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  date = {2021-09-02},
  number = {arXiv:2104.08691},
  eprint = {arXiv:2104.08691},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.08691},
  urldate = {2022-12-13},
  abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
  pubstate = {preprint},
  keywords = {*peft,3-prefix tuning,read},
  file = {/Users/lukakuma/Zotero/storage/RD52397I/Lester et al. - 2021 - The Power of Scale for Parameter-Efficient Prompt .pdf;/Users/lukakuma/Zotero/storage/796N9G7I/2104.html}
}

@online{lesterReducingRetrainingRecycling2022,
  title = {Reducing {{Retraining}} by {{Recycling Parameter-Efficient Prompts}}},
  author = {Lester, Brian and Yurtsever, Joshua and Shakeri, Siamak and Constant, Noah},
  date = {2022-08-10},
  number = {arXiv:2208.05577},
  eprint = {arXiv:2208.05577},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.05577},
  urldate = {2023-02-16},
  abstract = {Parameter-efficient methods are able to use a single frozen pre-trained large language model (LLM) to perform many tasks by learning task-specific soft prompts that modulate model behavior when concatenated to the input text. However, these learned prompts are tightly coupled to a given frozen model -- if the model is updated, corresponding new prompts need to be obtained. In this work, we propose and investigate several approaches to "Prompt Recycling'" where a prompt trained on a source model is transformed to work with the new target model. Our methods do not rely on supervised pairs of prompts, task-specific data, or training updates with the target model, which would be just as costly as re-tuning prompts with the target model from scratch. We show that recycling between models is possible (our best settings are able to successfully recycle \$88.9\textbackslash\%\$ of prompts, producing a prompt that out-performs baselines), but significant performance headroom remains, requiring improved recycling techniques.},
  pubstate = {preprint},
  keywords = {*peft,3-prefix tuning},
  file = {/Users/lukakuma/Zotero/storage/H8GMS8CM/Lester et al. - 2022 - Reducing Retraining by Recycling Parameter-Efficie.pdf;/Users/lukakuma/Zotero/storage/HZQG9EGH/2208.html}
}

@online{levineCS285Deep2020,
  title = {{{CS}} 285 {{Deep Reinforcement Learning}}},
  author = {Levine, Sergey},
  date = {2020},
  url = {http://rail.eecs.berkeley.edu/deeprlcourse/},
  urldate = {2022-07-20},
  file = {/Users/lukakuma/Zotero/storage/ZJLSP4WT/deeprlcourse.html}
}

@online{levineOfflineReinforcementLearning2020,
  title = {Offline {{Reinforcement Learning}}: {{Tutorial}}, {{Review}}, and {{Perspectives}} on {{Open Problems}}},
  shorttitle = {Offline {{Reinforcement Learning}}},
  author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  date = {2020-11-01},
  number = {arXiv:2005.01643},
  eprint = {arXiv:2005.01643},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.01643},
  urldate = {2022-07-25},
  abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
  pubstate = {preprint},
  version = {2},
  file = {/Users/lukakuma/Zotero/storage/935SWDY3/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf;/Users/lukakuma/Zotero/storage/D7SBVDX7/2005.html}
}

@online{levineUnderstandingWorldAction2021,
  title = {Understanding the {{World Through Action}}},
  author = {Levine, Sergey},
  date = {2021-10-24},
  number = {arXiv:2110.12543},
  eprint = {arXiv:2110.12543},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.12543},
  urldate = {2022-06-28},
  abstract = {The recent history of machine learning research has taught us that machine learning methods can be most effective when they are provided with very large, high-capacity models, and trained on very large and diverse datasets. This has spurred the community to search for ways to remove any bottlenecks to scale. Often the foremost among such bottlenecks is the need for human effort, including the effort of curating and labeling datasets. As a result, considerable attention in recent years has been devoted to utilizing unlabeled data, which can be collected in vast quantities. However, some of the most widely used methods for training on such unlabeled data themselves require human-designed objective functions that must correlate in some meaningful way to downstream tasks. I will argue that a general, principled, and powerful framework for utilizing unlabeled data can be derived from reinforcement learning, using general purpose unsupervised or self-supervised reinforcement learning objectives in concert with offline reinforcement learning methods that can leverage large datasets. I will discuss how such a procedure is more closely aligned with potential downstream tasks, and how it could build on existing techniques that have been developed in recent years.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/JYDASPNS/Levine - 2021 - Understanding the World Through Action.pdf;/Users/lukakuma/Zotero/storage/75P37B42/2110.html}
}

@inproceedings{lewisBARTDenoisingSequencetoSequence2020,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  date = {2020},
  pages = {7871--7880},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  url = {https://www.aclweb.org/anthology/2020.acl-main.703},
  urldate = {2022-11-28},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/QW8H52LG/Lewis et al. - 2020 - BART Denoising Sequence-to-Sequence Pre-training .pdf}
}

@online{lewisBASELayersSimplifying2021,
  title = {{{BASE Layers}}: {{Simplifying Training}} of {{Large}}, {{Sparse Models}}},
  shorttitle = {{{BASE Layers}}},
  author = {Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  date = {2021-03-30},
  number = {arXiv:2103.16716},
  eprint = {arXiv:2103.16716},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.16716},
  urldate = {2022-10-18},
  abstract = {We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released at https://github.com/pytorch/fairseq/},
  langid = {english},
  pubstate = {preprint},
  keywords = {sparsity},
  file = {/Users/lukakuma/Zotero/storage/N226GL85/Lewis et al. - 2021 - BASE Layers Simplifying Training of Large, Sparse.pdf}
}

@unpublished{lewisHippocampalSpatialMapping2021,
  title = {Hippocampal {{Spatial Mapping As Fast Graph Learning}}},
  author = {Lewis, Marcus},
  date = {2021-07-01},
  number = {arXiv:2107.00567},
  eprint = {2107.00567},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2107.00567},
  urldate = {2022-06-13},
  abstract = {The hippocampal formation is thought to learn spatial maps of environments, and in many models this learning process consists of forming a sensory association for each location in the environment. This is inefficient, akin to learning a large lookup table for each environment. Spatial maps can be learned much more efficiently if the maps instead consist of arrangements of sparse environment parts. In this work, I approach spatial mapping as a problem of learning graphs of environment parts. Each node in the learned graph, represented by hippocampal engram cells, is associated with feature information in lateral entorhinal cortex (LEC) and location information in medial entorhinal cortex (MEC) using empirically observed neuron types. Each edge in the graph represents the relation between two parts, and it is associated with coarse displacement information. This core idea of associating arbitrary information with nodes and edges is not inherently spatial, so this proposed fast-relation-graph-learning algorithm can expand to incorporate many spatial and non-spatial tasks.},
  file = {/Users/lukakuma/Zotero/storage/TPPRWLE6/Lewis - 2021 - Hippocampal Spatial Mapping As Fast Graph Learning.pdf;/Users/lukakuma/Zotero/storage/S54NWDTA/2107.html}
}

@article{lewisLocationsNeocortexTheory2019,
  title = {Locations in the {{Neocortex}}: {{A Theory}} of {{Sensorimotor Object Recognition Using Cortical Grid Cells}}},
  shorttitle = {Locations in the {{Neocortex}}},
  author = {Lewis, Marcus and Purdy, Scott and Ahmad, Subutai and Hawkins, Jeff},
  date = {2019},
  journaltitle = {Frontiers in Neural Circuits},
  volume = {13},
  issn = {1662-5110},
  url = {https://www.frontiersin.org/article/10.3389/fncir.2019.00022},
  urldate = {2022-06-18},
  abstract = {The neocortex is capable of anticipating the sensory results of movement but the neural mechanisms are poorly understood. In the entorhinal cortex, grid cells represent the location of an animal in its environment, and this location is updated through movement and path integration. In this paper, we propose that sensory neocortex incorporates movement using grid cell-like neurons that represent the location of sensors on an object. We describe a two-layer neural network model that uses cortical grid cells and path integration to robustly learn and recognize objects through movement and predict sensory stimuli after movement. A layer of cells consisting of several grid cell-like modules represents a location in the reference frame of a specific object. Another layer of cells which processes sensory input receives this location input as context and uses it to encode the sensory input in the object's reference frame. Sensory input causes the network to invoke previously learned locations that are consistent with the input, and motor input causes the network to update those locations. Simulations show that the model can learn hundreds of objects even when object features alone are insufficient for disambiguation. We discuss the relationship of the model to cortical circuitry and suggest that the reciprocal connections between layers 4 and 6 fit the requirements of the model. We propose that the subgranular layers of cortical columns employ grid cell-like mechanisms to represent object specific locations that are updated through movement.},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/9FEKLP8N/Lewis et al. - 2019 - Locations in the Neocortex A Theory of Sensorimot.pdf}
}

@inproceedings{lewisRetrievalAugmentedGenerationKnowledgeIntensive2020,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"uttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"aschel, Tim and Riedel, Sebastian and Kiela, Douwe},
  date = {2020},
  volume = {33},
  pages = {9459--9474},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
  urldate = {2022-12-05},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/C6KRHFRQ/Lewis et al. - 2020 - Retrieval-Augmented Generation for Knowledge-Inten.pdf}
}

@online{lewkowyczSolvingQuantitativeReasoning2022a,
  title = {Solving {{Quantitative Reasoning Problems}} with {{Language Models}}},
  author = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and Wu, Yuhuai and Neyshabur, Behnam and Gur-Ari, Guy and Misra, Vedant},
  date = {2022-06-30},
  number = {arXiv:2206.14858},
  eprint = {arXiv:2206.14858},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.14858},
  urldate = {2022-11-09},
  abstract = {Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow,formal reasoning,read},
  file = {/Users/lukakuma/Zotero/storage/V8JZ7NBK/Lewkowycz et al. - 2022 - Solving Quantitative Reasoning Problems with Langu.pdf;/Users/lukakuma/Zotero/storage/7UFYH362/2206.html}
}

@video{lexfridmanAndrejKarpathyTesla2022,
  title = {Andrej {{Karpathy}}: {{Tesla AI}}, {{Self-Driving}}, {{Optimus}}, {{Aliens}}, and {{AGI}} | {{Lex Fridman Podcast}} \#333},
  shorttitle = {Andrej {{Karpathy}}},
  editor = {{Lex Fridman}},
  date = {2022-10-30},
  url = {https://www.youtube.com/watch?v=cdiD-9MMpb0},
  urldate = {2022-11-07},
  editortype = {director}
}

@video{lexfridmanDavidSilverAlphaGo2020,
  title = {David {{Silver}}: {{AlphaGo}}, {{AlphaZero}}, and {{Deep Reinforcement Learning}} | {{Lex Fridman Podcast}} \#86},
  shorttitle = {David {{Silver}}},
  editor = {{Lex Fridman}},
  date = {2020-04-04},
  url = {https://www.youtube.com/watch?v=uPUEq8d73JI},
  urldate = {2022-06-14},
  abstract = {David Silver leads the reinforcement learning research group at DeepMind and was lead researcher on AlphaGo, AlphaZero and co-lead on AlphaStar, and MuZero and lot of important work in reinforcement learning. Support this podcast by signing up with these sponsors: - MasterClass: https://masterclass.com/lex - Cash App - use code "LexPodcast" and download: - Cash App (App Store): https://apple.co/2sPrUHe - Cash App (Google Play): https://bit.ly/2MlvP5w EPISODE LINKS: Reinforcement learning (book): https://amzn.to/2Jwp5zG PODCAST INFO: Podcast website: https://lexfridman.com/podcast Apple Podcasts: https://apple.co/2lwqZIr Spotify: https://spoti.fi/2nEwCF8 RSS: https://lexfridman.com/feed/podcast/ Full episodes playlist: https://www.youtube.com/playlist?list... Clips playlist: https://www.youtube.com/playlist?list... OUTLINE: 0:00 - Introduction 4:09 - First program 11:11 - AlphaGo 21:42 - Rule of the game of Go 25:37 - Reinforcement learning: personal journey 30:15 - What is reinforcement learning? 43:51 - AlphaGo (continued) 53:40 - Supervised learning and self play in AlphaGo 1:06:12 - Lee Sedol retirement from Go play 1:08:57 - Garry Kasparov 1:14:10 - Alpha Zero and self play 1:31:29 - Creativity in AlphaZero 1:35:21 - AlphaZero applications 1:37:59 - Reward functions 1:40:51 - Meaning of life CONNECT: - Subscribe to this YouTube channel - Twitter: https://twitter.com/lexfridman - LinkedIn: https://www.linkedin.com/in/lexfridman - Facebook: https://www.facebook.com/LexFridmanPage - Instagram: https://www.instagram.com/lexfridman - Medium: https://medium.com/@lexfridman - Support on Patreon: https://www.patreon.com/lexfridman},
  editortype = {director}
}

@video{lexfridmanDemisHassabisDeepMind2022,
  title = {Demis {{Hassabis}}: {{DeepMind}} | {{Lex Fridman Podcast}} \#299},
  shorttitle = {Demis {{Hassabis}}},
  editor = {{Lex Fridman}},
  date = {2022-07-01},
  url = {https://www.youtube.com/watch?v=Gfr50f6ZBvo},
  urldate = {2022-07-02},
  abstract = {Demis Hassabis is the CEO and co-founder of DeepMind. Please support this podcast by checking out our sponsors: - Mailgun: https://lexfridman.com/mailgun - InsideTracker: https://insidetracker.com/lex to get 20\% off - Onnit: https://lexfridman.com/onnit to get up to 10\% off - Indeed: https://indeed.com/lex to get \$75 credit - Magic Spoon: https://magicspoon.com/lex and use code LEX to get \$5 off EPISODE LINKS: Demis's Twitter: https://twitter.com/demishassabis DeepMind's Twitter: https://twitter.com/DeepMind DeepMind's Instagram: https://instagram.com/deepmind DeepMind's Website: https://deepmind.com Plasma control paper: https://nature.com/articles/s41586-02... Quantum simulation paper: https://science.org/doi/10.1126/scien... The Emperor's New Mind (book): https://amzn.to/3bx03lo Life Ascending (book): https://amzn.to/3AhUP7z PODCAST INFO: Podcast website: https://lexfridman.com/podcast Apple Podcasts: https://apple.co/2lwqZIr Spotify: https://spoti.fi/2nEwCF8 RSS: https://lexfridman.com/feed/podcast/ Full episodes playlist: https://www.youtube.com/playlist?list... Clips playlist: https://www.youtube.com/playlist?list... OUTLINE: 0:00 - Introduction 1:01 - Turing Test 8:27 - Video games 30:02 - Simulation 32:13 - Consciousness 37:13 - AlphaFold 50:53 - Solving intelligence 1:03:12 - Open sourcing AlphaFold \& MuJoCo 1:13:18 - Nuclear fusion 1:17:22 - Quantum simulation 1:20:30 - Physics 1:23:57 - Origin of life 1:28:36 - Aliens 1:36:43 - Intelligent life 1:39:52 - Conscious AI 1:53:07 - Power 1:57:37 - Advice for young people 2:05:43 - Meaning of life SOCIAL: - Twitter: https://twitter.com/lexfridman - LinkedIn: https://www.linkedin.com/in/lexfridman - Facebook: https://www.facebook.com/lexfridman - Instagram: https://www.instagram.com/lexfridman - Medium: https://medium.com/@lexfridman - Reddit: https://reddit.com/r/lexfridman - Support on Patreon: https://www.patreon.com/lexfridman},
  editortype = {director},
  keywords = {DeepMind}
}

@video{lexfridmanIlyaSutskeverDeep2020,
  title = {Ilya {{Sutskever}}: {{Deep Learning}} | {{Lex Fridman Podcast}} \#94},
  shorttitle = {Ilya {{Sutskever}}},
  editor = {{Lex Fridman}},
  date = {2020-05-09},
  url = {https://www.youtube.com/watch?v=13CZPWmke6A},
  urldate = {2022-06-12},
  abstract = {Ilya Sutskever is the co-founder of OpenAI, is one of the most cited computer scientist in history with over 165,000 citations, and to me, is one of the most brilliant and insightful minds ever in the field of deep learning. There are very few people in this world who I would rather talk to and brainstorm with about deep learning, intelligence, and life than Ilya, on and off the mic. Support this podcast by signing up with these sponsors: - Cash App - use code "LexPodcast" and download: - Cash App (App Store): https://apple.co/2sPrUHe - Cash App (Google Play): https://bit.ly/2MlvP5w EPISODE LINKS: Ilya's Twitter: https://twitter.com/ilyasut Ilya's Website: https://www.cs.toronto.edu/\textasciitilde ilya/ PODCAST INFO: Podcast website: https://lexfridman.com/podcast Apple Podcasts: https://apple.co/2lwqZIr Spotify: https://spoti.fi/2nEwCF8 RSS: https://lexfridman.com/feed/podcast/ Full episodes playlist: https://www.youtube.com/playlist?list... Clips playlist: https://www.youtube.com/playlist?list... OUTLINE: 0:00 - Introduction 2:23 - AlexNet paper and the ImageNet moment 8:33 - Cost functions 13:39 - Recurrent neural networks 16:19 - Key ideas that led to success of deep learning 19:57 - What's harder to solve: language or vision? 29:35 - We're massively underestimating deep learning 36:04 - Deep double descent 41:20 - Backpropagation 42:42 - Can neural networks be made to reason? 50:35 - Long-term memory 56:37 - Language models 1:00:35 - GPT-2 1:07:14 - Active learning 1:08:52 - Staged release of AI systems 1:13:41 - How to build AGI? 1:25:00 - Question to AGI 1:32:07 - Meaning of life CONNECT: - Subscribe to this YouTube channel - Twitter: https://twitter.com/lexfridman - LinkedIn: https://www.linkedin.com/in/lexfridman - Facebook: https://www.facebook.com/LexFridmanPage - Instagram: https://www.instagram.com/lexfridman - Medium: https://medium.com/@lexfridman - Support on Patreon: https://www.patreon.com/lexfridman},
  editortype = {director},
  keywords = {read}
}

@video{lexfridmanJohnCarmackDoom2022,
  title = {John {{Carmack}}: {{Doom}}, {{Quake}}, {{VR}}, {{AGI}}, {{Programming}}, {{Video Games}}, and {{Rockets}} | {{Lex Fridman Podcast}} \#309},
  shorttitle = {John {{Carmack}}},
  editor = {{Lex Fridman}},
  date = {2022-08-05},
  url = {https://www.youtube.com/watch?v=I845O57ZSy4},
  urldate = {2022-08-12},
  editortype = {director}
}

@video{lexfridmanOriolVinyalsDeep2022,
  title = {Oriol {{Vinyals}}: {{Deep Learning}} and {{Artificial General Intelligence}} | {{Lex Fridman Podcast}} \#306},
  shorttitle = {Oriol {{Vinyals}}},
  editor = {{Lex Fridman}},
  date = {2022-07-27},
  url = {https://www.youtube.com/watch?v=aGBLRlLe7X8},
  urldate = {2022-07-28},
  editortype = {director},
  keywords = {DeepMind}
}

@video{lexfridmanRichardWolffMarxism2022,
  title = {Richard {{Wolff}}: {{Marxism}} and {{Communism}} | {{Lex Fridman Podcast}} \#295},
  shorttitle = {Richard {{Wolff}}},
  editor = {{Lex Fridman}},
  date = {2022-06-18},
  url = {https://www.youtube.com/watch?v=o0Bi-q89j5Y},
  urldate = {2022-07-05},
  abstract = {Richard Wolff is a Marxist philosopher and economist. Please support this podcast by checking out our sponsors: - Skiff: https://skiff.org/lex to get early access - Indeed: https://indeed.com/lex to get \$75 credit - Onnit: https://lexfridman.com/onnit to get up to 10\% off - Blinkist: https://blinkist.com/lex and use code LEX to get 25\% off premium - Linode: https://linode.com/lex to get \$100 free credit EPISODE LINKS: Richard's Twitter: https://twitter.com/profwolff Richard's Website: https://www.rdwolff.com Contending Economic Theories (book): https://amzn.to/3HykPwT Understanding Marxism (book): https://amzn.to/39qpm8b Understanding Socialism (book): https://amzn.to/3Og9XG3 PODCAST INFO: Podcast website: https://lexfridman.com/podcast Apple Podcasts: https://apple.co/2lwqZIr Spotify: https://spoti.fi/2nEwCF8 RSS: https://lexfridman.com/feed/podcast/ Full episodes playlist: https://www.youtube.com/playlist?list... Clips playlist: https://www.youtube.com/playlist?list... OUTLINE: 0:00 - Introduction 1:52 - Marxism 10:21 - Communism 45:27 - Human nature 57:43 - Economics 1:04:34 - Capitalism 1:36:58 - Governments and corporations 1:47:53 - Stalinism 2:01:52 - Nazis 2:08:48 - Socialism vs Marxism 2:16:28 - Bernie Sanders and AOC 2:33:29 - Cultural Marxism 2:40:28 - Darkest moments 2:45:58 - Advice for young people 2:48:17 - Mortality 2:52:08 - Meaning of life SOCIAL: - Twitter: https://twitter.com/lexfridman - LinkedIn: https://www.linkedin.com/in/lexfridman - Facebook: https://www.facebook.com/lexfridman - Instagram: https://www.instagram.com/lexfridman - Medium: https://medium.com/@lexfridman - Reddit: https://reddit.com/r/lexfridman - Support on Patreon: https://www.patreon.com/lexfridman},
  editortype = {director}
}

@online{liangCodePoliciesLanguage2022a,
  title = {Code as {{Policies}}: {{Language Model Programs}} for {{Embodied Control}}},
  shorttitle = {Code as {{Policies}}},
  author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  date = {2022-09-19},
  number = {arXiv:2209.07753},
  eprint = {arXiv:2209.07753},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.07753},
  urldate = {2023-02-20},
  abstract = {Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formalization of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  pubstate = {preprint},
  keywords = {action/physical,code},
  file = {/Users/lukakuma/Zotero/storage/Q4CY83RM/Liang et al. - 2022 - Code as Policies Language Model Programs for Embo.pdf;/Users/lukakuma/Zotero/storage/V7QJ5M2H/2209.html}
}

@online{liangHolisticEvaluationLanguage2022,
  title = {Holistic {{Evaluation}} of {{Language Models}}},
  author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and R\'e, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
  date = {2022-11-16},
  number = {arXiv:2211.09110},
  eprint = {arXiv:2211.09110},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.09110},
  urldate = {2022-11-18},
  abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/25FRFMI7/Liang et al. - 2022 - Holistic Evaluation of Language Models.pdf}
}

@online{liBLIP2BootstrappingLanguageImage2023,
  title = {{{BLIP-2}}: {{Bootstrapping Language-Image Pre-training}} with {{Frozen Image Encoders}} and {{Large Language Models}}},
  shorttitle = {{{BLIP-2}}},
  author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  date = {2023-01-29},
  number = {arXiv:2301.12597},
  eprint = {arXiv:2301.12597},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.12597},
  urldate = {2023-02-07},
  abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
  pubstate = {preprint},
  keywords = {2-adapter,7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/9KI7CUC7/Li et al. - 2023 - BLIP-2 Bootstrapping Language-Image Pre-training .pdf;/Users/lukakuma/Zotero/storage/63U3R6AZ/2301.html}
}

@unpublished{liBLIPBootstrappingLanguageImage2022,
  title = {{{BLIP}}: {{Bootstrapping Language-Image Pre-training}} for {{Unified Vision-Language Understanding}} and {{Generation}}},
  shorttitle = {{{BLIP}}},
  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  date = {2022-02-15},
  eprint = {2201.12086},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.12086},
  urldate = {2022-03-31},
  abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7\% in average recall@1), image captioning (+2.8\% in CIDEr), and VQA (+1.6\% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.},
  file = {/Users/lukakuma/Zotero/storage/VFN7PLC3/Li et al. - 2022 - BLIP Bootstrapping Language-Image Pre-training fo.pdf;/Users/lukakuma/Zotero/storage/8Y33ZD32/2201.html}
}

@online{liBranchTrainMergeEmbarrassinglyParallel2022,
  title = {Branch-{{Train-Merge}}: {{Embarrassingly Parallel Training}} of {{Expert Language Models}}},
  shorttitle = {Branch-{{Train-Merge}}},
  author = {Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
  date = {2022-08-05},
  number = {arXiv:2208.03306},
  eprint = {arXiv:2208.03306},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.03306},
  urldate = {2022-10-18},
  abstract = {We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.},
  pubstate = {preprint},
  keywords = {model fusion,sparsity},
  file = {/Users/lukakuma/Zotero/storage/7K35A46J/Li et al. - 2022 - Branch-Train-Merge Embarrassingly Parallel Traini.pdf;/Users/lukakuma/Zotero/storage/P2FUVXAX/2208.html}
}

@online{liCompetitionLevelCodeGeneration2022a,
  title = {Competition-{{Level Code Generation}} with {{AlphaCode}}},
  author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R\'emi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and Hubert, Thomas and Choy, Peter and family=Autume, given=Cyprien de Masson, prefix=d', useprefix=true and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and family=Freitas, given=Nando, prefix=de, useprefix=true and Kavukcuoglu, Koray and Vinyals, Oriol},
  date = {2022-02-08},
  number = {arXiv:2203.07814},
  eprint = {arXiv:2203.07814},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.07814},
  urldate = {2022-10-31},
  abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3\% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
  pubstate = {preprint},
  keywords = {code,DeepMind},
  file = {/Users/lukakuma/Zotero/storage/4HREEXIC/Li et al. - 2022 - Competition-Level Code Generation with AlphaCode.pdf;/Users/lukakuma/Zotero/storage/WP9X9ATU/2203.html}
}

@online{liConvMLPHierarchicalConvolutional2021,
  title = {{{ConvMLP}}: {{Hierarchical Convolutional MLPs}} for {{Vision}}},
  shorttitle = {{{ConvMLP}}},
  author = {Li, Jiachen and Hassani, Ali and Walton, Steven and Shi, Humphrey},
  date = {2021-09-18},
  number = {arXiv:2109.04454},
  eprint = {arXiv:2109.04454},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.04454},
  urldate = {2022-06-09},
  abstract = {MLP-based architectures, which consist of a sequence of consecutive multi-layer perceptron blocks, have recently been found to reach comparable results to convolutional and transformer-based methods. However, most adopt spatial MLPs which take fixed dimension inputs, therefore making it difficult to apply them to downstream tasks, such as object detection and semantic segmentation. Moreover, single-stage designs further limit performance in other computer vision tasks and fully connected layers bear heavy computation. To tackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for visual recognition, which is a light-weight, stage-wise, co-design of convolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8\% top-1 accuracy on ImageNet-1k with 9M parameters and 2.4G MACs (15\% and 19\% of MLP-Mixer-B/16, respectively). Experiments on object detection and semantic segmentation further show that visual representation learned by ConvMLP can be seamlessly transferred and achieve competitive results with fewer parameters. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Convolutional-MLPs.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/SWE5U6LD/Li et al. - 2021 - ConvMLP Hierarchical Convolutional MLPs for Visio.pdf}
}

@article{liDeepReinforcementLearning2017,
  title = {Deep {{Reinforcement Learning}}: {{An Overview}}},
  shorttitle = {Deep {{Reinforcement Learning}}},
  author = {Li, Yuxi},
  date = {2017},
  journaltitle = {ArXiv},
  abstract = {This work discusses core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration, and important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.  Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.},
  file = {/Users/lukakuma/Zotero/storage/MLZRFDZW/Li - 2017 - Deep Reinforcement Learning An Overview.pdf}
}

@online{liDiffusionLMImprovesControllable2022a,
  title = {Diffusion-{{LM Improves Controllable Text Generation}}},
  author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
  date = {2022-05-27},
  number = {arXiv:2205.14217},
  eprint = {arXiv:2205.14217},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.14217},
  urldate = {2023-02-25},
  abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
  pubstate = {preprint},
  keywords = {*diffusion},
  file = {/Users/lukakuma/Zotero/storage/3FFXMCJQ/Li et al. - 2022 - Diffusion-LM Improves Controllable Text Generation.pdf;/Users/lukakuma/Zotero/storage/7TSY9PUQ/2205.html}
}

@online{liLargeLanguageModels2022,
  title = {Large {{Language Models Can Be Strong Differentially Private Learners}}},
  author = {Li, Xuechen and Tram\`er, Florian and Liang, Percy and Hashimoto, Tatsunori},
  date = {2022-10-12},
  number = {arXiv:2110.05679},
  eprint = {arXiv:2110.05679},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.05679},
  urldate = {2022-11-02},
  abstract = {Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) finetuning objectives which are aligned with the pretraining procedure. With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines\textemdash by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models doesn't tend to suffer from dimension-dependent performance degradation. Code to reproduce results can be found at https: //github.com/lxuechen/private-transformers.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/PKV7YFYD/Li et al. - 2022 - Large Language Models Can Be Strong Differentially.pdf}
}

@online{liLargeLanguageModels2022a,
  title = {Large {{Language Models}} with {{Controllable Working Memory}}},
  author = {Li, Daliang and Rawat, Ankit Singh and Zaheer, Manzil and Wang, Xin and Lukasik, Michal and Veit, Andreas and Yu, Felix and Kumar, Sanjiv},
  date = {2022-11-09},
  number = {arXiv:2211.05110},
  eprint = {arXiv:2211.05110},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.05110},
  urldate = {2022-11-10},
  abstract = {Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), owing to their excellent understanding and generation abilities. Remarkably, what further sets these models apart is the massive amounts of world knowledge they internalize during pretraining. While many downstream applications provide the model with an informational context to aid its performance on the underlying task, how the model's world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model's memorized knowledge. This enables model predictions to be grounded in the context, which can then be used to update or correct specific model predictions without frequent retraining. By contrast, when the context is irrelevant to the task, the model should ignore it and fall back on its internal knowledge. In this paper, we undertake a first joint study of the aforementioned two properties, namely controllability and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned) could exhibit poor controllability and robustness, which do not scale with increasing model size. As a solution, we propose a novel method - Knowledge Aware FineTuning (KAFT) - to strengthen both controllability and robustness by incorporating counterfactual and irrelevant contexts to standard supervised datasets. Our comprehensive evaluation showcases the utility of KAFT across model architectures and sizes.},
  pubstate = {preprint},
  keywords = {memory},
  file = {/Users/lukakuma/Zotero/storage/PBSLIDS4/Li et al. - 2022 - Large Language Models with Controllable Working Me.pdf;/Users/lukakuma/Zotero/storage/KDRVFCMM/2211.html}
}

@online{liLEMMABootstrappingHighLevel2022,
  title = {{{LEMMA}}: {{Bootstrapping High-Level Mathematical Reasoning}} with {{Learned Symbolic Abstractions}}},
  shorttitle = {{{LEMMA}}},
  author = {Li, Zhening and Poesia, Gabriel and Costilla-Reyes, Omar and Goodman, Noah and Solar-Lezama, Armando},
  date = {2022-11-15},
  number = {arXiv:2211.08671},
  eprint = {arXiv:2211.08671},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.08671},
  urldate = {2022-11-23},
  abstract = {Humans tame the complexity of mathematical reasoning by developing hierarchies of abstractions. With proper abstractions, solutions to hard problems can be expressed concisely, thus making them more likely to be found. In this paper, we propose Learning Mathematical Abstractions (LEMMA): an algorithm that implements this idea for reinforcement learning agents in mathematical domains. LEMMA augments Expert Iteration with an abstraction step, where solutions found so far are revisited and rewritten in terms of new higher-level actions, which then become available to solve new problems. We evaluate LEMMA on two mathematical reasoning tasks--equation solving and fraction simplification--in a step-by-step fashion. In these two domains, LEMMA improves the ability of an existing agent, both solving more problems and generalizing more effectively to harder problems than those seen during training.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/LDESUMIN/Li et al. - 2022 - LEMMA Bootstrapping High-Level Mathematical Reaso.pdf;/Users/lukakuma/Zotero/storage/XJXZW8DG/2211.html}
}

@article{lillicrapBackpropagationBrain2020,
  title = {Backpropagation and the Brain},
  author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
  date = {2020-06},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {21},
  number = {6},
  pages = {335--346},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  url = {https://www.nature.com/articles/s41583-020-0277-3},
  urldate = {2022-06-18},
  abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
  issue = {6},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/MSF3A68W/Lillicrap et al. - 2020 - Backpropagation and the brain.pdf;/Users/lukakuma/Zotero/storage/ALN5WJQA/s41583-020-0277-3.html}
}

@online{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  date = {2019-07-05},
  number = {arXiv:1509.02971},
  eprint = {arXiv:1509.02971},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1509.02971},
  urldate = {2022-07-19},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/M8P2DJQQ/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf;/Users/lukakuma/Zotero/storage/RBNB329W/1509.html}
}

@article{linADAPTVisionLanguageNavigation2022,
  title = {{{ADAPT}}: {{Vision-Language Navigation}} with {{Modality-Aligned Action Prompts}}},
  author = {Lin, Bingqian and Zhu, Yi and Chen, Zicong and Liang, Xiwen and Liu, Jianzhuang and Liang, Xiaodan},
  date = {2022},
  pages = {7},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/66CJAUQB/Lin et al. - ADAPT Vision-Language Navigation with Modality-Al.pdf}
}

@online{lindnerTracrCompiledTransformers2023,
  title = {Tracr: {{Compiled Transformers}} as a {{Laboratory}} for {{Interpretability}}},
  shorttitle = {Tracr},
  author = {Lindner, David and Kram\'ar, J\'anos and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
  date = {2023-01-12},
  number = {arXiv:2301.05062},
  eprint = {arXiv:2301.05062},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.05062},
  urldate = {2023-01-13},
  abstract = {Interpretability research aims to build tools for understanding machine learning (ML) models. However, such tools are inherently hard to evaluate because we do not have ground truth information about how ML models actually work. In this work, we propose to build transformer models manually as a testbed for interpretability research. We introduce Tracr, a "compiler" for translating human-readable programs into weights of a transformer model. Tracr takes code written in RASP, a domain-specific language (Weiss et al. 2021), and translates it into weights for a standard, decoder-only, GPT-like transformer architecture. We use Tracr to create a range of ground truth transformers that implement programs including computing token frequencies, sorting, and Dyck-n parenthesis checking, among others. To enable the broader research community to explore and use compiled models, we provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.},
  pubstate = {preprint},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/58BW6F8B/Lindner et al. - 2023 - Tracr Compiled Transformers as a Laboratory for I.pdf;/Users/lukakuma/Zotero/storage/GNGAIBUT/2301.html}
}

@online{linHowTrainYour2023,
  title = {How to {{Train Your DRAGON}}: {{Diverse Augmentation Towards Generalizable Dense Retrieval}}},
  shorttitle = {How to {{Train Your DRAGON}}},
  author = {Lin, Sheng-Chieh and Asai, Akari and Li, Minghan and Oguz, Barlas and Lin, Jimmy and Mehdad, Yashar and Yih, Wen-tau and Chen, Xilun},
  date = {2023-02-14},
  number = {arXiv:2302.07452},
  eprint = {arXiv:2302.07452},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.07452},
  urldate = {2023-02-25},
  abstract = {Various techniques have been developed in recent years to improve dense retrieval (DR), such as unsupervised contrastive learning and pseudo-query generation. Existing DRs, however, often suffer from effectiveness tradeoffs between supervised and zero-shot retrieval, which some argue was due to the limited model capacity. We contradict this hypothesis and show that a generalizable DR can be trained to achieve high accuracy in both supervised and zero-shot retrieval without increasing model size. In particular, we systematically examine the contrastive learning of DRs, under the framework of Data Augmentation (DA). Our study shows that common DA practices such as query augmentation with generative models and pseudo-relevance label creation using a cross-encoder, are often inefficient and sub-optimal. We hence propose a new DA approach with diverse queries and sources of supervision to progressively train a generalizable DR. As a result, DRAGON, our dense retriever trained with diverse augmentation, is the first BERT-base-sized DR to achieve state-of-the-art effectiveness in both supervised and zero-shot evaluations and even competes with models using more complex late interaction (ColBERTv2 and SPLADE++).},
  pubstate = {preprint},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/7Y99Q22R/Lin et al. - 2023 - How to Train Your DRAGON Diverse Augmentation Tow.pdf;/Users/lukakuma/Zotero/storage/MIYQRH9Y/2302.html}
}

@online{linLanguageModelsProtein2022,
  title = {Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction},
  author = {Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Costa, Allan dos Santos and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Sal and Rives, Alexander},
  date = {2022-07-21},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2022.07.20.500902},
  url = {https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1},
  urldate = {2022-07-21},
  abstract = {Large language models have recently been shown to develop emergent capabilities with scale, going beyond simple pattern matching to perform higher level reasoning and generate lifelike images and text. While language models trained on protein sequences have been studied at a smaller scale, little is known about what they learn about biology as they are scaled up. In this work we train models up to 15 billion parameters, the largest language models of proteins to be evaluated to date. We find that as models are scaled they learn information enabling the prediction of the three-dimensional structure of a protein at the resolution of individual atoms. We present ESMFold for high accuracy end-to-end atomic level structure prediction directly from the individual sequence of a protein. ESMFold has similar accuracy to AlphaFold2 and RoseTTAFold for sequences with low perplexity that are well understood by the language model. ESMFold inference is an order of magnitude faster than AlphaFold2, enabling exploration of the structural space of metagenomic proteins in practical timescales.},
  langid = {english},
  pubstate = {preprint},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/7FI9V7HM/Lin et al. - 2022 - Language models of protein sequences at the scale .pdf;/Users/lukakuma/Zotero/storage/TC7FDPFK/2022.07.20.html}
}

@online{linTruthfulQAMeasuringHow2022,
  title = {{{TruthfulQA}}: {{Measuring How Models Mimic Human Falsehoods}}},
  shorttitle = {{{TruthfulQA}}},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  date = {2022-05-07},
  number = {arXiv:2109.07958},
  eprint = {arXiv:2109.07958},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.07958},
  urldate = {2022-12-08},
  abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/XABY7QSD/Lin et al. - 2022 - TruthfulQA Measuring How Models Mimic Human False.pdf;/Users/lukakuma/Zotero/storage/CFDP8LCX/2109.html}
}

@online{liPrefixTuningOptimizingContinuous2021,
  title = {Prefix-{{Tuning}}: {{Optimizing Continuous Prompts}} for {{Generation}}},
  shorttitle = {Prefix-{{Tuning}}},
  author = {Li, Xiang Lisa and Liang, Percy},
  date = {2021-01-01},
  number = {arXiv:2101.00190},
  eprint = {arXiv:2101.00190},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.00190},
  urldate = {2022-11-02},
  abstract = {Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were ``virtual tokens''. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.},
  langid = {english},
  pubstate = {preprint},
  keywords = {*peft,3-prefix tuning},
  file = {/Users/lukakuma/Zotero/storage/KNVQGHKW/Li and Liang - 2021 - Prefix-Tuning Optimizing Continuous Prompts for G.pdf}
}

@online{liPreTrainedLanguageModels2022a,
  title = {Pre-{{Trained Language Models}} for {{Interactive Decision-Making}}},
  author = {Li, Shuang and Puig, Xavier and Paxton, Chris and Du, Yilun and Wang, Clinton and Fan, Linxi and Chen, Tao and Huang, De-An and Aky\"urek, Ekin and Anandkumar, Anima and Andreas, Jacob and Mordatch, Igor and Torralba, Antonio and Zhu, Yuke},
  date = {2022-10-29},
  number = {arXiv:2202.01771},
  eprint = {arXiv:2202.01771},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.01771},
  urldate = {2023-02-20},
  abstract = {Language model (LM) pre-training is useful in many language processing tasks. But can pre-trained LMs be further leveraged for more general machine learning problems? We propose an approach for using LMs to scaffold learning and generalization in general sequential decision-making problems. In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action. We demonstrate that this framework enables effective combinatorial generalization across different environments and supervisory modalities. We begin by assuming access to a set of expert demonstrations, and show that initializing policies with LMs and fine-tuning them via behavior cloning improves task completion rates by 43.6\% in the VirtualHome environment. Next, we integrate an active data gathering procedure in which agents iteratively interact with the environment, relabel past "failed" experiences with new goals, and update their policies in a self-supervised loop. Active data gathering further improves combinatorial generalization, outperforming the best baseline by 25.1\%. Finally, we explain these results by investigating three possible factors underlying the effectiveness of the LM-based policy. We find that sequential input representations (vs. fixed-dimensional feature vectors) and LM-based weight initialization are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans; these representations can aid learning and generalization even outside of language processing.},
  pubstate = {preprint},
  keywords = {action/digital},
  file = {/Users/lukakuma/Zotero/storage/GJRQRQ5X/Li et al. - 2022 - Pre-Trained Language Models for Interactive Decisi.pdf;/Users/lukakuma/Zotero/storage/NY24XT5G/Li et al. - 2022 - Pre-Trained Language Models for Interactive Decisi.pdf;/Users/lukakuma/Zotero/storage/4QDXBVSH/2202.html;/Users/lukakuma/Zotero/storage/7EPWJBYY/2202.html}
}

@unpublished{liReinforcementLearningPractice2022,
  title = {Reinforcement {{Learning}} in {{Practice}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Reinforcement {{Learning}} in {{Practice}}},
  author = {Li, Yuxi},
  date = {2022-04-22},
  number = {arXiv:2202.11296},
  eprint = {2202.11296},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.11296},
  urldate = {2022-06-09},
  abstract = {This article is a gentle discussion about the field of reinforcement learning in practice, about opportunities and challenges, touching a broad range of topics, with perspectives and without technical details. The article is based on both historical and recent research papers, surveys, tutorials, talks, blogs, books, (panel) discussions, and workshops/conferences. Various groups of readers, like researchers, engineers, students, managers, investors, officers, and people wanting to know more about the field, may find the article interesting. In this article, we first give a brief introduction to reinforcement learning (RL), and its relationship with deep learning, machine learning and AI. Then we discuss opportunities of RL, in particular, products and services, games, bandits, recommender systems, robotics, transportation, finance and economics, healthcare, education, combinatorial optimization, computer systems, and science and engineering. Then we discuss challenges, in particular, 1) foundation, 2) representation, 3) reward, 4) exploration, 5) model, simulation, planning, and benchmarks, 6) off-policy/offline learning, 7) learning to learn a.k.a. meta-learning, 8) explainability and interpretability, 9) constraints, 10) software development and deployment, 11) business perspectives, and 12) more challenges. We conclude with a discussion, attempting to answer: "Why has RL not been widely adopted in practice yet?" and "When is RL helpful?".},
  file = {/Users/lukakuma/Zotero/storage/U3KZ7866/Li - 2022 - Reinforcement Learning in Practice Opportunities .pdf;/Users/lukakuma/Zotero/storage/VF2PRUHK/2202.html}
}

@online{liSystematicInvestigationCommonsense2022,
  title = {A {{Systematic Investigation}} of {{Commonsense Knowledge}} in {{Large Language Models}}},
  author = {Li, Xiang Lorraine and Kuncoro, Adhiguna and Hoffmann, Jordan and family=Autume, given=Cyprien de Masson, prefix=d', useprefix=true and Blunsom, Phil and Nematzadeh, Aida},
  date = {2022-10-31},
  number = {arXiv:2111.00607},
  eprint = {arXiv:2111.00607},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.00607},
  urldate = {2022-12-17},
  abstract = {Language models (LMs) trained on large amounts of data have shown impressive performance on many NLP tasks under the zero-shot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge -- a critical component of many NLP applications. We conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation of large pre-trained LMs, where we: (i) carefully control for the LMs' ability to exploit potential surface cues and annotation artefacts, and (ii) account for variations in performance that arise from factors that are not related to commonsense knowledge. Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models or few-shot evaluation are insufficient to achieve human-level commonsense performance.},
  pubstate = {preprint},
  keywords = {knowledge},
  file = {/Users/lukakuma/Zotero/storage/U62CK54J/Li et al. - 2022 - A Systematic Investigation of Commonsense Knowledg.pdf;/Users/lukakuma/Zotero/storage/S3YWWVP3/2111.html}
}

@online{liTrainLargeThen2020,
  title = {Train {{Large}}, {{Then Compress}}: {{Rethinking Model Size}} for {{Efficient Training}} and {{Inference}} of {{Transformers}}},
  shorttitle = {Train {{Large}}, {{Then Compress}}},
  author = {Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joseph E.},
  date = {2020-06-22},
  number = {arXiv:2002.11794},
  eprint = {arXiv:2002.11794},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.11794},
  urldate = {2023-02-27},
  abstract = {Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/HNH8FKUA/Li et al. - 2020 - Train Large, Then Compress Rethinking Model Size .pdf;/Users/lukakuma/Zotero/storage/EYSRG62S/2002.html}
}

@inproceedings{liu*GeneratingWikipediaSummarizing2022,
  title = {Generating {{Wikipedia}} by {{Summarizing Long Sequences}}},
  author = {Liu*, Peter J. and Saleh*, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  date = {2022-02-10},
  url = {https://openreview.net/forum?id=Hyg0vbWC-},
  urldate = {2022-08-01},
  abstract = {We generate Wikipedia articles abstractively conditioned on source document text.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {summarization},
  file = {/Users/lukakuma/Zotero/storage/B3LPCKPN/Liu et al. - 2022 - Generating Wikipedia by Summarizing Long Sequences.pdf;/Users/lukakuma/Zotero/storage/T5WP38RU/forum.html}
}

@unpublished{liuDiscreteValuedNeuralCommunication2021,
  title = {Discrete-{{Valued Neural Communication}}},
  author = {Liu, Dianbo and Lamb, Alex and Kawaguchi, Kenji and Goyal, Anirudh and Sun, Chen and Mozer, Michael Curtis and Bengio, Yoshua},
  date = {2021-07-10},
  eprint = {2107.02367},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2107.02367},
  urldate = {2022-05-12},
  abstract = {Deep learning has advanced from fully connected architectures to structured models organized into components, e.g., the transformer composed of positional elements, modular architectures divided into slots, and graph neural nets made up of nodes. In structured models, an interesting question is how to conduct dynamic and possibly sparse communication among the separate components. Here, we explore the hypothesis that restricting the transmitted information among components to discrete representations is a beneficial bottleneck. The motivating intuition is human language in which communication occurs through discrete symbols. Even though individuals have different understandings of what a "cat" is based on their specific experiences, the shared discrete token makes it possible for communication among individuals to be unimpeded by individual differences in internal representation. To discretize the values of concepts dynamically communicated among specialist components, we extend the quantization mechanism from the Vector-Quantized Variational Autoencoder to multi-headed discretization with shared codebooks and use it for discrete-valued neural communication (DVNC). Our experiments show that DVNC substantially improves systematic generalization in a variety of architectures -- transformers, modular architectures, and graph neural networks. We also show that the DVNC is robust to the choice of hyperparameters, making the method very useful in practice. Moreover, we establish a theoretical justification of our discretization process, proving that it has the ability to increase noise robustness and reduce the underlying dimensionality of the model.},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/E6A7W8LA/Liu et al. - 2021 - Discrete-Valued Neural Communication.pdf;/Users/lukakuma/Zotero/storage/VNXGS6QX/2107.html}
}

@online{liuFewShotParameterEfficientFineTuning2022,
  title = {Few-{{Shot Parameter-Efficient Fine-Tuning}} Is {{Better}} and {{Cheaper}} than {{In-Context Learning}}},
  author = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
  date = {2022-08-26},
  number = {arXiv:2205.05638},
  eprint = {arXiv:2205.05638},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.05638},
  urldate = {2022-10-27},
  abstract = {Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)\$\^3\$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6\% absolute. All of the code used in our experiments is publicly available.},
  pubstate = {preprint},
  keywords = {*peft,2-adapter,read},
  file = {/Users/lukakuma/Zotero/storage/NQ576J5I/Liu et al. - 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better.pdf;/Users/lukakuma/Zotero/storage/2EWYR322/2205.html}
}

@online{liuGPTUnderstandsToo2021,
  title = {{{GPT Understands}}, {{Too}}},
  author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  date = {2021-03-18},
  number = {arXiv:2103.10385},
  eprint = {arXiv:2103.10385},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.10385},
  urldate = {2023-02-15},
  abstract = {While GPTs with traditional fine-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning -- which employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64\textbackslash\% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we find that P-tuning also improves BERTs' performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, P-tuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.},
  pubstate = {preprint},
  keywords = {*peft},
  file = {/Users/lukakuma/Zotero/storage/AJBECMDL/Liu et al. - 2021 - GPT Understands, Too.pdf;/Users/lukakuma/Zotero/storage/2YWJCZMS/2103.html}
}

@online{liuLanguageQuantizedAutoEncoders2023,
  title = {Language {{Quantized AutoEncoders}}: {{Towards Unsupervised Text-Image Alignment}}},
  shorttitle = {Language {{Quantized AutoEncoders}}},
  author = {Liu, Hao and Yan, Wilson and Abbeel, Pieter},
  date = {2023-02-03},
  number = {arXiv:2302.00902},
  eprint = {arXiv:2302.00902},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.00902},
  urldate = {2023-02-14},
  abstract = {Recent progress in scaling up large language models has shown impressive capabilities in performing few-shot learning across a wide range of text-based tasks. However, a key limitation is that these language models fundamentally lack visual perception - a crucial attribute needed to extend these models to be able to interact with the real world and solve vision tasks, such as in visual-question answering and robotics. Prior works have largely connected image to text through pretraining and/or fine-tuning on curated image-text datasets, which can be a costly and expensive process. In order to resolve this limitation, we propose a simple yet effective approach called Language-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to align text-image data in an unsupervised manner by leveraging pretrained language models (e.g., BERT, RoBERTa). Our main idea is to encode image as sequences of text tokens by directly quantizing image embeddings using a pretrained language codebook. We then apply random masking followed by a BERT model, and have the decoder reconstruct the original image from BERT predicted text token embeddings. By doing so, LQAE learns to represent similar images with similar clusters of text tokens, thereby aligning these two modalities without the use of aligned text-image pairs. This enables few-shot image classification with large language models (e.g., GPT-3) as well as linear classification of images based on BERT text features. To the best of our knowledge, our work is the first work that uses unaligned images for multimodal tasks by leveraging the power of pretrained language models.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/4QEMJHMW/Liu et al. - 2023 - Language Quantized AutoEncoders Towards Unsupervi.pdf;/Users/lukakuma/Zotero/storage/Y6J3HGM6/2302.html}
}

@inreference{LiuLangZheZhiGeXiaoShuo2022,
  title = {ÊµÅÊµ™ËÄÖ‰πãÊ≠å (Â∞èË™™)},
  booktitle = {Á∂≠Âü∫ÁôæÁßëÔºåËá™Áî±ÁöÑÁôæÁßëÂÖ®Êõ∏},
  date = {2022-03-08T07:11:38Z},
  url = {https://zh.wikipedia.org/w/index.php?title=%E6%B5%81%E6%B5%AA%E8%80%85%E4%B9%8B%E6%AD%8C_(%E5%B0%8F%E8%AA%AA)&oldid=70517780},
  urldate = {2022-03-21},
  abstract = {„ÄäÊµÅÊµ™ËÄÖ‰πãÊ≠å„ÄãÔºàSiddharthaÔºâÔºåÂèàË≠Ø„ÄäÊÇâÈÅîÂ§ö„ÄãÔºåÂæ∑ÂúãÂ∞èË™™ÂÆ∂Ëµ´Â°ûÂú®1922Âπ¥ÊâÄËëóÁöÑ‰∏âÈÉ®ÂºèÊñáÂ≠∏‰ΩúÂìÅÔºå‰πüÊòØÂÖ∂Á¨¨‰πùÈÉ®‰ΩúÂìÅÔºåÊèèÂØ´‰∏ª‰∫∫ÂÖ¨ÊÇâÈÅîÂ§öÂú®Âè§ËÄÅÁöÑÂç∞Â∫¶ËøΩÊ±Ç‰ªñËá™Â∑±ÁöÑ‰∏âÂÄãÈáçË¶ÅÊ≠∑Á®ãÁöÑÈÅéÁ®ã„ÄÇÂæûËá™Ë¶∫ÁöÑÁ¶ÅÊ¨≤‰∏ªÁæ©ÔºåÈÄöÈÅéËá™ÊàëÊîæÈÄêÂíåË¶ñË¶∫ÊÑüÂÆò‰∫´ÂèóÔºåÊúÄÁµÇÁü•Ë≠òÂèäÂíåÂπ≥ÊàêÁÇ∫‰ªñËøΩÊ±ÇÁîüÂëΩÁöÑÊúÄÁµÇÁõÆÊ®ô„ÄÇÁî®‰∏ÄÁ®ÆÁ∞°ÂñÆÔºåË©©ÊÑèÂåñÁöÑË™ûË®ÄÂØ´Â∞±„ÄÇËµ´Â°ûÂ∞áÊõ∏ÁöÑÁ¨¨‰∏ÄÁ´†ÁçªÁµ¶‰∫ÜÁæÖÊõºÁæÖËò≠ÔºåÁ¨¨‰∫åÁ´†ÁçªÁµ¶‰∫Ü‰ªñÁöÑË°®ÂÖÑÔºåÂ®ÅÂªâ{$\cdot$}Ë≤¢Âæ∑Áâπ„ÄÇ SiddharthaÊòØÁî±ÂÖ©ÂÄãÊ¢µË™û‰∏≠ÁöÑË©ûÁµÑÊàêÔºöSiddhaÔºàÊÑèÁÇ∫Â∑≤Áç≤ÂæóÔºâ+arthaÔºàÂ∞ãÊ±Ç‰πãÁâ©Ôºâ„ÄÇËÄåÈÄôÂÖ©ÂÄãË©ûÈÄ£Êé•Âú®‰∏ÄËµ∑ÁöÑÊÑèÊÄùÂç≥ÊôÇ„ÄåÂ∑≤ÊâæÂ∞ãÂà∞ÔºàÂ≠òÂú®ÁöÑÔºâÊÑèÁæ©‰πã‰∫∫„ÄçÊàñ„ÄåÂ∑≤ÂÆåÊàêÁõÆÊ®ô‰πã‰∫∫„Äç„ÄÅ„Äå‰øÆË°åÂúìÊªø‰πã‰∫∫„Äç„ÄÇ},
  langid = {chinese},
  annotation = {Page Version ID: 70517780},
  file = {/Users/lukakuma/Zotero/storage/RVQR3R7U/ÊµÅÊµ™ËÄÖ‰πãÊ≠å_(Â∞èË™™).html}
}

@unpublished{liuLearningFunctionalDistributional2022,
  title = {Learning {{Functional Distributional Semantics}} with {{Visual Data}}},
  author = {Liu, Yinhong and Emerson, Guy},
  date = {2022-04-22},
  eprint = {2204.10624},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.10624},
  urldate = {2022-05-12},
  abstract = {Functional Distributional Semantics is a recently proposed framework for learning distributional semantics that provides linguistic interpretability. It models the meaning of a word as a binary classifier rather than a numerical vector. In this work, we propose a method to train a Functional Distributional Semantics model with grounded visual data. We train it on the Visual Genome dataset, which is closer to the kind of data encountered in human language acquisition than a large text corpus. On four external evaluation datasets, our model outperforms previous work on learning semantics from Visual Genome.},
  file = {/Users/lukakuma/Zotero/storage/3Z55PBIT/Liu and Emerson - 2022 - Learning Functional Distributional Semantics with .pdf;/Users/lukakuma/Zotero/storage/ZC6WM4SZ/2204.html}
}

@online{liuMindEyeGrounded2022,
  title = {Mind's {{Eye}}: {{Grounded Language Model Reasoning}} through {{Simulation}}},
  shorttitle = {Mind's {{Eye}}},
  author = {Liu, Ruibo and Wei, Jason and Gu, Shixiang Shane and Wu, Te-Yen and Vosoughi, Soroush and Cui, Claire and Zhou, Denny and Dai, Andrew M.},
  date = {2022-10-11},
  number = {arXiv:2210.05359},
  eprint = {arXiv:2210.05359},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.05359},
  urldate = {2022-10-27},
  abstract = {Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9\% zero-shot, and 46.0\% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.},
  pubstate = {preprint},
  keywords = {*general tools,5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/CCI7VZRA/Liu et al. - 2022 - Mind's Eye Grounded Language Model Reasoning thro.pdf;/Users/lukakuma/Zotero/storage/K9YHI9GY/2210.html}
}

@inproceedings{liuNeuPLNeuralPopulation2021,
  title = {{{NeuPL}}: {{Neural Population Learning}}},
  shorttitle = {{{NeuPL}}},
  author = {Liu, Siqi and Marris, Luke and Hennes, Daniel and Merel, Josh and Heess, Nicolas and Graepel, Thore},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=MIX3fJkl_1&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2022%2FConference%2FAuthors%23your-submissions)},
  urldate = {2022-06-10},
  abstract = {Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/47HK9VH2/Liu et al. - 2021 - NeuPL Neural Population Learning.pdf;/Users/lukakuma/Zotero/storage/Y9JT9AXZ/forum.html}
}

@online{liuPayAttentionMLPs2021,
  title = {Pay {{Attention}} to {{MLPs}}},
  author = {Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.},
  date = {2021-06-01},
  number = {arXiv:2105.08050},
  eprint = {arXiv:2105.08050},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.08050},
  urldate = {2022-07-30},
  abstract = {Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/EW6STAXA/Liu et al. - 2021 - Pay Attention to MLPs.pdf;/Users/lukakuma/Zotero/storage/EME3D57E/2105.html}
}

@online{liuPretrainPromptPredict2021a,
  title = {Pre-Train, {{Prompt}}, and {{Predict}}: {{A Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-Train, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  date = {2021-07-28},
  number = {arXiv:2107.13586},
  eprint = {arXiv:2107.13586},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.13586},
  urldate = {2022-11-04},
  abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  pubstate = {preprint},
  keywords = {*peft},
  file = {/Users/lukakuma/Zotero/storage/PK78Z3IZ/Liu et al. - 2021 - Pre-train, Prompt, and Predict A Systematic Surve.pdf;/Users/lukakuma/Zotero/storage/ASTGQT2V/2107.html}
}

@online{liuPTuningV2Prompt2022a,
  title = {P-{{Tuning}} v2: {{Prompt Tuning Can Be Comparable}} to {{Fine-tuning Universally Across Scales}} and {{Tasks}}},
  shorttitle = {P-{{Tuning}} V2},
  author = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  date = {2022-03-20},
  number = {arXiv:2110.07602},
  eprint = {arXiv:2110.07602},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.07602},
  urldate = {2023-02-15},
  abstract = {Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1\%-3\% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning \textbackslash cite\{li2021prefix,qin2021learning\} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P-tuning-v2.},
  pubstate = {preprint},
  keywords = {*peft,read},
  file = {/Users/lukakuma/Zotero/storage/2ZAXYWDS/Liu et al. - 2022 - P-Tuning v2 Prompt Tuning Can Be Comparable to Fi.pdf;/Users/lukakuma/Zotero/storage/QBWAZL29/Liu et al. - 2022 - P-Tuning v2 Prompt Tuning Can Be Comparable to Fi.pdf;/Users/lukakuma/Zotero/storage/66NJL5LN/2110.html}
}

@online{liuRainierReinforcedKnowledge2022,
  title = {Rainier: {{Reinforced Knowledge Introspector}} for {{Commonsense Question Answering}}},
  shorttitle = {Rainier},
  author = {Liu, Jiacheng and Hallinan, Skyler and Lu, Ximing and He, Pengfei and Welleck, Sean and Hajishirzi, Hannaneh and Choi, Yejin},
  date = {2022-10-22},
  number = {arXiv:2210.03078},
  eprint = {arXiv:2210.03078},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.03078},
  urldate = {2023-02-20},
  abstract = {Knowledge underpins reasoning. Recent research demonstrates that when relevant knowledge is provided as additional context to commonsense question answering (QA), it can substantially enhance the performance even on top of state-of-the-art. The fundamental challenge is where and how to find such knowledge that is high quality and on point with respect to the question; knowledge retrieved from knowledge bases are incomplete and knowledge generated from language models are inconsistent. We present Rainier, or Reinforced Knowledge Introspector, that learns to generate contextually relevant knowledge in response to given questions. Our approach starts by imitating knowledge generated by GPT-3, then learns to generate its own knowledge via reinforcement learning where rewards are shaped based on the increased performance on the resulting question answering. Rainier demonstrates substantial and consistent performance gains when tested over 9 different commonsense benchmarks: including 5 datasets that are seen during model training, as well as 4 datasets that are kept unseen. Our work is the first to report that knowledge generated by models that are orders of magnitude smaller than GPT-3, even without direct supervision on the knowledge itself, can exceed the quality of commonsense knowledge elicited from GPT-3.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/3QP53LT2/Liu et al. - 2022 - Rainier Reinforced Knowledge Introspector for Com.pdf;/Users/lukakuma/Zotero/storage/XQMKR6PC/2210.html}
}

@online{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  date = {2019-07-26},
  number = {arXiv:1907.11692},
  eprint = {arXiv:1907.11692},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2022-10-31},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/6CM6IAZL/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf}
}

@article{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-03-25},
  url = {https://arxiv.org/abs/2103.14030v2},
  urldate = {2022-04-28},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/P4XI69XD/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf;/Users/lukakuma/Zotero/storage/EDVIFV95/2103.html}
}

@online{liuWANLIWorkerAI2022,
  title = {{{WANLI}}: {{Worker}} and {{AI Collaboration}} for {{Natural Language Inference Dataset Creation}}},
  shorttitle = {{{WANLI}}},
  author = {Liu, Alisa and Swayamdipta, Swabha and Smith, Noah A. and Choi, Yejin},
  date = {2022-11-14},
  number = {arXiv:2201.05955},
  eprint = {arXiv:2201.05955},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.05955},
  urldate = {2022-12-14},
  abstract = {A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11\% on HANS and 9\% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/W53RQITE/Liu et al. - 2022 - WANLI Worker and AI Collaboration for Natural Lan.pdf;/Users/lukakuma/Zotero/storage/KGLT9TF6/2201.html}
}

@online{liVisionFeaturesMultimodal2022,
  title = {On {{Vision Features}} in {{Multimodal Machine Translation}}},
  author = {Li, Bei and Lv, Chuanhao and Zhou, Zefan and Zhou, Tao and Xiao, Tong and Ma, Anxiang and Zhu, JingBo},
  date = {2022-03-17},
  number = {arXiv:2203.09173},
  eprint = {arXiv:2203.09173},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.09173},
  urldate = {2023-02-07},
  abstract = {Previous work on multimodal machine translation (MMT) has focused on the way of incorporating vision features into translation but little attention is on the quality of vision models. In this work, we investigate the impact of vision models on MMT. Given the fact that Transformer is becoming popular in computer vision, we experiment with various strong models (such as Vision Transformer) and enhanced features (such as object-detection and image captioning). We develop a selective attention model to study the patch-level contribution of an image in MMT. On detailed probing tasks, we find that stronger vision models are helpful for learning translation from the visual modality. Our results also suggest the need of carefully examining MMT models, especially when current benchmarks are small-scale and biased. Our code could be found at \textbackslash url\{https://github.com/libeineu/fairseq\_mmt\}.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/Y5IK3UI5/Li et al. - 2022 - On Vision Features in Multimodal Machine Translati.pdf;/Users/lukakuma/Zotero/storage/7WU4XBSE/2203.html}
}

@online{loganivCuttingPromptsParameters2021,
  title = {Cutting {{Down}} on {{Prompts}} and {{Parameters}}: {{Simple Few-Shot Learning}} with {{Language Models}}},
  shorttitle = {Cutting {{Down}} on {{Prompts}} and {{Parameters}}},
  author = {Logan IV, Robert L. and Bala\v{z}evi\'c, Ivana and Wallace, Eric and Petroni, Fabio and Singh, Sameer and Riedel, Sebastian},
  date = {2021-07-01},
  number = {arXiv:2106.13353},
  eprint = {arXiv:2106.13353},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.13353},
  urldate = {2022-11-02},
  abstract = {Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither taskspecific templates nor training examples, and achieve competitive accuracy to manuallytuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1\% of the parameters. All in all, we recommend finetuning LMs for few-shot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/5JDR9XNU/Logan IV et al. - 2021 - Cutting Down on Prompts and Parameters Simple Few.pdf}
}

@unpublished{lonesHowAvoidMachine2021,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  shorttitle = {How to Avoid Machine Learning Pitfalls},
  author = {Lones, Michael A.},
  date = {2021-08-05},
  eprint = {2108.02497},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2108.02497},
  urldate = {2022-03-08},
  abstract = {This document gives a concise outline of some of the common mistakes that occur when using machine learning techniques, and what can be done to avoid them. It is intended primarily as a guide for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
  version = {1},
  file = {/Users/lukakuma/Zotero/storage/T39X3UTZ/Lones - 2021 - How to avoid machine learning pitfalls a guide fo.pdf;/Users/lukakuma/Zotero/storage/MAW5PPXL/2108.html}
}

@online{longpreFlanCollectionDesigning2023,
  title = {The {{Flan Collection}}: {{Designing Data}} and {{Methods}} for {{Effective Instruction Tuning}}},
  shorttitle = {The {{Flan Collection}}},
  author = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
  date = {2023-01-31},
  number = {arXiv:2301.13688},
  eprint = {arXiv:2301.13688},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.13688},
  urldate = {2023-02-01},
  abstract = {We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17\%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2\%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/BSFY57V8/Longpre et al. - 2023 - The Flan Collection Designing Data and Methods fo.pdf;/Users/lukakuma/Zotero/storage/43D49HMD/2301.html}
}

@online{luanSparseDenseAttentional2021,
  title = {Sparse, {{Dense}}, and {{Attentional Representations}} for {{Text Retrieval}}},
  author = {Luan, Yi and Eisenstein, Jacob and Toutanova, Kristina and Collins, Michael},
  date = {2021-02-16},
  number = {arXiv:2005.00181},
  eprint = {arXiv:2005.00181},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.00181},
  urldate = {2023-02-20},
  abstract = {Dual encoders perform retrieval by encoding documents and queries into dense lowdimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.},
  pubstate = {preprint},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/JQULZBQZ/Luan et al. - 2021 - Sparse, Dense, and Attentional Representations for.pdf;/Users/lukakuma/Zotero/storage/B4UVFSB9/2005.html}
}

@online{lucunSelfsupervisedLearningDark2021,
  title = {Self-Supervised Learning: {{The}} Dark Matter of Intelligence},
  shorttitle = {Self-Supervised Learning},
  author = {Lucun, Yann and Misra, Ishan},
  date = {2021},
  url = {https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/},
  urldate = {2021-03-18},
  abstract = {How can we build machines with human-level intelligence? There's a limit to how far the field of AI can go with supervised learning alone. Here's why self-supervised learning is one of the most promising ways to make significant progress in AI.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/KLADYL7K/self-supervised-learning-the-dark-matter-of-intelligence.html}
}

@online{luFantasticallyOrderedPrompts2022,
  title = {Fantastically {{Ordered Prompts}} and {{Where}} to {{Find Them}}: {{Overcoming Few-Shot Prompt Order Sensitivity}}},
  shorttitle = {Fantastically {{Ordered Prompts}} and {{Where}} to {{Find Them}}},
  author = {Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  date = {2022-03-03},
  number = {arXiv:2104.08786},
  eprint = {arXiv:2104.08786},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.08786},
  urldate = {2022-11-02},
  abstract = {When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, finetuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are ``fantastic'' and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true fewshot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13\% relative improvement for GPTfamily models across eleven different established text classification tasks.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/D95AYNWF/Lu et al. - 2022 - Fantastically Ordered Prompts and Where to Find Th.pdf}
}

@inproceedings{luFrozenPretrainedTransformers2022,
  title = {Frozen {{Pretrained Transformers}} as {{Universal Computation Engines}}},
  booktitle = {{{AAAI}}},
  author = {Lu, Kevin and Grover, Aditya and Abbeel, P. and Mordatch, Igor},
  date = {2022},
  doi = {10.1609/aaai.v36i7.20729},
  abstract = {This work investigates the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning, and finds language-pretrained transformers can obtain strong performance on a variety of non-language tasks. We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/GDRMFNRY/Lu et al. - 2022 - Frozen Pretrained Transformers as Universal Comput.pdf}
}

@unpublished{luketinaSurveyReinforcementLearning2019,
  title = {A {{Survey}} of {{Reinforcement Learning Informed}} by {{Natural Language}}},
  author = {Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rockt\"aschel, Tim},
  date = {2019-06-10},
  number = {arXiv:1906.03926},
  eprint = {1906.03926},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1906.03926},
  urldate = {2022-06-09},
  abstract = {To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.},
  file = {/Users/lukakuma/Zotero/storage/7JH2BREB/Luketina et al. - 2019 - A Survey of Reinforcement Learning Informed by Nat.pdf;/Users/lukakuma/Zotero/storage/32XWHQ8V/1906.html}
}

@online{luLearnExplainMultimodal2022,
  title = {Learn to {{Explain}}: {{Multimodal Reasoning}} via {{Thought Chains}} for {{Science Question Answering}}},
  shorttitle = {Learn to {{Explain}}},
  author = {Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  date = {2022-10-17},
  number = {arXiv:2209.09513},
  eprint = {arXiv:2209.09513},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.09513},
  urldate = {2023-02-08},
  abstract = {When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of \textasciitilde 21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20\% in few-shot GPT-3 and 3.99\% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96\%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40\% of the data. The data and code are available at https://scienceqa.github.io.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/6HEFXFN8/Lu et al. - 2022 - Learn to Explain Multimodal Reasoning via Thought.pdf;/Users/lukakuma/Zotero/storage/ERP99EU5/2209.html}
}

@unpublished{luNeuroLogicDecodingSupervised2021,
  title = {{{NeuroLogic Decoding}}: ({{Un}})Supervised {{Neural Text Generation}} with {{Predicate Logic Constraints}}},
  shorttitle = {{{NeuroLogic Decoding}}},
  author = {Lu, Ximing and West, Peter and Zellers, Rowan and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  date = {2021-04-20},
  eprint = {2010.12884},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.12884},
  urldate = {2022-03-08},
  abstract = {Conditional text generation often requires lexical constraints, i.e., which words should or shouldn't be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose NeuroLogic Decoding, a simple yet effective algorithm that enables neural language models -- supervised or not -- to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NeuroLogic Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NeuroLogic Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.},
  file = {/Users/lukakuma/Zotero/storage/MMWDNIEC/Lu et al. - 2021 - NeuroLogic Decoding (Un)supervised Neural Text Ge.pdf;/Users/lukakuma/Zotero/storage/HJSGAM8C/2010.html}
}

@online{luNeuroLogicEsqueDecoding2021,
  title = {{{NeuroLogic A}}*esque {{Decoding}}: {{Constrained Text Generation}} with {{Lookahead Heuristics}}},
  shorttitle = {{{NeuroLogic A}}*esque {{Decoding}}},
  author = {Lu, Ximing and Welleck, Sean and West, Peter and Jiang, Liwei and Kasai, Jungo and Khashabi, Daniel and Bras, Ronan Le and Qin, Lianhui and Yu, Youngjae and Zellers, Rowan and Smith, Noah A. and Choi, Yejin},
  date = {2021-12-16},
  number = {arXiv:2112.08726},
  eprint = {arXiv:2112.08726},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.08726},
  urldate = {2022-12-14},
  abstract = {The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A* search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop efficient lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/TKTELNTD/Lu et al. - 2021 - NeuroLogic Aesque Decoding Constrained Text Gene.pdf;/Users/lukakuma/Zotero/storage/SGSFRQ4J/2112.html}
}

@online{luoFrustratinglySimpleApproach2022a,
  title = {A {{Frustratingly Simple Approach}} for {{End-to-End Image Captioning}}},
  author = {Luo, Ziyang and Xi, Yadong and Zhang, Rongsheng and Ma, Jing},
  date = {2022-04-14},
  number = {arXiv:2201.12723},
  eprint = {arXiv:2201.12723},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.12723},
  urldate = {2023-02-15},
  abstract = {Image Captioning is a fundamental task to join vision and language, concerning about cross-modal understanding and text generation. Recent years witness the emerging attention on image captioning. Most of existing works follow a traditional two-stage training paradigm. Before training the captioning models, an extra object detector is utilized to recognize the objects in the image at first. However, they require sizeable datasets with fine-grained object annotation for training the object detector, which is a daunting task. In addition, the errors of the object detectors are easy to propagate to the following captioning models, degenerating models' performance. To alleviate such defects, we propose a frustratingly simple but highly effective end-to-end image captioning framework, Visual Conditioned GPT (VC-GPT), by connecting the pre-trained visual encoder (CLIP-ViT) and language decoder (GPT2). Different from the vanilla connection method that directly inserts the cross-attention modules into GPT2, we come up with a self-ensemble cross-modal fusion mechanism that comprehensively considers both the single- and cross-modal knowledge. As a result, we do not need extra object detectors for model training. Experimental results conducted on three popular image captioning benchmarks (MSCOCO, Flickr30k and NoCaps) demonstrate that our VC-GPT achieves either the best or the second-best performance across all evaluation metrics over extensive baseline systems.},
  pubstate = {preprint},
  keywords = {(ext) Flamingo,2-adapter,7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/FV3JR3AB/Luo et al. - 2022 - A Frustratingly Simple Approach for End-to-End Ima.pdf;/Users/lukakuma/Zotero/storage/3AI79E4C/2201.html}
}

@online{luoUnderstandingDiffusionModels2022,
  title = {Understanding {{Diffusion Models}}: {{A Unified Perspective}}},
  shorttitle = {Understanding {{Diffusion Models}}},
  author = {Luo, Calvin},
  date = {2022-08-25},
  number = {arXiv:2208.11970},
  eprint = {arXiv:2208.11970},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.11970},
  urldate = {2022-08-26},
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
  pubstate = {preprint},
  version = {1},
  keywords = {*diffusion,image generation},
  file = {/Users/lukakuma/Zotero/storage/INEV2T4E/Luo - 2022 - Understanding Diffusion Models A Unified Perspect.pdf;/Users/lukakuma/Zotero/storage/RC49N8MU/2208.html}
}

@online{luSurveyDeepLearning2022,
  title = {A {{Survey}} of {{Deep Learning}} for {{Mathematical Reasoning}}},
  author = {Lu, Pan and Qiu, Liang and Yu, Wenhao and Welleck, Sean and Chang, Kai-Wei},
  date = {2022-12-20},
  number = {arXiv:2212.10535},
  eprint = {arXiv:2212.10535},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.10535},
  urldate = {2022-12-23},
  abstract = {Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/W88I65JK/Lu et al. - 2022 - A Survey of Deep Learning for Mathematical Reasoni.pdf;/Users/lukakuma/Zotero/storage/AJKFQACI/2212.html}
}

@online{luUnifiedIOUnifiedModel2022,
  title = {Unified-{{IO}}: {{A Unified Model}} for {{Vision}}, {{Language}}, and {{Multi-Modal Tasks}}},
  shorttitle = {Unified-{{IO}}},
  author = {Lu, Jiasen and Clark, Christopher and Zellers, Rowan and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
  date = {2022-06-17},
  number = {arXiv:2206.08916},
  eprint = {arXiv:2206.08916},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.08916},
  urldate = {2022-08-08},
  abstract = {We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression comprehension, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 80 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task or benchmark specific fine-tuning. Demos for Unified-IO are available at https://unified-io.allenai.org.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/4J4JSIJR/Lu et al. - 2022 - Unified-IO A Unified Model for Vision, Language, .pdf;/Users/lukakuma/Zotero/storage/G98YJMHT/2206.html}
}

@inproceedings{lyleUnderstandingPreventingCapacity2021,
  title = {Understanding and {{Preventing Capacity Loss}} in {{Reinforcement Learning}}},
  author = {Lyle, Clare and Rowland, Mark and Dabney, Will},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=ZkC8wKoLbQ7},
  urldate = {2022-05-15},
  abstract = {The reinforcement learning (RL) problem is rife with sources of non-stationarity that can destabilize or inhibit learning progress. We identify a key mechanism by which this occurs in agents using...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/KY9XUQ3N/Lyle et al. - 2021 - Understanding and Preventing Capacity Loss in Rein.pdf;/Users/lukakuma/Zotero/storage/DQMY8V2U/forum.html}
}

@article{maBaGuaLuTargetingBrain2022,
  title = {{{BaGuaLu}}: {{Targeting Brain Scale Pretrained Models}} with over 37 {{Million Cores}}},
  author = {Ma, Zixuan and He, Jiaao and Qiu, Jiezhong and Cao, Huanqi and Wang, Yuanwei and Sun, Zhenbo and Zheng, Liyan and Wang, Haojie and Tang, Shizhi and Zheng, Tianyu and Lin, Junyang and Feng, Guanyu and Huang, Zeqiang and Gao, Jie and Zeng, Aohan and Zhang, Jianwei and Zhong, Runxin and Shi, Tianhui and Liu, Sha and Zheng, Weimin and Tang, Jie and Yang, Hongxia and Liu, Xin and Zhai, Jidong and Chen, Wenguang},
  date = {2022},
  pages = {13},
  abstract = {Large-scale pretrained AI models have shown state-of-theart accuracy in a series of important applications. As the size of pretrained AI models grows dramatically each year in an effort to achieve higher accuracy, training such models requires massive computing and memory capabilities, which accelerates the convergence of AI and HPC. However, there are still gaps in deploying AI applications on HPC systems, which need application and system co-design based on specific hardware features. To this end, this paper proposes BaGuaLu1, the first work targeting training brain scale models on an entire exascale supercomputer, the New Generation Sunway Supercomputer. By combining hardware-specific intra-node optimization and hybrid parallel strategies, BaGuaLu enables decent performance and scalability on unprecedentedly large models. The evaluation shows that BaGuaLu can train 14.5-trillionparameter models with a performance of over 1 EFLOPS using mixed-precision and has the capability to train 174trillion-parameter models, which rivals the number of synapses in a human brain.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/CDJ25CKK/Ma et al. - 2022 - BaGuaLu Targeting Brain Scale Pretrained Models w.pdf}
}

@video{machinelearningstreettalk063ProfYOSHUA2022,
  title = {\#063 - {{Prof}}. {{YOSHUA BENGIO}} - {{GFlowNets}}, {{Consciousness}} \& {{Causality}}},
  editor = {{Machine Learning Street Talk}},
  date = {2022-02-22},
  url = {https://www.youtube.com/watch?v=M49TMqK5uCE},
  urldate = {2022-05-10},
  abstract = {We are now sponsored by Weights and Biases! Please visit our sponsor link: http://wandb.me/MLST to get started creating a centralised, system of record for your team's machine learning work. Patreon: https://www.patreon.com/mlst Discord: https://discord.gg/ESrGqhf5CB For Yoshua Bengio, GFlowNets are the most exciting thing on the horizon of Machine Learning today. He believes they can solve previously intractable problems and hold the key to unlocking machine abstract reasoning itself. This discussion explores the promise of GFlowNets and the personal journey Prof. Bengio traveled to reach them. Pod version (with no music): https://anchor.fm/machinelearningstre... Our special thanks to:  - Alexander Mattick (Zickzack) References: Yoshua Bengio @ MILA (https://mila.quebec/en/person/bengio-...) GFlowNet Foundations (https://arxiv.org/pdf/2111.09266.pdf) Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation (https://arxiv.org/pdf/2106.04399.pdf) Interpolation Consistency Training for Semi-Supervised Learning (https://arxiv.org/pdf/1903.03825.pdf) Towards Causal Representation Learning (https://arxiv.org/pdf/2102.11107.pdf) Causal inference using invariant prediction: identification and confidence intervals (https://arxiv.org/pdf/1501.01332.pdf) A simple introduction to Markov Chain Monte\textendash Carlo sampling  https://link.springer.com/content/pdf... [00:00:00] Housekeeping [00:01:20] Weights and Biases sponsor clip [00:03:26] GFlowNets Introduction [00:16:24] Interview kick off [00:19:18] Galton Board Analogy [00:22:20] Free Energy Principle Connection [00:26:37] Diversity Preservation and Evolutionary Algorithms [00:28:25] The multi-armed bandit perspective [00:30:37] Avoiding Deception, Finding Unknown Unknows [00:33:53] Where GFlowNets Find Free Lunch [00:36:20] AlphaZero vs FlowZero (GFlowNets on Chess) [00:40:08] Using GFlowNets for Interactive Search [00:42:55] Learning Casaul Models as Graphs [00:46:39] Learning Abstract World Models [00:51:05] Can Machines Meta-Learn Categories [00:54:22] The Consciousness Prior. Is GPT-3 Conscious? [00:58:18] A Question For David Chalmers [01:01:25] Why are linear models dominating? They are abstraction! [01:05:23] Prof. Bengios Personal Journey (with Gary Marcus reference) [01:10:02] Debrief: A Dream Come True! [01:17:21] Abstraction is a Key [01:21:27] A Funny Definition of Causal [01:25:04] Arguing Semantics with a Semanticist [01:30:07] Human Learning Over Evolutionary Time Scales},
  editortype = {director},
  keywords = {GFlowNet,read}
}

@book{mackayInformationTheoryInference2003,
  title = {Information {{Theory}}, {{Inference}}, and {{Learning Algorithms}}},
  author = {MacKay, David J C},
  date = {2003},
  langid = {english},
  keywords = {information theory},
  file = {/Users/lukakuma/Zotero/storage/76685G7P/MacKay - Information Theory, Inference, and Learning Algori.pdf}
}

@online{madaanLanguageModelsCode2022,
  title = {Language {{Models}} of {{Code}} Are {{Few-Shot Commonsense Learners}}},
  author = {Madaan, Aman and Zhou, Shuyan and Alon, Uri and Yang, Yiming and Neubig, Graham},
  date = {2022-10-24},
  number = {arXiv:2210.07128},
  eprint = {arXiv:2210.07128},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.07128},
  urldate = {2022-11-22},
  abstract = {We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event -- or a reasoning-graph. To employ large language models (LMs) for this task, existing approaches ``serialize'' the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the target task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot setting.},
  pubstate = {preprint},
  keywords = {code},
  file = {/Users/lukakuma/Zotero/storage/4U39TT6A/Madaan et al. - 2022 - Language Models of Code are Few-Shot Commonsense L.pdf;/Users/lukakuma/Zotero/storage/3PYVZLAD/2210.html}
}

@book{maDeepLearningGraphs2021,
  title = {Deep {{Learning}} on {{Graphs}}},
  author = {Ma, Yao and Tang, Jiliang},
  date = {2021-11-30},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  isbn = {978-1-108-83174-1},
  langid = {english},
  pagetotal = {400},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/YCEKE8SJ/Ma and Tang - 2021 - Deep Learning on Graphs.pdf}
}

@online{makoviychukIsaacGymHigh2021,
  title = {Isaac {{Gym}}: {{High Performance GPU-Based Physics Simulation For Robot Learning}}},
  shorttitle = {Isaac {{Gym}}},
  author = {Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and State, Gavriel},
  date = {2021-08-25},
  number = {arXiv:2108.10470},
  eprint = {arXiv:2108.10470},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.10470},
  urldate = {2022-07-29},
  abstract = {Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU. Both physics simulation and the neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through any CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks. We host the results and videos at \textbackslash url\{https://sites.google.com/view/isaacgym-nvidia\} and isaac gym can be downloaded at \textbackslash url\{https://developer.nvidia.com/isaac-gym\}.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/ENXNW3HQ/Makoviychuk et al. - 2021 - Isaac Gym High Performance GPU-Based Physics Simu.pdf;/Users/lukakuma/Zotero/storage/K84A8PTN/2108.html}
}

@unpublished{malkinTrajectoryBalanceImproved2022,
  title = {Trajectory {{Balance}}: {{Improved Credit Assignment}} in {{GFlowNets}}},
  shorttitle = {Trajectory {{Balance}}},
  author = {Malkin, Nikolay and Jain, Moksh and Bengio, Emmanuel and Sun, Chen and Bengio, Yoshua},
  date = {2022-01-31},
  eprint = {2201.13259},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2201.13259},
  urldate = {2022-05-10},
  abstract = {Generative Flow Networks (GFlowNets) are a method for learning a stochastic policy for generating compositional objects, such as graphs or strings, from a given unnormalized density by sequences of actions, where many possible action sequences may lead to the same object. Prior temporal difference-like learning objectives for training GFlowNets, such as flow matching and detailed balance, are prone to inefficient credit propagation across action sequences, particularly in the case of long sequences. We propose a new learning objective for GFlowNets, trajectory balance, as a more efficient alternative to previously used objectives. We prove that any global minimizer of the trajectory balance objective can define a policy that samples exactly from the target distribution. In experiments on four distinct domains, we empirically demonstrate the benefits of the trajectory balance objective for GFlowNet convergence, diversity of generated samples, and robustness to long action sequences and large action spaces.},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/ZY59YVZP/Malkin et al. - 2022 - Trajectory Balance Improved Credit Assignment in .pdf;/Users/lukakuma/Zotero/storage/ZQN6D8MQ/2201.html}
}

@article{mallenWhenNotTrust,
  title = {When {{Not}} to {{Trust Language Models}}: {{Investigating Effectiveness}} and {{Limitations}} of {{Parametric}} and {{Non-Parametric Memories}}},
  author = {Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Hajishirzi, Hannaneh and Khashabi, Daniel},
  date = {2022-12-21},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/5SGGMPLW/Mallen et al. - When Not to Trust Language Models Investigating E.pdf}
}

@online{maLunaLinearUnified2021,
  title = {Luna: {{Linear Unified Nested Attention}}},
  shorttitle = {Luna},
  author = {Ma, Xuezhe and Kong, Xiang and Wang, Sinong and Zhou, Chunting and May, Jonathan and Ma, Hao and Zettlemoyer, Luke},
  date = {2021-11-02},
  number = {arXiv:2106.01540},
  eprint = {arXiv:2106.01540},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.01540},
  urldate = {2022-08-24},
  abstract = {The quadratic computational and memory complexities of the Transformer's attention mechanism have limited its scalability for modeling long sequences. In this paper, we propose Luna, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity. Specifically, with the first attention function, Luna packs the input sequence into a sequence of fixed length. Then, the packed sequence is unpacked using the second attention function. As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a fixed length as input and an additional corresponding output, which allows Luna to perform attention operation linearly, while also storing adequate contextual information. We perform extensive evaluations on three benchmarks of sequence modeling tasks: long-context sequence modeling, neural machine translation and masked language modeling for large-scale pretraining. Competitive or even better experimental results demonstrate both the effectiveness and efficiency of Luna compared to a variety},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/JQ5M4R55/Ma et al. - 2021 - Luna Linear Unified Nested Attention.pdf;/Users/lukakuma/Zotero/storage/9RYIUCLQ/2106.html}
}

@article{mandhaneMuZeroSelfcompetitionRate2022,
  title = {{{MuZero}} with {{Self-competition}} for {{Rate Control}} in {{VP9 Video Compression}}},
  author = {Mandhane, Amol and Zhernov, Anton and Rauh, Maribeth and Gu, Chenjie and Wang, Miaosen and Xue, Flora and Shang, Wendy and Pang, Derek and Claus, Rene and Chiang, Ching-Han and Chen, Cheng and Han, Jingning and Chen, Angie and Mankowitz, Daniel J. and Broshear, Jackson and Schrittwieser, Julian and Hubert, Thomas and Vinyals, Oriol and Mann, Timothy},
  date = {2022-02-14},
  url = {https://arxiv.org/abs/2202.06626v1},
  urldate = {2022-06-07},
  abstract = {Video streaming usage has seen a significant rise as entertainment, education, and business increasingly rely on online video. Optimizing video compression has the potential to increase access and quality of content to users, and reduce energy use and costs overall. In this paper, we present an application of the MuZero algorithm to the challenge of video compression. Specifically, we target the problem of learning a rate control policy to select the quantization parameters (QP) in the encoding process of libvpx, an open source VP9 video compression library widely used by popular video-on-demand (VOD) services. We treat this as a sequential decision making problem to maximize the video quality with an episodic constraint imposed by the target bitrate. Notably, we introduce a novel self-competition based reward mechanism to solve constrained RL with variable constraint satisfaction difficulty, which is challenging for existing constrained RL methods. We demonstrate that the MuZero-based rate control achieves an average 6.28\% reduction in size of the compressed videos for the same delivered video quality level (measured as PSNR BD-rate) compared to libvpx's two-pass VBR rate control policy, while having better constraint satisfaction behavior.},
  langid = {english},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/FPNYCSFN/Mandhane et al. - 2022 - MuZero with Self-competition for Rate Control in V.pdf;/Users/lukakuma/Zotero/storage/54EDIYQK/2202.html}
}

@unpublished{mandiEffectivenessFinetuningMetareinforcement2022,
  title = {On the {{Effectiveness}} of {{Fine-tuning Versus Meta-reinforcement Learning}}},
  author = {Mandi, Zhao and Abbeel, Pieter and James, Stephen},
  date = {2022-06-07},
  number = {arXiv:2206.03271},
  eprint = {2206.03271},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.03271},
  urldate = {2022-06-08},
  abstract = {Intelligent agents should have the ability to leverage knowledge from previously learned tasks in order to learn new ones quickly and efficiently. Meta-learning approaches have emerged as a popular solution to achieve this. However, meta-reinforcement learning (meta-RL) algorithms have thus far been restricted to simple environments with narrow task distributions. Moreover, the paradigm of pretraining followed by fine-tuning to adapt to new tasks has emerged as a simple yet effective solution in supervised and self-supervised learning. This calls into question the benefits of meta-learning approaches also in reinforcement learning, which typically come at the cost of high complexity. We hence investigate meta-RL approaches in a variety of vision-based benchmarks, including Procgen, RLBench, and Atari, where evaluations are made on completely novel tasks. Our findings show that when meta-learning approaches are evaluated on different tasks (rather than different variations of the same task), multi-task pretraining with fine-tuning on new tasks performs equally as well, or better, than meta-pretraining with meta test-time adaptation. This is encouraging for future research, as multi-task pretraining tends to be simpler and computationally cheaper than meta-RL. From these findings, we advocate for evaluating future meta-RL methods on more challenging tasks and including multi-task pretraining with fine-tuning as a simple, yet strong baseline.},
  file = {/Users/lukakuma/Zotero/storage/KRDM6LAD/Mandi et al. - 2022 - On the Effectiveness of Fine-tuning Versus Meta-re.pdf;/Users/lukakuma/Zotero/storage/S3WBW8CP/2206.html}
}

@article{manningHumanLanguageUnderstanding2022,
  title = {Human {{Language Understanding}} \& {{Reasoning}}},
  author = {Manning, Christopher D},
  date = {2022},
  pages = {12},
  langid = {english},
  keywords = {read}
}

@online{maPrinciplesParsimonySelfConsistency2022,
  title = {On the {{Principles}} of {{Parsimony}} and {{Self-Consistency}} for the {{Emergence}} of {{Intelligence}}},
  author = {Ma, Yi and Tsao, Doris and Shum, Heung-Yeung},
  date = {2022-07-27},
  number = {arXiv:2207.04630},
  eprint = {arXiv:2207.04630},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.04630},
  urldate = {2022-07-29},
  abstract = {Ten years into the revival of deep networks and artificial intelligence, we propose a theoretical framework that sheds light on understanding deep networks within a bigger picture of Intelligence in general. We introduce two fundamental principles, Parsimony and Self-consistency, that address two fundamental questions regarding Intelligence: what to learn and how to learn, respectively. We believe the two principles are the cornerstones for the emergence of Intelligence, artificial or natural. While these two principles have rich classical roots, we argue that they can be stated anew in entirely measurable and computable ways. More specifically, the two principles lead to an effective and efficient computational framework, compressive closed-loop transcription, that unifies and explains the evolution of modern deep networks and many artificial intelligence practices. While we mainly use modeling of visual data as an example, we believe the two principles will unify understanding of broad families of autonomous intelligent systems and provide a framework for understanding the brain.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/398J3RTD/Ma et al. - 2022 - On the Principles of Parsimony and Self-Consistenc.pdf;/Users/lukakuma/Zotero/storage/C3SY4WX5/2207.html}
}

@video{marksaroufimGraphNeuralNetworks2021,
  title = {Graph {{Neural Networks}} on {{Point Clouds}}},
  editor = {{Mark Saroufim}},
  date = {2021-01-07},
  url = {https://www.youtube.com/watch?v=NXZLmTzRGdw},
  urldate = {2022-03-24},
  abstract = {References Latent graph neural networks Manifold learning 2.0: https://towardsdatascience.com/manifo... Dynamic Graph CNN: https://arxiv.org/abs/1801.07829 Differentiable Graph Module (DGM) for Graph Convolutional Networks: https://arxiv.org/abs/2002.04999 Intro: (0:00) Manifold learning 2.0: (9:53) Dynamic Graph CNN for Learning on Point Clouds: (18:54) Differentiable Graph Module: (29:51) Conclusion: (36:40)},
  editortype = {director},
  keywords = {GNN}
}

@online{martelACORNAdaptiveCoordinate2021,
  title = {{{ACORN}}: {{Adaptive Coordinate Networks}} for {{Neural Scene Representation}}},
  shorttitle = {{{ACORN}}},
  author = {Martel, Julien N. P. and Lindell, David B. and Lin, Connor Z. and Chan, Eric R. and Monteiro, Marco and Wetzstein, Gordon},
  date = {2021-05-06},
  number = {arXiv:2105.02788},
  eprint = {arXiv:2105.02788},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.02788},
  urldate = {2022-08-29},
  abstract = {Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.},
  pubstate = {preprint},
  keywords = {functa},
  file = {/Users/lukakuma/Zotero/storage/C4RMQJEB/Martel et al. - 2021 - ACORN Adaptive Coordinate Networks for Neural Sce.pdf;/Users/lukakuma/Zotero/storage/4GW2JPZI/2105.html}
}

@article{marxBiologyBeginsTangle2021,
  title = {Biology Begins to Tangle with Quantum Computing},
  author = {Marx, Vivien},
  date = {2021-07},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {18},
  number = {7},
  pages = {715--719},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  url = {https://www.nature.com/articles/s41592-021-01199-z},
  urldate = {2022-05-26},
  abstract = {Quantum computing promises plenty, such as how it can massively accelerate some bioinformatics calculations.},
  issue = {7},
  langid = {english},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/KZHGSDFT/Marx - 2021 - Biology begins to tangle with quantum computing.pdf;/Users/lukakuma/Zotero/storage/5CTMA99B/s41592-021-01199-z.html}
}

@online{matenaMergingModelsFisherWeighted2022,
  title = {Merging {{Models}} with {{Fisher-Weighted Averaging}}},
  author = {Matena, Michael and Raffel, Colin},
  date = {2022-08-26},
  number = {arXiv:2111.09832},
  eprint = {arXiv:2111.09832},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.09832},
  urldate = {2022-12-12},
  abstract = {Averaging the parameters of models that have the same architecture and initialization can provide a means of combining their respective capabilities. In this paper, we take the perspective that this "merging" operation can be seen as choosing parameters that approximately maximize the joint likelihood of the posteriors of the models' parameters. Computing a simple average of the models' parameters therefore corresponds to making an isotropic Gaussian approximation to their posteriors. We develop an alternative merging procedure based on the Laplace approximation where we approximate each model's posterior as a Gaussian distribution whose precision matrix corresponds to its Fisher information. We first show that our "Fisher merging" technique provides a performance boost in settings where simple parameter averaging is currently used -- specifically, robust fine-tuning and model ensembling. Then, we compare merging to standard gradient-based transfer learning and demonstrate that merging enables a fundamentally different method for transferring capabilities across models. Specifically, we show that Fisher merging is competitive with gradient-based transfer learning approaches (while being significantly cheaper) in intermediate-task training and domain-adaptive pre-training. We also show that our merging procedure makes it possible to combine models in previously unexplored ways. We release our code to facilitate future research into methods for merging models.},
  pubstate = {preprint},
  keywords = {model fusion},
  file = {/Users/lukakuma/Zotero/storage/J4QDBM8M/Matena and Raffel - 2022 - Merging Models with Fisher-Weighted Averaging.pdf;/Users/lukakuma/Zotero/storage/9AL64VHI/2111.html}
}

@online{MathGraphNeural2022,
  title = {Math {{Behind Graph Neural Networks}}},
  date = {2022-03-20T00:00:00+08:00},
  url = {https://rish-16.github.io/posts/gnn-math/},
  urldate = {2022-03-29},
  langid = {english},
  organization = {{Rishabh Anand}},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/INQZ9YBP/gnn-math.html}
}

@online{matiisenTeacherStudentCurriculumLearning2017,
  title = {Teacher-{{Student Curriculum Learning}}},
  author = {Matiisen, Tambet and Oliver, Avital and Cohen, Taco and Schulman, John},
  date = {2017-11-29},
  number = {arXiv:1707.00183},
  eprint = {arXiv:1707.00183},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1707.00183},
  urldate = {2022-07-23},
  abstract = {We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e. where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with LSTM and navigation in Minecraft. Using our automatically generated curriculum enabled to solve a Minecraft maze that could not be solved at all when training directly on solving the maze, and the learning was an order of magnitude faster than uniform sampling of subtasks.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/A9MYSAP2/Matiisen et al. - 2017 - Teacher-Student Curriculum Learning.pdf;/Users/lukakuma/Zotero/storage/DPCGSDJI/1707.html}
}

@article{matsuoDeepLearningReinforcement2022,
  title = {Deep Learning, Reinforcement Learning, and World Models},
  author = {Matsuo, Yutaka and LeCun, Yann and Sahani, Maneesh and Precup, Doina and Silver, David and Sugiyama, Masashi and Uchibe, Eiji and Morimoto, Jun},
  date = {2022-04},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {152},
  pages = {267--275},
  issn = {0893-6080},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608022001150},
  urldate = {2022-06-06},
  abstract = {Deep learning (DL) and reinforcement learning (RL) methods seem to be a part of indispensable factors to achieve human-level or super-human AI systems. On the other hand, both DL and RL have strong connections with our brain functions and with neuroscientific findings. In this review, we summarize talks and discussions in the ``Deep Learning and Reinforcement Learning'' session of the symposium, International Symposium on Artificial Intelligence and Brain Science. In this session, we discussed whether we can achieve comprehensive understanding of human intelligence based on the recent advances of deep learning and reinforcement learning algorithms. Speakers contributed to provide talks about their recent studies that can be key technologies to achieve human-level intelligence.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/TYUAF8TU/Matsuo et al. - 2022 - Deep learning, reinforcement learning, and world m.pdf}
}

@unpublished{maUniversalSuccessorFeatures2020,
  title = {Universal {{Successor Features}} for {{Transfer Reinforcement Learning}}},
  author = {Ma, Chen and Ashley, Dylan R. and Wen, Junfeng and Bengio, Yoshua},
  date = {2020-01-04},
  number = {arXiv:2001.04025},
  eprint = {2001.04025},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2001.04025},
  urldate = {2022-06-13},
  abstract = {Transfer in Reinforcement Learning (RL) refers to the idea of applying knowledge gained from previous tasks to solving related tasks. Learning a universal value function (Schaul et al., 2015), which generalizes over goals and states, has previously been shown to be useful for transfer. However, successor features are believed to be more suitable than values for transfer (Dayan, 1993; Barreto et al.,2017), even though they cannot directly generalize to new goals. In this paper, we propose (1) Universal Successor Features (USFs) to capture the underlying dynamics of the environment while allowing generalization to unseen goals and (2) a flexible end-to-end model of USFs that can be trained by interacting with the environment. We show that learning USFs is compatible with any RL algorithm that learns state values using a temporal difference method. Our experiments in a simple gridworld and with two MuJoCo environments show that USFs can greatly accelerate training when learning multiple tasks and can effectively transfer knowledge to new tasks.},
  file = {/Users/lukakuma/Zotero/storage/SD7LYIJP/Ma et al. - 2020 - Universal Successor Features for Transfer Reinforc.pdf;/Users/lukakuma/Zotero/storage/NTFFPHKS/2001.html}
}

@online{mccandlishEmpiricalModelLargeBatch2018,
  title = {An {{Empirical Model}} of {{Large-Batch Training}}},
  author = {McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  date = {2018-12-14},
  number = {arXiv:1812.06162},
  eprint = {arXiv:1812.06162},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1812.06162},
  urldate = {2022-07-29},
  abstract = {In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/KQ5VNV7E/McCandlish et al. - 2018 - An Empirical Model of Large-Batch Training.pdf;/Users/lukakuma/Zotero/storage/FX9VJJB3/1812.html}
}

@online{mcdonnellFutureAIActionDriven2022,
  type = {Substack newsletter},
  title = {The {{Near Future}} of {{AI}} Is {{Action-Driven}}},
  author = {McDonnell, John},
  date = {2022-11-16},
  url = {https://jmcdonnell.substack.com/p/the-near-future-of-ai-is-action-driven},
  urldate = {2022-11-17},
  abstract = {\ldots and it will look a lot like AGI. LLMs can become extremely powerful by leveraging external cognitive assets and choosing actions.},
  organization = {{Causal Deference}},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/FHBF47TH/the-near-future-of-ai-is-action-driven.html}
}

@unpublished{mcmahanCommunicationEfficientLearningDeep2017,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and family=Arcas, given=Blaise Ag\"uera, prefix=y, useprefix=false},
  date = {2017-02-28},
  eprint = {1602.05629},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1602.05629},
  urldate = {2022-03-11},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  file = {/Users/lukakuma/Zotero/storage/MXCL63T4/McMahan et al. - 2017 - Communication-Efficient Learning of Deep Networks .pdf;/Users/lukakuma/Zotero/storage/9W27KDBZ/1602.html}
}

@online{mcmillan-majorDocumentingGeographicallyContextually2022,
  title = {Documenting {{Geographically}} and {{Contextually Diverse Data Sources}}: {{The BigScience Catalogue}} of {{Language Data}} and {{Resources}}},
  shorttitle = {Documenting {{Geographically}} and {{Contextually Diverse Data Sources}}},
  author = {McMillan-Major, Angelina and Alyafeai, Zaid and Biderman, Stella and Chen, Kimbo and De Toni, Francesco and Dupont, G\'erard and Elsahar, Hady and Emezue, Chris and Aji, Alham Fikri and Ili\'c, Suzana and Khamis, Nurulaqilla and Leong, Colin and Masoud, Maraim and Soroa, Aitor and Suarez, Pedro Ortiz and Talat, Zeerak and family=Strien, given=Daniel, prefix=van, useprefix=true and Jernite, Yacine},
  date = {2022-01-24},
  number = {arXiv:2201.10066},
  eprint = {arXiv:2201.10066},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.10066},
  urldate = {2022-10-30},
  abstract = {In recent years, large-scale data collection efforts have prioritized the amount of data collected in order to improve the modeling capabilities of large language models. This prioritization, however, has resulted in concerns with respect to the rights of data subjects represented in data collections, particularly when considering the difficulty in interrogating these collections due to insufficient documentation and tools for analysis. Mindful of these pitfalls, we present our methodology for a documentation-first, human-centered data collection project as part of the BigScience initiative. We identified a geographically diverse set of target language groups (Arabic, Basque, Chinese, Catalan, English, French, Indic languages, Indonesian, Niger-Congo languages, Portuguese, Spanish, and Vietnamese, as well as programming languages) for which to collect metadata on potential data sources. To structure this effort, we developed our online catalogue as a supporting tool for gathering metadata through organized public hackathons. We present our development process; analyses of the resulting resource metadata, including distributions over languages, regions, and resource types; and our lessons learned in this endeavor.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/L9ZBYT3S/McMillan-Major et al. - 2022 - Documenting Geographically and Contextually Divers.pdf;/Users/lukakuma/Zotero/storage/8G9UYF4J/2201.html}
}

@online{meisterLocallyTypicalSampling2022,
  title = {Locally {{Typical Sampling}}},
  author = {Meister, Clara and Pimentel, Tiago and Wiher, Gian and Cotterell, Ryan},
  date = {2022-10-16},
  number = {arXiv:2202.00666},
  eprint = {arXiv:2202.00666},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.00666},
  urldate = {2023-02-02},
  abstract = {Today's probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics, e.g., perplexity. This discrepancy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language generation as a discrete stochastic process--which allows for an information-theoretic analysis--can provide new insights into the behavior of probabilistic language generators, e.g., why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner; in fact, psycholinguistics research suggests humans choose each word in a string with this subconscious goal in mind. We formally define the set of strings that meet this criterion: those for which each word has an information content close to the expected information content, i.e., the conditional entropy of our model. We then propose a simple and efficient procedure for enforcing this criterion when generating from probabilistic models, which we call locally typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, locally typical sampling offers competitive performance (in both abstractive summarization and story generation) in terms of quality while consistently reducing degenerate repetitions.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/BVK55V8Q/Meister et al. - 2022 - Locally Typical Sampling.pdf;/Users/lukakuma/Zotero/storage/6CIP3RFE/2202.html}
}

@unpublished{mendoncaDiscoveringAchievingGoals2021,
  title = {Discovering and {{Achieving Goals}} via {{World Models}}},
  author = {Mendonca, Russell and Rybkin, Oleh and Daniilidis, Kostas and Hafner, Danijar and Pathak, Deepak},
  date = {2021-10-18},
  number = {arXiv:2110.09514},
  eprint = {2110.09514},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.09514},
  urldate = {2022-05-13},
  abstract = {How can artificial agents learn to solve many diverse tasks in complex visual environments in the absence of any supervision? We decompose this question into two problems: discovering new goals and learning to reliably achieve them. We introduce Latent Explorer Achiever (LEXA), a unified solution to these that learns a world model from image inputs and uses it to train an explorer and an achiever policy from imagined rollouts. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever to practice. After the unsupervised phase, LEXA solves tasks specified as goal images zero-shot without any additional learning. LEXA substantially outperforms previous approaches to unsupervised goal-reaching, both on prior benchmarks and on a new challenging benchmark with a total of 40 test tasks spanning across four standard robotic manipulation and locomotion domains. LEXA further achieves goals that require interacting with multiple objects in sequence. Finally, to demonstrate the scalability and generality of LEXA, we train a single general agent across four distinct environments. Code and videos at https://orybkin.github.io/lexa/},
  file = {/Users/lukakuma/Zotero/storage/WBI7UTGJ/Mendonca et al. - 2021 - Discovering and Achieving Goals via World Models.pdf;/Users/lukakuma/Zotero/storage/2T4HLNNY/2110.html}
}

@online{menickGeneratingHighFidelity2018,
  title = {Generating {{High Fidelity Images}} with {{Subscale Pixel Networks}} and {{Multidimensional Upscaling}}},
  author = {Menick, Jacob and Kalchbrenner, Nal},
  date = {2018-12-04},
  number = {arXiv:1812.01608},
  eprint = {arXiv:1812.01608},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1812.01608},
  urldate = {2022-08-03},
  abstract = {The unconditional generation of high fidelity images is a longstanding benchmark for testing the performance of image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem. Among the major challenges are the capacity to encode the vast previous context and the sheer difficulty of learning a distribution that preserves both global semantic coherence and exactness of detail. To address the former challenge, we propose the Subscale Pixel Network (SPN), a conditional decoder architecture that generates an image as a sequence of sub-images of equal size. The SPN compactly captures image-wide spatial dependencies and requires a fraction of the memory and the computation required by other fully autoregressive models. To address the latter challenge, we propose to use Multidimensional Upscaling to grow an image in both size and depth via intermediate stages utilising distinct SPNs. We evaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in multiple settings, set up new benchmark results in previously unexplored settings and are able to generate very high fidelity large scale samples on the basis of both datasets.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/LRKKGFFM/Menick and Kalchbrenner - 2018 - Generating High Fidelity Images with Subscale Pixe.pdf;/Users/lukakuma/Zotero/storage/5JU6NY78/1812.html}
}

@online{menickTeachingLanguageModels2022a,
  title = {Teaching Language Models to Support Answers with Verified Quotes},
  author = {Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and McAleese, Nat},
  date = {2022-03-21},
  number = {arXiv:2203.11147},
  eprint = {arXiv:2203.11147},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.11147},
  urldate = {2022-12-28},
  abstract = {Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train "open-book" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\textbackslash\% of the time on this Natural Questions subset, and 67\textbackslash\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\textbackslash\% and 80\textbackslash\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/VQNMPIUL/Menick et al. - 2022 - Teaching language models to support answers with v.pdf;/Users/lukakuma/Zotero/storage/59IZ29GI/2203.html}
}

@article{merigouxCatalaProgrammingLanguage2021,
  title = {Catala: {{A Programming Language}} for the {{Law}}},
  shorttitle = {Catala},
  author = {Merigoux, Denis and Chataing, Nicolas and Protzenko, Jonathan},
  date = {2021-08-22},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {5},
  eprint = {2103.03198},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--29},
  issn = {2475-1421},
  url = {http://arxiv.org/abs/2103.03198},
  urldate = {2023-02-02},
  abstract = {Law at large underpins modern society, codifying and governing many aspects of citizens' daily lives. Oftentimes, law is subject to interpretation, debate and challenges throughout various courts and jurisdictions. But in some other areas, law leaves little room for interpretation, and essentially aims to rigorously describe a computation, a decision procedure or, simply said, an algorithm. Unfortunately, prose remains a woefully inadequate tool for the job. The lack of formalism leaves room for ambiguities; the structure of legal statutes, with many paragraphs and sub-sections spread across multiple pages, makes it hard to compute the intended outcome of the algorithm underlying a given text; and, as with any other piece of poorly-specified critical software, the use of informal language leaves corner cases unaddressed. We introduce Catala, a new programming language that we specifically designed to allow a straightforward and systematic translation of statutory law into an executable implementation. Catala aims to bring together lawyers and programmers through a shared medium, which together they can understand, edit and evolve, bridging a gap that often results in dramatically incorrect implementations of the law. We have implemented a compiler for Catala, and have proven the correctness of its core compilation steps using the F* proof assistant. We evaluate Catala on several legal texts that are algorithms in disguise, notably section 121 of the US federal income tax and the byzantine French family benefits; in doing so, we uncover a bug in the official implementation. We observe as a consequence of the formalization process that using Catala enables rich interactions between lawyers and programmers, leading to a greater understanding of the original legislative intent, while producing a correct-by-construction executable specification reusable by the greater software ecosystem.},
  issue = {ICFP},
  file = {/Users/lukakuma/Zotero/storage/A2CALAKF/Merigoux et al. - 2021 - Catala A Programming Language for the Law.pdf;/Users/lukakuma/Zotero/storage/Y7SJDH52/2103.html}
}

@online{merulloLinearlyMappingImage2022,
  title = {Linearly {{Mapping}} from {{Image}} to {{Text Space}}},
  author = {Merullo, Jack and Castricato, Louis and Eickhoff, Carsten and Pavlick, Ellie},
  date = {2022-09-29},
  number = {arXiv:2209.15162},
  eprint = {arXiv:2209.15162},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.15162},
  urldate = {2023-02-27},
  abstract = {The extent to which text-only language models (LMs) learn to represent the physical, non-linguistic world is an open question. Prior work has shown that pretrained LMs can be taught to ``understand'' visual inputs when the models' parameters are updated on image captioning tasks. We test a stronger hypothesis: that the conceptual representations learned by text-only models are functionally equivalent (up to a linear transformation) to those learned by models trained on vision tasks. Specifically, we show that the image representations from vision models can be transferred as continuous prompts to frozen LMs by training only a single linear projection. Using these to prompt the LM achieves competitive performance on captioning and visual question answering tasks compared to models that tune both the image encoder and text decoder (such as the MAGMA model). We compare three image encoders with increasing amounts of linguistic supervision seen during pretraining: BEIT (no linguistic information), NF-ResNET (lexical category information), and CLIP (full natural language descriptions). We find that all three encoders perform equally well at transferring visual property information to the language model (e.g., whether an animal is large or small), but that image encoders pretrained with linguistic supervision more saliently encode category information (e.g., distinguishing hippo vs.\textbackslash{} elephant) and thus perform significantly better on benchmark language-and-vision tasks. Our results indicate that LMs encode conceptual information structurally similarly to vision-based models, even those that are solely trained on images.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/LW3SEI44/Merullo et al. - 2022 - Linearly Mapping from Image to Text Space.pdf;/Users/lukakuma/Zotero/storage/NYF68EI9/2209.html}
}

@article{metafundamentalairesearchdiplomacyteamfairHumanlevelPlayGame2022,
  title = {Human-Level Play in the Game of {{Diplomacy}} by Combining Language Models with Strategic Reasoning},
  author = {{META FUNDAMENTAL AI RESEARCH DIPLOMACY TEAM (FAIR)} and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and Jacob, Athul Paul and Komeili, Mojtaba and Konath, Karthik and Kwon, Minae and Lerer, Adam and Lewis, Mike and Miller, Alexander H. and Mitts, Sasha and Renduchintala, Adithya and Roller, Stephen and Rowe, Dirk and Shi, Weiyan and Spisak, Joe and Wei, Alexander and Wu, David and Zhang, Hugh and Zijlstra, Markus},
  date = {2022-11-22},
  journaltitle = {Science},
  volume = {0},
  number = {0},
  pages = {eade9097},
  publisher = {{American Association for the Advancement of Science}},
  url = {https://www.science.org/doi/10.1126/science.ade9097},
  urldate = {2022-11-24},
  abstract = {Despite much progress in training AI systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players' beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10\% of participants who played more than one game.},
  keywords = {(ext) CICERO,read},
  file = {/Users/lukakuma/Zotero/storage/KWSS54CJ/META FUNDAMENTAL AI RESEARCH DIPLOMACY TEAM (FAIR) et al. - 2022 - Human-level play in the game of Diplomacy by combi.pdf}
}

@article{metzlerRethinkingSearchMaking2021,
  title = {Rethinking {{Search}}: {{Making Domain Experts}} out of {{Dilettantes}}},
  shorttitle = {Rethinking {{Search}}},
  author = {Metzler, Donald and Tay, Yi and Bahri, Dara and Najork, Marc},
  date = {2021-06},
  journaltitle = {ACM SIGIR Forum},
  shortjournal = {SIGIR Forum},
  volume = {55},
  number = {1},
  eprint = {2105.02274},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--27},
  issn = {0163-5840},
  url = {http://arxiv.org/abs/2105.02274},
  urldate = {2023-01-14},
  abstract = {When experiencing an information need, users want to engage with a domain expert, but often turn to an information retrieval system, such as a search engine, instead. Classical information retrieval systems do not answer information needs directly, but instead provide references to (hopefully authoritative) answers. Successful question answering systems offer a limited corpus created on-demand by human experts, which is neither timely nor scalable. Pre-trained language models, by contrast, are capable of directly generating prose that may be responsive to an information need, but at present they are dilettantes rather than domain experts -- they do not have a true understanding of the world, they are prone to hallucinating, and crucially they are incapable of justifying their utterances by referring to supporting documents in the corpus they were trained over. This paper examines how ideas from classical information retrieval and pre-trained language models can be synthesized and evolved into systems that truly deliver on the promise of domain expert advice.},
  file = {/Users/lukakuma/Zotero/storage/2PTUAWIC/Metzler et al. - 2021 - Rethinking Search Making Domain Experts out of Di.pdf;/Users/lukakuma/Zotero/storage/NJFLVADF/2105.html}
}

@online{metzVeLOTrainingVersatile2022,
  title = {{{VeLO}}: {{Training Versatile Learned Optimizers}} by {{Scaling Up}}},
  shorttitle = {{{VeLO}}},
  author = {Metz, Luke and Harrison, James and Freeman, C. Daniel and Merchant, Amil and Beyer, Lucas and Bradbury, James and Agrawal, Naman and Poole, Ben and Mordatch, Igor and Roberts, Adam and Sohl-Dickstein, Jascha},
  date = {2022-11-17},
  number = {arXiv:2211.09760},
  eprint = {arXiv:2211.09760},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.09760},
  urldate = {2022-11-18},
  abstract = {While deep learning models have replaced hand-designed features across many domains, these models are still trained with hand-designed optimizers. In this work, we leverage the same scaling approach behind the success of deep learning to learn versatile optimizers. We train an optimizer for deep learning which is itself a small neural network that ingests gradients and outputs parameter updates. Meta-trained with approximately four thousand TPU-months of compute on a wide variety of optimization tasks, our optimizer not only exhibits compelling performance, but optimizes in interesting and unexpected ways. It requires no hyperparameter tuning, instead automatically adapting to the specifics of the problem being optimized. We open source our learned optimizer, meta-training code, the associated train and test data, and an extensive optimizer benchmark suite with baselines at velo-code.github.io.},
  pubstate = {preprint},
  keywords = {optimizer},
  file = {/Users/lukakuma/Zotero/storage/NSTDH4G5/Metz et al. - 2022 - VeLO Training Versatile Learned Optimizers by Sca.pdf;/Users/lukakuma/Zotero/storage/88YVVA57/2211.html}
}

@online{mialonAugmentedLanguageModels2023,
  title = {Augmented {{Language Models}}: A {{Survey}}},
  shorttitle = {Augmented {{Language Models}}},
  author = {Mialon, Gr\'egoire and Dess\`i, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi\`ere, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and Grave, Edouard and LeCun, Yann and Scialom, Thomas},
  date = {2023-02-15},
  number = {arXiv:2302.07842},
  eprint = {arXiv:2302.07842},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.07842},
  urldate = {2023-02-19},
  abstract = {This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.},
  pubstate = {preprint},
  keywords = {multiagent,read},
  file = {/Users/lukakuma/Zotero/storage/A4EHSXTS/Mialon et al. - 2023 - Augmented Language Models a Survey.pdf;/Users/lukakuma/Zotero/storage/DGBTMGU6/2302.html}
}

@online{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  date = {2020-08-03},
  number = {arXiv:2003.08934},
  eprint = {arXiv:2003.08934},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2003.08934},
  urldate = {2022-08-29},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  pubstate = {preprint},
  keywords = {functa},
  file = {/Users/lukakuma/Zotero/storage/B8MPCGKR/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf;/Users/lukakuma/Zotero/storage/INXQIWIE/2003.html}
}

@online{minNoisyChannelLanguage2022,
  title = {Noisy {{Channel Language Model Prompting}} for {{Few-Shot Text Classification}}},
  author = {Min, Sewon and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  date = {2022-03-15},
  number = {arXiv:2108.04106},
  eprint = {arXiv:2108.04106},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.04106},
  urldate = {2022-11-02},
  abstract = {We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every .  word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worstcase accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive methods (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/V9R8RU32/Min et al. - 2022 - Noisy Channel Language Model Prompting for Few-Sho.pdf}
}

@online{minRethinkingRoleDemonstrations2022a,
  title = {Rethinking the {{Role}} of {{Demonstrations}}: {{What Makes In-Context Learning Work}}?},
  shorttitle = {Rethinking the {{Role}} of {{Demonstrations}}},
  author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  date = {2022-10-20},
  number = {arXiv:2202.12837},
  eprint = {arXiv:2202.12837},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.12837},
  urldate = {2022-11-02},
  abstract = {Large language models (LMs) are able to incontext learn\textemdash perform a new task via inference alone by conditioning on a few inputlabel pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required\textemdash randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  langid = {english},
  pubstate = {preprint},
  keywords = {5-reasoning,read},
  file = {/Users/lukakuma/Zotero/storage/W9HB2VZY/Min et al. - 2022 - Rethinking the Role of Demonstrations What Makes .pdf;/Users/lukakuma/Zotero/storage/7SPLTQRN/2202.html}
}

@article{mirowskiCoWritingScreenplaysTheatre,
  title = {Co-{{Writing Screenplays}} and {{Theatre Scripts}} with {{Language Models}}  {{An Evaluation}} by {{Industry Professionals}}},
  author = {Mirowski, Piotr and Mathewson, Kory W and Pittman, Jaylen and Evans, Richard},
  date = {2022-09-29},
  pages = {102},
  langid = {english},
  keywords = {8-chat,DeepMind},
  file = {/Users/lukakuma/Zotero/storage/6SFZBP5Z/Mirowski et al. - Co-Writing Screenplays and Theatre Scripts with La.pdf}
}

@online{mishchenkoProxSkipYesLocal2022,
  title = {{{ProxSkip}}: {{Yes}}! {{Local Gradient Steps Provably Lead}} to {{Communication Acceleration}}! {{Finally}}!},
  shorttitle = {{{ProxSkip}}},
  author = {Mishchenko, Konstantin and Malinovsky, Grigory and Stich, Sebastian and Richt\'arik, Peter},
  date = {2022-02-18},
  number = {arXiv:2202.09357},
  eprint = {arXiv:2202.09357},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.09357},
  urldate = {2022-11-30},
  abstract = {We introduce \textbackslash algname\{ProxSkip\} -- a surprisingly simple and provably efficient method for minimizing the sum of a smooth (\$f\$) and an expensive nonsmooth proximable (\$\textbackslash psi\$) function. The canonical approach to solving such problems is via the proximal gradient descent (\textbackslash algname\{ProxGD\}) algorithm, which is based on the evaluation of the gradient of \$f\$ and the prox operator of \$\textbackslash psi\$ in each iteration. In this work we are specifically interested in the regime in which the evaluation of prox is costly relative to the evaluation of the gradient, which is the case in many applications. \textbackslash algname\{ProxSkip\} allows for the expensive prox operator to be skipped in most iterations: while its iteration complexity is \$\textbackslash cO(\textbackslash kappa \textbackslash log \textbackslash nicefrac\{1\}\{\textbackslash varepsilon\})\$, where \$\textbackslash kappa\$ is the condition number of \$f\$, the number of prox evaluations is \$\textbackslash cO(\textbackslash sqrt\{\textbackslash kappa\} \textbackslash log \textbackslash nicefrac\{1\}\{\textbackslash varepsilon\})\$ only. Our main motivation comes from federated learning, where evaluation of the gradient operator corresponds to taking a local \textbackslash algname\{GD\} step independently on all devices, and evaluation of prox corresponds to (expensive) communication in the form of gradient averaging. In this context, \textbackslash algname\{ProxSkip\} offers an effective \{\textbackslash em acceleration\} of communication complexity. Unlike other local gradient-type methods, such as \textbackslash algname\{FedAvg\}, \textbackslash algname\{SCAFFOLD\}, \textbackslash algname\{S-Local-GD\} and \textbackslash algname\{FedLin\}, whose theoretical communication complexity is worse than, or at best matching, that of vanilla \textbackslash algname\{GD\} in the heterogeneous data regime, we obtain a provable and large improvement without any heterogeneity-bounding assumptions.},
  pubstate = {preprint},
  keywords = {*open source AI},
  file = {/Users/lukakuma/Zotero/storage/WFU3AW6J/Mishchenko et al. - 2022 - ProxSkip Yes! Local Gradient Steps Provably Lead .pdf;/Users/lukakuma/Zotero/storage/YBN2FNN2/2202.html}
}

@online{mishraCrossTaskGeneralizationNatural2022,
  title = {Cross-{{Task Generalization}} via {{Natural Language Crowdsourcing Instructions}}},
  author = {Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  date = {2022-03-14},
  number = {arXiv:2104.08773},
  eprint = {arXiv:2104.08773},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.08773},
  urldate = {2022-12-09},
  abstract = {Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19\% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.},
  pubstate = {preprint},
  keywords = {finetuning},
  file = {/Users/lukakuma/Zotero/storage/XYDUECV7/Mishra et al. - 2022 - Cross-Task Generalization via Natural Language Cro.pdf;/Users/lukakuma/Zotero/storage/USXMDDFM/2104.html}
}

@unpublished{misraSelfSupervisedLearningPretextInvariant2019,
  title = {Self-{{Supervised Learning}} of {{Pretext-Invariant Representations}}},
  author = {Misra, Ishan and family=Maaten, given=Laurens, prefix=van der, useprefix=true},
  date = {2019-12-04},
  eprint = {1912.01991},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.01991},
  urldate = {2022-04-24},
  abstract = {The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations for a large training set of images. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as "pearl") that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in self-supervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised learning of image representations with good invariance properties.},
  file = {/Users/lukakuma/Zotero/storage/2BFYXL6L/Misra and van der Maaten - 2019 - Self-Supervised Learning of Pretext-Invariant Repr.pdf;/Users/lukakuma/Zotero/storage/IGGS22HZ/1912.html}
}

@online{MissingSemesterYour2020,
  title = {The {{Missing Semester}} of {{Your CS Education}}},
  date = {2020},
  url = {https://missing.csail.mit.edu/},
  urldate = {2022-09-15},
  langid = {english},
  organization = {{the missing semester of your cs education}},
  file = {/Users/lukakuma/Zotero/storage/UNEQH4AW/missing.csail.mit.edu.html}
}

@online{mitchellMemoryBasedModelEditing2022a,
  title = {Memory-{{Based Model Editing}} at {{Scale}}},
  author = {Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D. and Finn, Chelsea},
  date = {2022-06-13},
  number = {arXiv:2206.06520},
  eprint = {arXiv:2206.06520},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.06520},
  urldate = {2022-12-08},
  abstract = {Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit's intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model's predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.},
  pubstate = {preprint},
  keywords = {finetuning},
  file = {/Users/lukakuma/Zotero/storage/DGNBFRN8/Mitchell et al. - 2022 - Memory-Based Model Editing at Scale.pdf;/Users/lukakuma/Zotero/storage/GNL8PLPP/2206.html}
}

@online{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri\`a Puigdom\`enech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  date = {2016-06-16},
  number = {arXiv:1602.01783},
  eprint = {arXiv:1602.01783},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1602.01783},
  urldate = {2022-07-19},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/IVGR9HSE/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf;/Users/lukakuma/Zotero/storage/X5LV73WE/1602.html}
}

@article{moffatBiofoundryWorkflowIdentification2021,
  title = {A Biofoundry Workflow for the Identification of Genetic Determinants of Microbial Growth Inhibition},
  author = {Moffat, Alaster D and Elliston, Adam and Patron, Nicola J and Truman, Andrew W and Carrasco Lopez, Jose A},
  date = {2021-10-01},
  journaltitle = {Synthetic Biology},
  shortjournal = {Synthetic Biology},
  volume = {6},
  number = {1},
  pages = {ysab004},
  issn = {2397-7000},
  url = {https://doi.org/10.1093/synbio/ysab004},
  urldate = {2022-05-25},
  abstract = {Biofoundries integrate high-throughput software and hardware platforms with synthetic biology approaches to enable the design, execution and analyses of large-scale experiments. The unique and powerful combination of laboratory infrastructure and expertise in molecular biology and automation programming, provide flexible resources for a wide range of workflows and research areas. Here, we demonstrate the applicability of biofoundries to molecular microbiology, describing the development and application of automated workflows to identify the genetic basis of growth inhibition of the plant pathogen Streptomyces scabies by a Pseudomonas strain isolated from a potato field. Combining transposon mutagenesis with automated high-throughput antagonistic assays, the workflow accelerated the screening of 2880 mutants to correlate growth inhibition with a biosynthetic gene cluster within 2 weeks.},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/2HHT72U3/Moffat et al. - 2021 - A biofoundry workflow for the identification of ge.pdf;/Users/lukakuma/Zotero/storage/P4SAMH7X/6122745.html}
}

@online{mokadyClipCapCLIPPrefix2021a,
  title = {{{ClipCap}}: {{CLIP Prefix}} for {{Image Captioning}}},
  shorttitle = {{{ClipCap}}},
  author = {Mokady, Ron and Hertz, Amir and Bermano, Amit H.},
  date = {2021-11-18},
  number = {arXiv:2111.09734},
  eprint = {arXiv:2111.09734},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.09734},
  urldate = {2023-02-15},
  abstract = {Image captioning is a fundamental task in vision-language understanding, where the model predicts a textual informative caption to a given input image. In this paper, we present a simple approach to address this task. We use CLIP encoding as a prefix to the caption, by employing a simple mapping network, and then fine-tunes a language model to generate the image captions. The recently proposed CLIP model contains rich semantic features which were trained with textual context, making it best for vision-language perception. Our key idea is that together with a pre-trained language model (GPT2), we obtain a wide understanding of both visual and textual data. Hence, our approach only requires rather quick training to produce a competent captioning model. Without additional annotations or pre-training, it efficiently generates meaningful captions for large-scale and diverse datasets. Surprisingly, our method works well even when only the mapping network is trained, while both CLIP and the language model remain frozen, allowing a lighter architecture with less trainable parameters. Through quantitative evaluation, we demonstrate our model achieves comparable results to state-of-the-art methods on the challenging Conceptual Captions and nocaps datasets, while it is simpler, faster, and lighter. Our code is available in https://github.com/rmokady/CLIP\_prefix\_caption.},
  pubstate = {preprint},
  keywords = {2-adapter,7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/RSMIZJMX/Mokady et al. - 2021 - ClipCap CLIP Prefix for Image Captioning.pdf;/Users/lukakuma/Zotero/storage/FLEH5PKF/2111.html}
}

@unpublished{montiDualPrimalGraphConvolutional2018,
  title = {Dual-{{Primal Graph Convolutional Networks}}},
  author = {Monti, Federico and Shchur, Oleksandr and Bojchevski, Aleksandar and Litany, Or and G\"unnemann, Stephan and Bronstein, Michael M.},
  date = {2018-06-03},
  eprint = {1806.00770},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.00770},
  urldate = {2022-03-28},
  abstract = {In recent years, there has been a surge of interest in developing deep learning methods for non-Euclidean structured data such as graphs. In this paper, we propose Dual-Primal Graph CNN, a graph convolutional architecture that alternates convolution-like operations on the graph and its dual. Our approach allows to learn both vertex- and edge features and generalizes the previous graph attention (GAT) model. We provide extensive experimental validation showing state-of-the-art results on a variety of tasks tested on established graph benchmarks, including CORA and Citeseer citation networks as well as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender systems.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/8RWMFEV3/Monti et al. - 2018 - Dual-Primal Graph Convolutional Networks.pdf;/Users/lukakuma/Zotero/storage/XNIJDJY5/1806.html}
}

@article{morgadoAudioVisualInstanceDiscrimination2020,
  title = {Audio-{{Visual Instance Discrimination}} with {{Cross-Modal Agreement}}},
  author = {Morgado, Pedro and Vasconcelos, Nuno and Misra, Ishan},
  date = {2020-04-27},
  url = {https://arxiv.org/abs/2004.12943v3},
  urldate = {2022-04-25},
  abstract = {We present a self-supervised learning approach to learn audio-visual representations from video and audio. Our method uses contrastive learning for cross-modal discrimination of video from audio and vice-versa. We show that optimizing for cross-modal discrimination, rather than within-modal discrimination, is important to learn good representations from video and audio. With this simple but powerful insight, our method achieves highly competitive performance when finetuned on action recognition tasks. Furthermore, while recent work in contrastive learning defines positive and negative samples as individual instances, we generalize this definition by exploring cross-modal agreement. We group together multiple instances as positives by measuring their similarity in both the video and audio feature spaces. Cross-modal agreement creates better positive and negative sets, which allows us to calibrate visual similarities by seeking within-modal discrimination of positive instances, and achieve significant gains on downstream tasks.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/RSFTWQKH/Morgado et al. - 2020 - Audio-Visual Instance Discrimination with Cross-Mo.pdf;/Users/lukakuma/Zotero/storage/7HHWKRB5/2004.html}
}

@online{morganDeepDiveGoogle2022,
  title = {Deep {{Dive On Google}}'s {{Exascale TPUv4 AI Systems}}},
  author = {Morgan, Timothy Prickett},
  date = {2022-10-11T16:36:44+00:00},
  url = {https://www.nextplatform.com/2022/10/11/deep-dive-on-googles-exascale-tpuv4-ai-systems/},
  urldate = {2022-10-13},
  abstract = {It seems like we have been talking about Google's TPUv4 machine learning accelerators for a long time, and that is because we have been. And today, at the},
  langid = {american},
  organization = {{The Next Platform}},
  file = {/Users/lukakuma/Zotero/storage/923QEGTI/deep-dive-on-googles-exascale-tpuv4-ai-systems.html}
}

@online{MosaicLLMsPart2022,
  title = {Mosaic {{LLMs}} ({{Part}} 1): {{Billion-Parameter GPT Training Made Easy}}},
  shorttitle = {Mosaic {{LLMs}} ({{Part}} 1)},
  date = {2022-08-11},
  url = {https://www.mosaicml.com/blog/billion-parameter-gpt-training-made-easy},
  urldate = {2022-08-18},
  abstract = {In Part 1 of this LLM blog post series, we use the MosaicML platform to train vanilla GPT-3 models up to 1.3B params, and show how to cut training times down to hours with strong multi-node scaling. We also discover that larger models can train more efficiently than smaller models on modern hardware, and that a 10x in parameter count may only result in \textasciitilde 5x the training time.},
  file = {/Users/lukakuma/Zotero/storage/BKF5JMFR/billion-parameter-gpt-training-made-easy.html}
}

@article{moserPlaceCellsGrid2015,
  title = {Place {{Cells}}, {{Grid Cells}}, and {{Memory}}},
  author = {Moser, May-Britt and Rowland, David C. and Moser, Edvard I.},
  date = {2015-02},
  journaltitle = {Cold Spring Harbor Perspectives in Biology},
  shortjournal = {Cold Spring Harb Perspect Biol},
  volume = {7},
  number = {2},
  eprint = {25646382},
  eprinttype = {pmid},
  pages = {a021808},
  issn = {1943-0264},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4315928/},
  urldate = {2022-06-18},
  abstract = {The hippocampal system is critical for storage and retrieval of declarative memories, including memories for locations and events that take place at those locations. Spatial memories place high demands on capacity. Memories must be distinct to be recalled without interference and encoding must be fast. Recent studies have indicated that hippocampal networks allow for fast storage of large quantities of uncorrelated spatial information. The aim of the this article is to review and discuss some of this work, taking as a starting point the discovery of multiple functionally specialized cell types of the hippocampal\textendash entorhinal circuit, such as place, grid, and border cells. We will show that grid cells provide the hippocampus with a metric, as well as a putative mechanism for decorrelation of representations, that the formation of environment-specific place maps depends on mechanisms for long-term plasticity in the hippocampus, and that long-term spatiotemporal memory storage may depend on offline consolidation processes related to sharp-wave ripple activity in the hippocampus. The multitude of representations generated through interactions between a variety of functionally specialized cell types in the entorhinal\textendash hippocampal circuit may be at the heart of the mechanism for declarative memory formation., Hippocampal networks allow for fast storage of large quantities of uncorrelated spatial information. Functionally specialized cell types (e.g., place cells and grid cells) may form the basis of this system.},
  pmcid = {PMC4315928},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/N6CIB7RC/Moser et al. - 2015 - Place Cells, Grid Cells, and Memory.pdf}
}

@video{mrclaboratoryofmolecularbiologyKendrewLecture20212021,
  title = {Kendrew {{Lecture}} 2021 Pt2 - {{Highly}} Accurate Protein Structure Prediction with {{AlphaFold}} - {{John Jumper}}},
  editor = {{MRC Laboratory of Molecular Biology}},
  date = {2021-10-20},
  url = {https://www.youtube.com/watch?v=jTO6odQNp90},
  urldate = {2022-05-31},
  abstract = {MRC Laboratory of Molecular Biology John Kendrew Lecture  2021 part 2 Highly accurate protein structure prediction with AlphaFold Speaker: John Jumper, AlphaFold lead, DeepMind. Abstract: Predicting a protein's structure from its primary sequence has been a grand challenge in biology for the past 50 years, holding the promise to bridge the gap between the pace of genomics discovery and resulting structural characterization. In this talk, we will describe work at DeepMind to develop AlphaFold, a new deep learning-based system for structure prediction that achieves high accuracy across a wide range of targets. We demonstrated our system in the 14th biennial Critical Assessment of Protein Structure Prediction (CASP14) across a wide range of difficult targets, where the assessors judged our predictions to be at an accuracy ``competitive with experiment'' for approximately 2/3rds of proteins. The talk will cover both the underlying machine learning ideas and the implications for biological research. Read more about the lecture and David's work here:  https://www2.mrc-lmb.cam.ac.uk/john-k... - About the MRC Laboratory of Molecular Biology (LMB): The LMB is one of the world's leading research institutes. Discoveries and inventions developed at the LMB, for example DNA sequencing and methods to determine the structure of proteins, have revolutionised all areas of biology. Its scientists work to advance understanding of biological processes at the molecular level. This information will help us to understand the workings of complex systems, such as the immune system and the brain, and solve key problems in human health. More links: Official Site: https://www2.mrc-lmb.cam.ac.uk/ Facebook: https://www.facebook.com/MRC.LMB Twitter: https://twitter.com/MRC\_LMB Instagram: https://www.instagram.com/mrc\_lmb/ LinkedIn: https://www.linkedin.com/company/mrc-... Click here to subscribe to MRC Laboratory of Molecular Biology on YouTube: https://www.youtube.com/user/LMBCambr...},
  editortype = {director},
  keywords = {DeepMind,read}
}

@video{mrclaboratoryofmolecularbiologyKendrewLecture20212021a,
  title = {Kendrew {{Lecture}} 2021 Pt1 - {{Using AI}} to Accelerate Scientific Discovery - {{Demis Hassabis}}},
  editor = {{MRC Laboratory of Molecular Biology}},
  date = {2021-10-20},
  url = {https://www.youtube.com/watch?v=sm-VkgVX-2o},
  urldate = {2022-06-03},
  abstract = {MRC Laboratory of Molecular Biology John Kendrew Lecture  2021 part 1 Using AI to accelerate scientific discovery Speaker: Demis Hassabis, Founder and CEO, DeepMind. Abstract: The past decade has seen incredible advances in the field of Artificial Intelligence (AI). DeepMind has been in the vanguard of many of these big breakthroughs, pioneering the development of self-learning systems like AlphaGo, the first program to beat the world champion at the complex game of Go. Games have proven to be a great training ground for developing and testing AI algorithms, but the aim at DeepMind has always been to build general learning systems ultimately capable of solving important problems in the real world. Excitingly, I believe we are on the cusp of a new era in science with AI poised to be a powerful tool for accelerating scientific discovery itself. We recently demonstrated this potential with our AlphaFold system, a solution to the 50-year grand challenge of protein structure prediction, culminating in the release of the most accurate and complete picture of the human proteome. Read more about the lecture and David's work here: https://www2.mrc-lmb.cam.ac.uk/john-k... - About the MRC Laboratory of Molecular Biology (LMB): The LMB is one of the world's leading research institutes. Discoveries and inventions developed at the LMB, for example DNA sequencing and methods to determine the structure of proteins, have revolutionised all areas of biology. Its scientists work to advance understanding of biological processes at the molecular level. This information will help us to understand the workings of complex systems, such as the immune system and the brain, and solve key problems in human health. More links: Official Site: https://www2.mrc-lmb.cam.ac.uk/ Facebook: https://www.facebook.com/MRC.LMB Twitter: https://twitter.com/MRC\_LMB Instagram: https://www.instagram.com/mrc\_lmb/ LinkedIn: https://www.linkedin.com/company/mrc-... Click here to subscribe to MRC Laboratory of Molecular Biology on YouTube: https://www.youtube.com/user/LMBCambr...},
  editortype = {director},
  keywords = {read}
}

@online{muennighoffCrosslingualGeneralizationMultitask2022,
  title = {Crosslingual {{Generalization}} through {{Multitask Finetuning}}},
  author = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M. Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Albert and Raff, Edward and Raffel, Colin},
  date = {2022-11-03},
  number = {arXiv:2211.01786},
  eprint = {arXiv:2211.01786},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.01786},
  urldate = {2022-12-08},
  abstract = {Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are publicly available at https://github.com/bigscience-workshop/xmtf.},
  pubstate = {preprint},
  keywords = {finetuning,multilingual,read},
  file = {/Users/lukakuma/Zotero/storage/5WVZTZIT/Muennighoff et al. - 2022 - Crosslingual Generalization through Multitask Fine.pdf;/Users/lukakuma/Zotero/storage/DMZAMWT7/2211.html}
}

@article{muennighoffMTEBMassiveText2022,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo\"ic and Reimers, Nils},
  date = {2022-10-13},
  url = {https://arxiv.org/abs/2210.07316v1},
  urldate = {2023-01-28},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 56 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://huggingface.co/spaces/mteb/leaderboard.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/UF6X859E/Muennighoff et al. - 2022 - MTEB Massive Text Embedding Benchmark.pdf}
}

@article{mullardWhatDoesAlphaFold2021,
  title = {What Does {{AlphaFold}} Mean for Drug Discovery?},
  author = {Mullard, Asher},
  date = {2021-09-14},
  journaltitle = {Nature Reviews Drug Discovery},
  volume = {20},
  number = {10},
  pages = {725--727},
  publisher = {{Nature Publishing Group}},
  url = {https://www.nature.com/articles/d41573-021-00161-0},
  urldate = {2022-05-26},
  abstract = {AlphaFold and RoseTTAFold have delivered a revolutionary advance for protein structure predictions, but the implications for drug discovery are more incremental. For now.},
  issue = {10},
  langid = {english},
  keywords = {DeepMind},
  annotation = {Bandiera\_abtest: a Cg\_type: News},
  file = {/Users/lukakuma/Zotero/storage/4ZGT4KY2/Mullard - 2021 - What does AlphaFold mean for drug discovery.pdf;/Users/lukakuma/Zotero/storage/7HBZ6KBW/d41573-021-00161-0.html}
}

@article{mullingLearningSelectGeneralize2013,
  title = {Learning to Select and Generalize Striking Movements in Robot Table Tennis},
  author = {M\"ulling, Katharina and Kober, Jens and Kroemer, Oliver and Peters, Jan},
  date = {2013-03},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {32},
  number = {3},
  pages = {263--279},
  issn = {0278-3649, 1741-3176},
  url = {http://journals.sagepub.com/doi/10.1177/0278364912472380},
  urldate = {2022-07-21},
  abstract = {Learning new motor tasks autonomously from interaction with a human being is an important goal for both robotics and machine learning. However, when moving beyond basic skills, most monolithic machine learning approaches fail to scale. In this paper, we take the task of learning table tennis as an example and present a new framework which allows a robot to learn cooperative table tennis from interaction with a human. Therefore, the robot first learns a set of elementary table tennis hitting movements from a human teacher by kinesthetic teach-in, which is compiled into a set of dynamical system motor primitives (DMPs). Subsequently, the system generalizes these movements to a wider range of situations using our mixture of motor primitives (MoMP) approach. The resulting policy enables the robot to select appropriate motor primitives as well as to generalize between them. Finally, the robot plays with a human table tennis partner and learns online to improve its behavior.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/5394QXB3/M√ºlling et al. - 2013 - Learning to select and generalize striking movemen.pdf}
}

@book{murphyProbabilisticMachineLearning2022,
  title = {Probabilistic {{Machine Learning}}},
  shorttitle = {Probabilistic {{Machine Learning}}},
  author = {Murphy, Kevin P.},
  date = {2022-03-01},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-04682-4},
  langid = {english},
  pagetotal = {864},
  file = {/Users/lukakuma/Zotero/storage/D8U6LJTT/book1.pdf;/Users/lukakuma/Zotero/storage/MRBY4BDL/solution-book1.pdf;/Users/lukakuma/Zotero/storage/T445T97L/book2.pdf;/Users/lukakuma/Zotero/storage/VBPZFVTD/supp-book2.pdf}
}

@article{nadiradzeSwarmSGDScalableDecentralized2020,
  title = {{{SwarmSGD}}: {{Scalable Decentralized SGD}} with {{Local Updates}}.},
  shorttitle = {{{SwarmSGD}}},
  author = {Nadiradze, G. and Sabour, Amirmojtaba and Alistarh, Dan and Sharma, Aditya and Markov, Ilia and Aksenov, V.},
  date = {2020},
  journaltitle = {undefined},
  url = {https://www.semanticscholar.org/paper/SwarmSGD%3A-Scalable-Decentralized-SGD-with-Local-Nadiradze-Sabour/d38429653f2d0f91e1cd4fa00f0c1dd1fe99d092},
  urldate = {2022-04-14},
  abstract = {This paper instantiates SGD onto a supercomputing environment, and shows that it can successfully converge and scale for large-scale image classification models, matching or even slightly improving the accuracy of the baseline parallel variants. The ability to scale distributed optimization to large node counts has been one of the main enablers of recent progress in machine learning. To this end, several techniques have been explored, such as asynchronous and decentralized execution--which significantly reduce the impact of communication and synchronization, and the ability for nodes to perform several local model updates before communicating--which reduces the frequency of communication. In this paper, we show that these techniques, which have so far been considered independently, can be jointly leveraged to obtain near-perfect scalability for training neural network models via stochastic gradient descent (SGD). We consider a setting with minimal coordination: we have a large number of nodes on a communication graph, each with a local subset of data, performing independent SGD updates onto their local models. After some number of local updates, each node chooses an interaction partner uniformly at random from its neighbors, and averages its local model with the neighbor\&\#39;s model. Our first contribution is in proving that, even under such a relaxed setting, SGD can still be guaranteed to converge to local minima under standard assumptions. The proof improves existing techniques by jointly handling decentralization, asynchrony, and local updates, and by bounding their impact. On the practical side, we instantiate this algorithm onto a supercomputing environment, and show that it can successfully converge and scale for large-scale image classification models, matching or even slightly improving the accuracy of the baseline parallel variants.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/6P5IRI2R/Nadiradze et al. - 2020 - SwarmSGD Scalable Decentralized SGD with Local Up.pdf;/Users/lukakuma/Zotero/storage/UUH2IDS7/d38429653f2d0f91e1cd4fa00f0c1dd1fe99d092.html}
}

@online{nagraniAttentionBottlenecksMultimodal2022,
  title = {Attention {{Bottlenecks}} for {{Multimodal Fusion}}},
  author = {Nagrani, Arsha and Yang, Shan and Arnab, Anurag and Jansen, Aren and Schmid, Cordelia and Sun, Chen},
  date = {2022-11-30},
  number = {arXiv:2107.00135},
  eprint = {arXiv:2107.00135},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.00135},
  urldate = {2023-01-19},
  abstract = {Humans perceive the world by concurrently processing and fusing high-dimensional inputs from multiple modalities such as vision and audio. Machine perception models, in stark contrast, are typically modality-specific and optimised for unimodal benchmarks, and hence late-stage fusion of final representations or predictions from each modality (`late-fusion') is still a dominant paradigm for multimodal video classification. Instead, we introduce a novel transformer based architecture that uses `fusion bottlenecks' for modality fusion at multiple layers. Compared to traditional pairwise self-attention, our model forces information between different modalities to pass through a small number of bottleneck latents, requiring the model to collate and condense the most relevant information in each modality and only share what is necessary. We find that such a strategy improves fusion performance, at the same time reducing computational cost. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple audio-visual classification benchmarks including Audioset, Epic-Kitchens and VGGSound. All code and models will be released.},
  pubstate = {preprint},
  keywords = {2-adapter,7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/GBL844L8/Nagrani et al. - 2022 - Attention Bottlenecks for Multimodal Fusion.pdf;/Users/lukakuma/Zotero/storage/BRTEUWUR/2107.html}
}

@article{nairAWACAcceleratingOnline2021,
  title = {{{AWAC}}: {{Accelerating Online Reinforcement Learning}} with {{Offline Datasets}}},
  shorttitle = {{{AWAC}}},
  author = {Nair, Ashvin and Dalal, Murtaza and Gupta, Abhishek and Levine, Sergey},
  date = {2021-03-05},
  url = {https://openreview.net/forum?id=OJiM1R3jAtZ},
  urldate = {2022-07-25},
  abstract = {Reinforcement learning provides an appealing formalism for learning control policies from experience. However, the classic active formulation of reinforcement learning necessitates a lengthy active...},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/YUSFT25L/Nair et al. - 2021 - AWAC Accelerating Online Reinforcement Learning w.pdf;/Users/lukakuma/Zotero/storage/KHG4T42Z/forum.html}
}

@online{nakanoWebGPTBrowserassistedQuestionanswering2022,
  title = {{{WebGPT}}: {{Browser-assisted}} Question-Answering with Human Feedback},
  shorttitle = {{{WebGPT}}},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  date = {2022-06-01},
  number = {arXiv:2112.09332},
  eprint = {arXiv:2112.09332},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.09332},
  urldate = {2022-09-20},
  abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
  pubstate = {preprint},
  keywords = {*general tools,4-document retrieval,alignment,multiagent,OpenAI,QA,read,RLHF},
  file = {/Users/lukakuma/Zotero/storage/XVD4TXD6/Nakano et al. - 2022 - WebGPT Browser-assisted question-answering with h.pdf;/Users/lukakuma/Zotero/storage/ISNGMR9N/2112.html;/Users/lukakuma/Zotero/storage/ZCUICQPE/2112.html}
}

@unpublished{nakkiranDeepDoubleDescent2019,
  title = {Deep {{Double Descent}}: {{Where Bigger Models}} and {{More Data Hurt}}},
  shorttitle = {Deep {{Double Descent}}},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  date = {2019-12-04},
  number = {arXiv:1912.02292},
  eprint = {1912.02292},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1912.02292},
  urldate = {2022-06-12},
  abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  file = {/Users/lukakuma/Zotero/storage/6GTJW7ZA/Nakkiran et al. - 2019 - Deep Double Descent Where Bigger Models and More .pdf;/Users/lukakuma/Zotero/storage/MY4I2KVR/1912.html}
}

@unpublished{narangFactoryFastContact2022,
  title = {Factory: {{Fast Contact}} for {{Robotic Assembly}}},
  shorttitle = {Factory},
  author = {Narang, Yashraj and Storey, Kier and Akinola, Iretiayo and Macklin, Miles and Reist, Philipp and Wawrzyniak, Lukasz and Guo, Yunrong and Moravanszky, Adam and State, Gavriel and Lu, Michelle and Handa, Ankur and Fox, Dieter},
  date = {2022-05-06},
  eprint = {2205.03532},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.03532},
  urldate = {2022-05-20},
  abstract = {Robotic assembly is one of the oldest and most challenging applications of robotics. In other areas of robotics, such as perception and grasping, simulation has rapidly accelerated research progress, particularly when combined with modern deep learning. However, accurately, efficiently, and robustly simulating the range of contact-rich interactions in assembly remains a longstanding challenge. In this work, we present Factory, a set of physics simulation methods and robot learning tools for such applications. We achieve real-time or faster simulation of a wide range of contact-rich scenes, including simultaneous simulation of 1000 nut-and-bolt interactions. We provide \$60\$ carefully-designed part models, 3 robotic assembly environments, and 7 robot controllers for training and testing virtual robots. Finally, we train and evaluate proof-of-concept reinforcement learning policies for nut-and-bolt assembly. We aim for Factory to open the doors to using simulation for robotic assembly, as well as many other contact-rich applications in robotics. Please see https://sites.google.com/nvidia.com/factory for supplementary content, including videos.},
  file = {/Users/lukakuma/Zotero/storage/N2UE37UL/Narang et al. - 2022 - Factory Fast Contact for Robotic Assembly.pdf}
}

@online{narayananEfficientLargeScaleLanguage2021,
  title = {Efficient {{Large-Scale Language Model Training}} on {{GPU Clusters Using Megatron-LM}}},
  author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
  date = {2021-08-23},
  number = {arXiv:2104.04473},
  eprint = {arXiv:2104.04473},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.04473},
  urldate = {2022-08-09},
  abstract = {Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress. In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+\% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52\% of theoretical peak. Our code is open sourced at https://github.com/nvidia/megatron-lm.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/JNMX4MPG/Narayanan et al. - 2021 - Efficient Large-Scale Language Model Training on G.pdf;/Users/lukakuma/Zotero/storage/546PW6EW/2104.html}
}

@online{NativeInternetProtocol,
  title = {A Native Internet Protocol for Social Media},
  author = {Dorsey, Jack},
  date = {2022-12-13},
  url = {https://www.getrevue.co/profile/jackjack/issues/a-native-internet-protocol-for-social-media-1503112},
  urldate = {2022-12-17},
  abstract = {There's a lot of conversation around the \#TwitterFiles. Here's my take, and thoughts on how to fix the issues identified. I'll start with the principles I've come to believe\ldots based on everything I've learned and experienced through my past actions as a Twitter co-founder and lead:Social media must be resilient to corporate and government control.Only the original author may remove content they produce.Moderation is best implemented by algorithmic choice.The Twitter when I led it and the Twitter \ldots},
  file = {/Users/lukakuma/Zotero/storage/JAEZTBSZ/a-native-internet-protocol-for-social-media-1503112.html}
}

@online{nayLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Fiduciaries}}: {{A Case Study Toward Robustly Communicating With Artificial Intelligence Through Legal Standards}}},
  shorttitle = {Large {{Language Models}} as {{Fiduciaries}}},
  author = {Nay, John J.},
  date = {2023-01-30},
  number = {arXiv:2301.10095},
  eprint = {arXiv:2301.10095},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.10095},
  urldate = {2023-02-02},
  abstract = {Artificial Intelligence (AI) is taking on increasingly autonomous roles, e.g., browsing the web as a research assistant and managing money. But specifying goals and restrictions for AI behavior is difficult. Similar to how parties to a legal contract cannot foresee every potential "if-then" contingency of their future relationship, we cannot specify desired AI behavior for all circumstances. Legal standards facilitate robust communication of inherently vague and underspecified goals. Instructions (in the case of language models, "prompts") that employ legal standards will allow AI agents to develop shared understandings of the spirit of a directive that generalize expectations regarding acceptable actions to take in unspecified states of the world. Standards have built-in context that is lacking from other goal specification languages, such as plain language and programming languages. Through an empirical study on thousands of evaluation labels we constructed from U.S. court opinions, we demonstrate that large language models (LLMs) are beginning to exhibit an "understanding" of one of the most relevant legal standards for AI agents: fiduciary obligations. Performance comparisons across models suggest that, as LLMs continue to exhibit improved core capabilities, their legal standards understanding will also continue to improve. OpenAI's latest LLM has 78\% accuracy on our data, their previous release has 73\% accuracy, and a model from their 2020 GPT-3 paper has 27\% accuracy (worse than random). Our research is an initial step toward a framework for evaluating AI understanding of legal standards more broadly, and for conducting reinforcement learning with legal feedback (RLLF).},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/Y2NECJ7B/Nay - 2023 - Large Language Models as Fiduciaries A Case Study.pdf;/Users/lukakuma/Zotero/storage/BY92QSXI/2301.html}
}

@online{neelakantanTextCodeEmbeddings2022,
  title = {Text and {{Code Embeddings}} by {{Contrastive Pre-Training}}},
  author = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and Heidecke, Johannes and Shyam, Pranav and Power, Boris and Nekoul, Tyna Eloundou and Sastry, Girish and Krueger, Gretchen and Schnurr, David and Such, Felipe Petroski and Hsu, Kenny and Thompson, Madeleine and Khan, Tabarak and Sherbakov, Toki and Jang, Joanne and Welinder, Peter and Weng, Lilian},
  date = {2022-01-24},
  number = {arXiv:2201.10005},
  eprint = {arXiv:2201.10005},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.10005},
  urldate = {2022-11-19},
  abstract = {Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4\% and 1.8\% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4\%, 14.7\%, and 10.6\% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8\% relative improvement over prior best work on code search.},
  pubstate = {preprint},
  keywords = {code},
  file = {/Users/lukakuma/Zotero/storage/A4E7YMW5/Neelakantan et al. - 2022 - Text and Code Embeddings by Contrastive Pre-Traini.pdf;/Users/lukakuma/Zotero/storage/2IGHX5NE/2201.html}
}

@online{ngoAlignmentProblemDeep2022a,
  title = {The Alignment Problem from a Deep Learning Perspective},
  author = {Ngo, Richard and Chan, Lawrence and Mindermann, S\"oren},
  date = {2022-12-16},
  number = {arXiv:2209.00626},
  eprint = {arXiv:2209.00626},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.00626},
  urldate = {2023-02-14},
  abstract = {Within the coming decades, artificial general intelligence (AGI) may surpass human capabilities at a wide range of important tasks. We outline a case for expecting that, without substantial effort to prevent it, AGIs could learn to pursue goals which are very undesirable (in other words, misaligned) from a human perspective. We argue that AGIs trained in similar ways as today's most capable models could learn to act deceptively to receive higher reward; learn internally-represented goals which generalize beyond their training distributions; and pursue those goals using power-seeking strategies. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing these problems.},
  pubstate = {preprint},
  keywords = {alignment,OpenAI},
  file = {/Users/lukakuma/Zotero/storage/LFHBZ8QR/Ngo et al. - 2022 - The alignment problem from a deep learning perspec.pdf;/Users/lukakuma/Zotero/storage/M9IXTN25/2209.html}
}

@inproceedings{nicaEvaluatingGeneralizationGFlowNets2022,
  title = {Evaluating {{Generalization}} in {{GFlowNets}} for {{Molecule Design}}},
  author = {Nica, Andrei Cristian and Jain, Moksh and Bengio, Emmanuel and Liu, Cheng-Hao and Korablyov, Maksym and Bronstein, Michael M. and Bengio, Yoshua},
  date = {2022-03-31},
  url = {https://openreview.net/forum?id=JFSaHKNZ35b},
  urldate = {2022-06-03},
  abstract = {Evaluating generalization in GFlowNets for the task of Molecule Design and analysing the metrics which are predictive of the desired search performance (i.e. finding high-scoring diverse candidates).},
  eventtitle = {{{ICLR2022 Machine Learning}} for {{Drug Discovery}}},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/XUJYYQZV/Nica et al. - 2022 - Evaluating Generalization in GFlowNets for Molecule Design.pdf}
}

@online{nicholFirstOrderMetaLearningAlgorithms2018,
  title = {On {{First-Order Meta-Learning Algorithms}}},
  author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
  date = {2018-10-22},
  number = {arXiv:1803.02999},
  eprint = {arXiv:1803.02999},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.02999},
  urldate = {2022-07-23},
  abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/FUADI2PP/Nichol et al. - 2018 - On First-Order Meta-Learning Algorithms.pdf;/Users/lukakuma/Zotero/storage/QMKJV2NX/1803.html}
}

@unpublished{nicholGLIDEPhotorealisticImage2022,
  title = {{{GLIDE}}: {{Towards Photorealistic Image Generation}} and {{Editing}} with {{Text-Guided Diffusion Models}}},
  shorttitle = {{{GLIDE}}},
  author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  date = {2022-03-08},
  eprint = {2112.10741},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.10741},
  urldate = {2022-04-27},
  abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
  keywords = {*diffusion,image generation},
  file = {/Users/lukakuma/Zotero/storage/2WDRUQJX/Nichol et al. - 2022 - GLIDE Towards Photorealistic Image Generation and.pdf;/Users/lukakuma/Zotero/storage/EPYVQDGN/2112.html}
}

@online{nicholGottaLearnFast2018,
  title = {Gotta {{Learn Fast}}: {{A New Benchmark}} for {{Generalization}} in {{RL}}},
  shorttitle = {Gotta {{Learn Fast}}},
  author = {Nichol, Alex and Pfau, Vicki and Hesse, Christopher and Klimov, Oleg and Schulman, John},
  date = {2018-04-23},
  number = {arXiv:1804.03720},
  eprint = {arXiv:1804.03720},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.03720},
  urldate = {2022-07-23},
  abstract = {In this report, we present a new reinforcement learning (RL) benchmark based on the Sonic the Hedgehog (TM) video game franchise. This benchmark is intended to measure the performance of transfer learning and few-shot learning algorithms in the RL domain. We also present and evaluate some baseline algorithms on the new benchmark.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/GVXR5I3N/Nichol et al. - 2018 - Gotta Learn Fast A New Benchmark for Generalizati.pdf;/Users/lukakuma/Zotero/storage/ISV9RJNC/1804.html}
}

@online{nielsenMapsMatter2021,
  title = {Maps of {{Matter}}},
  author = {Nielsen, Michael},
  date = {2021},
  url = {https://futureofmatter.com/index.html},
  urldate = {2022-06-01},
  file = {/Users/lukakuma/Zotero/storage/34SJ376P/maps_of_matter.html}
}

@online{nijkampCodeGenOpenLarge2022,
  title = {{{CodeGen}}: {{An Open Large Language Model}} for {{Code}} with {{Multi-Turn Program Synthesis}}},
  shorttitle = {{{CodeGen}}},
  author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  date = {2022-09-29},
  number = {arXiv:2203.13474},
  eprint = {arXiv:2203.13474},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.13474},
  urldate = {2022-10-31},
  abstract = {Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.},
  pubstate = {preprint},
  keywords = {code},
  file = {/Users/lukakuma/Zotero/storage/9JH66NVU/Nijkamp et al. - 2022 - A Conversational Paradigm for Program Synthesis.pdf;/Users/lukakuma/Zotero/storage/P2YPWVSZ/Nijkamp et al. - 2022 - CodeGen An Open Large Language Model for Code wit.pdf;/Users/lukakuma/Zotero/storage/2W5YKTUY/2203.html}
}

@online{nijkampProGen2ExploringBoundaries2022,
  title = {{{ProGen2}}: {{Exploring}} the {{Boundaries}} of {{Protein Language Models}}},
  shorttitle = {{{ProGen2}}},
  author = {Nijkamp, Erik and Ruffolo, Jeffrey and Weinstein, Eli N. and Naik, Nikhil and Madani, Ali},
  date = {2022-06-27},
  number = {arXiv:2206.13517},
  eprint = {arXiv:2206.13517},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.13517},
  urldate = {2022-06-30},
  abstract = {Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence-driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/salesforce/progen.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/WXDI3QMA/Nijkamp et al. - 2022 - ProGen2 Exploring the Boundaries of Protein Langu.pdf;/Users/lukakuma/Zotero/storage/JXSV62SU/2206.html}
}

@unpublished{nikolenkoSyntheticDataDeep2019,
  title = {Synthetic {{Data}} for {{Deep Learning}}},
  author = {Nikolenko, Sergey I.},
  date = {2019-09-25},
  eprint = {1909.11512},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1909.11512},
  urldate = {2022-04-21},
  abstract = {Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including synthetic-to-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies.},
  file = {/Users/lukakuma/Zotero/storage/MEWLERKF/Nikolenko - 2019 - Synthetic Data for Deep Learning.pdf;/Users/lukakuma/Zotero/storage/GPUQQS8F/1909.html}
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  date = {2006},
  series = {Springer Series in Operations Research},
  edition = {2nd ed},
  publisher = {{Springer}},
  location = {{New York}},
  isbn = {978-0-387-30303-1},
  langid = {english},
  pagetotal = {664},
  annotation = {OCLC: ocm68629100},
  file = {/Users/lukakuma/Zotero/storage/98GMV56F/Nocedal and Wright - 2006 - Numerical optimization.pdf}
}

@article{noyExperimentalEvidenceProductivity2023,
  title = {Experimental {{Evidence}} on the {{Productivity Effects}} of {{Generative Artificial Intelligence}}},
  author = {Noy, Shakked and Zhang, Whitney},
  date = {2023-03-02},
  abstract = {We examine the productivity effects of a generative artificial intelligence technology\textemdash the assistive chatbot ChatGPT\textemdash in the context of mid-level professional writing tasks. In a preregistered online experiment, we assign occupation-specific, incentivized writing tasks to 444 college-educated professionals, and randomly expose half of them to ChatGPT. Our results show that ChatGPT substantially raises average productivity: time taken decreases by 0.8 SDs and output quality rises by 0.4 SDs. Inequality between workers decreases, as ChatGPT compresses the productivity distribution by benefiting low-ability workers more. ChatGPT mostly substitutes for worker effort rather than complementing worker skills, and restructures tasks towards idea-generation and editing and away from rough-drafting. Exposure to ChatGPT increases job satisfaction and self-efficacy and heightens both concern and excitement about automation technologies.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/FY59HDRX/Noy and Zhang - Experimental Evidence on the Productivity Effects .pdf}
}

@video{nvidiaGTC2022Keynote2022,
  title = {{{GTC}} 2022 {{Keynote}} with {{NVIDIA CEO Jensen Huang}}},
  editor = {{NVIDIA}},
  date = {2022-03-22},
  url = {https://www.youtube.com/watch?v=39ubNuxnrK8},
  urldate = {2022-03-26},
  abstract = {NVIDIA CEO Jensen Huang kicks off \#GTC22 with a keynote that presents the latest breakthroughs in \#AI, data science, high performance computing, graphics, edge computing, networking, and autonomous machines. Deep dive into announcements and discover more content at https://www.nvidia.com/gtc 00:00 Fly-Through NVIDIA's Campus 02:02 I AM AI 05:24 The Dynamics Shaping Our Industry 09:13 Climate Science and FourCastNet 12:52 NVIDIA AI Suite of Libraries 27:02 NVIDIA H100, Hopper and Grace 44:58 Million X Speedups 51:04 Omniverse Digital Twins 1:03:19 NVIDIA Robotic Platforms: DRIVE, Clara Holoscan, Isaac 1:31:21 Summary 1:37:50 Closing Video Follow NVIDIA on Twitter: https://twitter.com/NVIDIAGTC https://twitter.com/NVIDIA},
  editortype = {director}
}

@video{nvidiaGTCSept20222022,
  title = {{{GTC Sept}} 2022 {{Keynote}} with {{NVIDIA CEO Jensen Huang}}},
  editor = {{NVIDIA}},
  date = {2022-09-20},
  url = {https://www.youtube.com/watch?v=PWcNlRI00jo},
  urldate = {2022-09-21},
  editortype = {director}
}

@online{nyeShowYourWork2021,
  title = {Show {{Your Work}}: {{Scratchpads}} for {{Intermediate Computation}} with {{Language Models}}},
  shorttitle = {Show {{Your Work}}},
  author = {Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
  date = {2021-11-30},
  number = {arXiv:2112.00114},
  eprint = {arXiv:2112.00114},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.00114},
  urldate = {2023-02-20},
  abstract = {Large pre-trained language models perform remarkably well on tasks that can be done "in one pass", such as generating realistic text or synthesizing computer programs. However, they struggle with tasks that require unbounded multi-step computation, such as adding integers or executing programs. Surprisingly, we find that these same models are able to perform complex multi-step computations -- even in the few-shot regime -- when asked to perform the operation "step by step", showing the results of intermediate computations. In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a "scratchpad". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/AH6SHITC/Nye et al. - 2021 - Show Your Work Scratchpads for Intermediate Compu.pdf;/Users/lukakuma/Zotero/storage/IRDRMMSI/2112.html}
}

@article{obadiaUnityStrengthFormalization2021,
  title = {Unity Is {{Strength}}: {{A Formalization}} of {{Cross-Domain Maximal Extractable Value}}},
  shorttitle = {Unity Is {{Strength}}},
  author = {Obadia, Alexandre and Salles, Alejo and Sankar, Lakshman and Chitra, Tarun and Chellani, Vaibhav and Daian, Philip},
  date = {2021-12-02},
  url = {https://arxiv.org/abs/2112.01472v2},
  urldate = {2022-03-15},
  abstract = {The multi-chain future is upon us. Modular architectures are coming to maturity across the ecosystem to scale bandwidth and throughput of cryptocurrency. One example of such is the Ethereum modular architecture, with its beacon chain, its execution chain, its Layer 2s, and soon its shards. These can all be thought as separate blockchains, heavily inter-connected with one another, and together forming an ecosystem. In this work, we call each of these interconnected blockchains "domains", and study the manifestation of Maximal Extractable Value (MEV, a generalization of "Miner Extractable Value") across them. In other words, we investigate whether there exists extractable value that depends on the ordering of transactions in two or more domains jointly. We first recall the definitions of Extractable and Maximal Extractable Value, before introducing a definition of Cross-Domain Maximal Extractable Value. We find that Cross-Domain MEV can be used to measure the incentive for transaction sequencers in different domains to collude with one another, and study the scenarios in which there exists such an incentive. We end the work with a list of negative externalities that might arise from cross-domain MEV extraction and lay out several open questions. We note that the formalism in this work is a work in progress, and we hope that it can serve as the basis for formal analysis tools in the style of those presented in Clockwork Finance, as well as for discussion on how to mitigate the upcoming negative externalities of substantial cross-domain MEV.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/PC4BDRN7/Obadia et al. - 2021 - Unity is Strength A Formalization of Cross-Domain.pdf;/Users/lukakuma/Zotero/storage/KSJQZVDZ/2112.html}
}

@unpublished{oguejiIntriguingPropertiesCompression2022,
  title = {Intriguing {{Properties}} of {{Compression}} on {{Multilingual Models}}},
  author = {Ogueji, Kelechi and Ahia, Orevaoghene and Onilude, Gbemileke and Gehrmann, Sebastian and Hooker, Sara and Kreutzer, Julia},
  date = {2022},
  eprint = {2211.02738},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.02738},
  urldate = {2022-12-17},
  abstract = {Multilingual models are often particularly dependent on scaling to generalize to a growing number of languages. Compression techniques are widely relied upon to reconcile the growth in model size with real world resource constraints, but compression can have a disparate effect on model performance for low-resource languages. It is thus crucial to understand the trade-offs between scale, multilingualism, and compression. In this work, we propose an experimental framework to characterize the impact of sparsifying multilingual pre-trained language models during fine-tuning. Applying this framework to mBERT named entity recognition models across 40 languages, we find that compression confers several intriguing and previously unknown generalization properties. In contrast to prior findings, we find that compression may improve model robustness over dense models. We additionally observe that under certain sparsification regimes compression may aid, rather than disproportionately impact the performance of low-resource languages.},
  keywords = {multilingual},
  file = {/Users/lukakuma/Zotero/storage/SS4L2DDT/Ogueji et al. - 2022 - Intriguing Properties of Compression on Multilingu.pdf;/Users/lukakuma/Zotero/storage/SMTBDH5V/2211.html}
}

@unpublished{ohDiscoveringReinforcementLearning2021,
  title = {Discovering {{Reinforcement Learning Algorithms}}},
  author = {Oh, Junhyuk and Hessel, Matteo and Czarnecki, Wojciech M. and Xu, Zhongwen and family=Hasselt, given=Hado, prefix=van, useprefix=true and Singh, Satinder and Silver, David},
  date = {2021-01-05},
  number = {arXiv:2007.08794},
  eprint = {2007.08794},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2007.08794},
  urldate = {2022-06-08},
  abstract = {Reinforcement learning (RL) algorithms update an agent's parameters according to one of several possible rules, discovered manually through years of research. Automating the discovery of update rules from data could lead to more efficient algorithms, or algorithms that are better adapted to specific environments. Although there have been prior attempts at addressing this significant scientific challenge, it remains an open question whether it is feasible to discover alternatives to fundamental concepts of RL such as value functions and temporal-difference learning. This paper introduces a new meta-learning approach that discovers an entire update rule which includes both 'what to predict' (e.g. value functions) and 'how to learn from it' (e.g. bootstrapping) by interacting with a set of environments. The output of this method is an RL algorithm that we call Learned Policy Gradient (LPG). Empirical results show that our method discovers its own alternative to the concept of value functions. Furthermore it discovers a bootstrapping mechanism to maintain and use its predictions. Surprisingly, when trained solely on toy environments, LPG generalises effectively to complex Atari games and achieves non-trivial performance. This shows the potential to discover general RL algorithms from data.},
  file = {/Users/lukakuma/Zotero/storage/ZV2P7L5P/Oh et al. - 2021 - Discovering Reinforcement Learning Algorithms.pdf;/Users/lukakuma/Zotero/storage/VRILX2QD/2007.html}
}

@article{omidshafieiNavigatingLandscapeMultiplayer2020,
  title = {Navigating the Landscape of Multiplayer Games},
  author = {Omidshafiei, Shayegan and Tuyls, Karl and Czarnecki, Wojciech M. and Santos, Francisco C. and Rowland, Mark and Connor, Jerome and Hennes, Daniel and Muller, Paul and P\'erolat, Julien and Vylder, Bart De and Gruslys, Audrunas and Munos, R\'emi},
  date = {2020-11-05},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {11},
  number = {1},
  pages = {5603},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  url = {https://www.nature.com/articles/s41467-020-19244-4},
  urldate = {2022-06-09},
  abstract = {Multiplayer games have long been used as testbeds in artificial intelligence research, aptly referred to as the Drosophila of artificial intelligence. Traditionally, researchers have focused on using well-known games to build strong agents. This progress, however, can be better informed by characterizing games and their topological landscape. Tackling this latter question can facilitate understanding of agents and help determine what game an agent should target next as part of its training. Here, we show how network measures applied to response graphs of large-scale games enable the creation of a landscape of games, quantifying relationships between games of varying sizes and characteristics. We illustrate our findings in domains ranging from canonical games to complex empirical games capturing the performance of trained agents pitted against one another. Our results culminate in a demonstration leveraging this information to generate new and interesting games, including mixtures of empirical games synthesized from real world games.},
  issue = {1},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/XX5DKC3P/Omidshafiei et al. - 2020 - Navigating the landscape of multiplayer games.pdf;/Users/lukakuma/Zotero/storage/DFQ99ACI/s41467-020-19244-4.html}
}

@article{oordRepresentationLearningContrastive2018,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {family=Oord, given=Aaron, prefix=van den, useprefix=false and Li, Yazhe and Vinyals, Oriol},
  date = {2018-07-10},
  url = {https://arxiv.org/abs/1807.03748v2},
  urldate = {2022-04-25},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/BX6ERSKN/Oord et al. - 2018 - Representation Learning with Contrastive Predictiv.pdf;/Users/lukakuma/Zotero/storage/6JWBCDDZ/1807.html}
}

@online{openaiDotaLargeScale2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D\k{e}biak, Przemys\l aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J\'ozefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  date = {2019-12-13},
  number = {arXiv:1912.06680},
  eprint = {arXiv:1912.06680},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1912.06680},
  urldate = {2022-07-20},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/R3IGB6E2/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf;/Users/lukakuma/Zotero/storage/F7RSL2S5/1912.html}
}

@online{openaiLearningDexterousInHand2019,
  title = {Learning {{Dexterous In-Hand Manipulation}}},
  author = {OpenAI and Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
  date = {2019-01-18},
  number = {arXiv:1808.00177},
  eprint = {arXiv:1808.00177},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1808.00177},
  urldate = {2022-07-29},
  abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/EVF2DEJS/OpenAI et al. - 2019 - Learning Dexterous In-Hand Manipulation.pdf;/Users/lukakuma/Zotero/storage/A3HSNJ3T/1808.html}
}

@online{OpenAILP2019,
  title = {{{OpenAI LP}}},
  date = {2019-03-11T15:59:19},
  url = {https://openai.com/blog/openai-lp/},
  urldate = {2022-03-11},
  abstract = {We've created OpenAI LP, a new ``capped-profit'' company that allows us to rapidly increase our investments in compute and talent while including checks and balances to actualize our mission.},
  langid = {english},
  organization = {{OpenAI}},
  file = {/Users/lukakuma/Zotero/storage/I2IYRI5P/openai-lp.html}
}

@online{openaiSolvingRubikCube2019,
  title = {Solving {{Rubik}}'s {{Cube}} with a {{Robot Hand}}},
  author = {OpenAI and Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
  date = {2019-10-15},
  number = {arXiv:1910.07113},
  eprint = {arXiv:1910.07113},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.07113},
  urldate = {2022-07-29},
  abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/XATW4FXT/OpenAI et al. - 2019 - Solving Rubik's Cube with a Robot Hand.pdf;/Users/lukakuma/Zotero/storage/8YSI3DMH/1910.html}
}

@unpublished{openendedlearningteamOpenEndedLearningLeads2021,
  title = {Open-{{Ended Learning Leads}} to {{Generally Capable Agents}}},
  author = {Open Ended Learning Team and Stooke, Adam and Mahajan, Anuj and Barros, Catarina and Deck, Charlie and Bauer, Jakob and Sygnowski, Jakub and Trebacz, Maja and Jaderberg, Max and Mathieu, Michael and McAleese, Nat and Bradley-Schmieg, Nathalie and Wong, Nathaniel and Porcel, Nicolas and Raileanu, Roberta and Hughes-Fitt, Steph and Dalibard, Valentin and Czarnecki, Wojciech Marian},
  date = {2021-07-31},
  number = {arXiv:2107.12808},
  eprint = {2107.12808},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2107.12808},
  urldate = {2022-06-10},
  abstract = {In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.},
  keywords = {(ext) Sparrow,DeepMind},
  file = {/Users/lukakuma/Zotero/storage/B357EVW5/Open Ended Learning Team et al. - 2021 - Open-Ended Learning Leads to Generally Capable Age.pdf;/Users/lukakuma/Zotero/storage/68X5RFHW/2107.html}
}

@online{opolkaSpatioTemporalDeepGraph2019,
  title = {Spatio-{{Temporal Deep Graph Infomax}}},
  author = {Opolka, Felix L. and Solomon, Aaron and Cangea, C\u{a}t\u{a}lina and Veli\v{c}kovi\'c, Petar and Li\`o, Pietro and Hjelm, R. Devon},
  date = {2019-04-12},
  number = {arXiv:1904.06316},
  eprint = {arXiv:1904.06316},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.06316},
  urldate = {2022-07-30},
  abstract = {Spatio-temporal graphs such as traffic networks or gene regulatory systems present challenges for the existing deep learning methods due to the complexity of structural changes over time. To address these issues, we introduce Spatio-Temporal Deep Graph Infomax (STDGI)---a fully unsupervised node representation learning approach based on mutual information maximization that exploits both the temporal and spatial dynamics of the graph. Our model tackles the challenging task of node-level regression by training embeddings to maximize the mutual information between patches of the graph, at any given time step, and between features of the central nodes of patches, in the future. We demonstrate through experiments and qualitative studies that the learned representations can successfully encode relevant information about the input graph and improve the predictive performance of spatio-temporal auto-regressive forecasting models.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/DT82HFKK/Opolka et al. - 2019 - Spatio-Temporal Deep Graph Infomax.pdf;/Users/lukakuma/Zotero/storage/NHH7TFTJ/1904.html}
}

@online{orseauLogarithmicPruningAll2020,
  title = {Logarithmic {{Pruning}} Is {{All You Need}}},
  author = {Orseau, Laurent and Hutter, Marcus and Rivasplata, Omar},
  date = {2020-10-25},
  number = {arXiv:2006.12156},
  eprint = {arXiv:2006.12156},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.12156},
  urldate = {2022-06-11},
  abstract = {The Lottery Ticket Hypothesis is a conjecture that every large neural network contains a subnetwork that, when trained in isolation, achieves comparable performance to the large network. An even stronger conjecture has been proven recently: Every sufficiently overparameterized network contains a subnetwork that, at random initialization, but without training, achieves comparable accuracy to the trained large network. This latter result, however, relies on a number of strong assumptions and guarantees a polynomial factor on the size of the large network compared to the target function. In this work, we remove the most limiting assumptions of this previous work while providing significantly tighter bounds: the overparameterized network only needs a logarithmic factor (in all variables but depth) number of neurons per weight of the target subnetwork.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/QWEQRRN7/Orseau et al. - 2020 - Logarithmic Pruning is All You Need.pdf}
}

@online{ortegaShakingFoundationsDelusions2021,
  title = {Shaking the Foundations: Delusions in Sequence Models for Interaction and Control},
  shorttitle = {Shaking the Foundations},
  author = {Ortega, Pedro A. and Kunesch, Markus and Del\'etang, Gr\'egoire and Genewein, Tim and Grau-Moya, Jordi and Veness, Joel and Buchli, Jonas and Degrave, Jonas and Piot, Bilal and Perolat, Julien and Everitt, Tom and Tallec, Corentin and Parisotto, Emilio and Erez, Tom and Chen, Yutian and Reed, Scott and Hutter, Marcus and family=Freitas, given=Nando, prefix=de, useprefix=true and Legg, Shane},
  date = {2021-10-20},
  number = {arXiv:2110.10819},
  eprint = {arXiv:2110.10819},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.10819},
  urldate = {2022-07-23},
  abstract = {The recent phenomenal success of language models has reinvigorated machine learning research, and large sequence models such as transformers are being applied to a variety of domains. One important problem class that has remained relatively elusive however is purposeful adaptive behavior. Currently there is a common perception that sequence models "lack the understanding of the cause and effect of their actions" leading them to draw incorrect inferences due to auto-suggestive delusions. In this report we explain where this mismatch originates, and show that it can be resolved by treating actions as causal interventions. Finally, we show that in supervised learning, one can teach a system to condition or intervene on data by training with factual and counterfactual error signals respectively.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/YLKGMTN5/Ortega et al. - 2021 - Shaking the foundations delusions in sequence mod.pdf;/Users/lukakuma/Zotero/storage/Y9B9VXF4/2110.html}
}

@article{outeiralProspectsQuantumComputing2021,
  title = {The Prospects of Quantum Computing in Computational Molecular Biology},
  author = {Outeiral, Carlos and Strahm, Martin and Shi, Jiye and Morris, Garrett M. and Benjamin, Simon C. and Deane, Charlotte M.},
  date = {2021},
  journaltitle = {WIREs Computational Molecular Science},
  volume = {11},
  number = {1},
  pages = {e1481},
  issn = {1759-0884},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcms.1481},
  urldate = {2022-05-30},
  abstract = {Quantum computers can in principle solve certain problems exponentially more quickly than their classical counterparts. We have not yet reached the advent of useful quantum computation, but when we do, it will affect nearly all scientific disciplines. In this review, we examine how current quantum algorithms could revolutionize computational biology and bioinformatics. There are potential benefits across the entire field, from the ability to process vast amounts of information and run machine learning algorithms far more efficiently, to algorithms for quantum simulation that are poised to improve computational calculations in drug discovery, to quantum algorithms for optimization that may advance fields from protein structure prediction to network analysis. However, these exciting prospects are susceptible to ``hype,'' and it is also important to recognize the caveats and challenges in this new technology. Our aim is to introduce the promise and limitations of emerging quantum computing technologies in the areas of computational molecular biology and bioinformatics. This article is categorized under: Structure and Mechanism {$>$} Computational Biochemistry and Biophysics Data Science {$>$} Computer Algorithms and Programming Electronic Structure Theory {$>$} Ab Initio Electronic Structure Methods},
  langid = {english},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/TUIFU9R4/Outeiral et al. - 2021 - The prospects of quantum computing in computationa.pdf;/Users/lukakuma/Zotero/storage/I5QRKTFA/wcms.html}
}

@article{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  date = {2022-03-04},
  url = {https://arxiv.org/abs/2203.02155v1},
  urldate = {2022-08-09},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  langid = {english},
  keywords = {alignment,OpenAI,read,RLHF},
  file = {/Users/lukakuma/Zotero/storage/DXRUFPTI/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf}
}

@online{palMedMCQALargescaleMultiSubject2022,
  title = {{{MedMCQA}} : {{A Large-scale Multi-Subject Multi-Choice Dataset}} for {{Medical}} Domain {{Question Answering}}},
  shorttitle = {{{MedMCQA}}},
  author = {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  date = {2022-03-27},
  number = {arXiv:2203.14371},
  eprint = {arXiv:2203.14371},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.14371},
  urldate = {2022-11-23},
  abstract = {This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS \textbackslash\& NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects \textbackslash\& topics. A detailed explanation of the solution, along with the above information, is provided in this study.},
  pubstate = {preprint},
  keywords = {QA},
  file = {/Users/lukakuma/Zotero/storage/59QX3WCD/Pal et al. - 2022 - MedMCQA  A Large-scale Multi-Subject Multi-Choice.pdf;/Users/lukakuma/Zotero/storage/N9IJDRGP/2203.html}
}

@unpublished{palowitchGraphWorldFakeGraphs2022,
  title = {{{GraphWorld}}: {{Fake Graphs Bring Real Insights}} for {{GNNs}}},
  shorttitle = {{{GraphWorld}}},
  author = {Palowitch, John and Tsitsulin, Anton and Mayer, Brandon and Perozzi, Bryan},
  date = {2022-02-28},
  eprint = {2203.00112},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.00112},
  urldate = {2022-05-05},
  abstract = {Despite advances in the field of Graph Neural Networks (GNNs), only a small number (\textasciitilde 5) of datasets are currently used to evaluate new models. This continued reliance on a handful of datasets provides minimal insight into the performance differences between models, and is especially challenging for industrial practitioners who are likely to have datasets which look very different from those used as academic benchmarks. In the course of our work on GNN infrastructure and open-source software at Google, we have sought to develop improved benchmarks that are robust, tunable, scalable,and generalizable. In this work we introduce GraphWorld, a novel methodology and system for benchmarking GNN models on an arbitrarily-large population of synthetic graphs for any conceivable GNN task. GraphWorld allows a user to efficiently generate a world with millions of statistically diverse datasets. It is accessible, scalable, and easy to use. GraphWorld can be run on a single machine without specialized hardware, or it can be easily scaled up to run on arbitrary clusters or cloud frameworks. Using GraphWorld, a user has fine-grained control over graph generator parameters, and can benchmark arbitrary GNN models with built-in hyperparameter tuning. We present insights from GraphWorld experiments regarding the performance characteristics of tens of thousands of GNN models over millions of benchmark datasets. We further show that GraphWorld efficiently explores regions of benchmark dataset space uncovered by standard benchmarks, revealing comparisons between models that have not been historically obtainable. Using GraphWorld, we also are able to study in-detail the relationship between graph properties and task performance metrics, which is nearly impossible with the classic collection of real-world benchmarks.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/2Q82EQRR/Palowitch et al. - 2022 - GraphWorld Fake Graphs Bring Real Insights for GN.pdf;/Users/lukakuma/Zotero/storage/H2QDAU9D/2203.html}
}

@article{pandaBuildingBiofoundryIndia2021,
  title = {Building {{Biofoundry India}}: Challenges and Path Forward},
  shorttitle = {Building {{Biofoundry India}}},
  author = {Panda, Binay and Dhar, Pawan K},
  date = {2021-10-01},
  journaltitle = {Synthetic Biology},
  shortjournal = {Synthetic Biology},
  volume = {6},
  number = {1},
  pages = {ysab015},
  issn = {2397-7000},
  url = {https://doi.org/10.1093/synbio/ysab015},
  urldate = {2022-05-25},
  abstract = {Biofoundry is a place where biomanufacturing meets automation. The highly modular structure of a biofoundry helps accelerate the design\textendash build\textendash test\textendash learn workflow to deliver products fast and in a streamlined fashion. In this perspective, we describe our efforts to build Biofoundry India, where we see the facility add a substantial value in supporting research, innovation and entrepreneurship. We describe three key areas of our focus, harnessing the potential of non-expressing parts of the sequenced genomes, using deep learning in pathway reconstruction and synthesising enzymes and metabolites. Toward the end, we describe specific challenges in building such facility in India and the path to mitigate some of those working with the other biofoundries worldwide.},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/L9RXDZGQ/Panda and Dhar - 2021 - Building Biofoundry India challenges and path for.pdf;/Users/lukakuma/Zotero/storage/6AF95JYQ/6309151.html}
}

@online{panLessMorePay2021,
  title = {Less Is {{More}}: {{Pay Less Attention}} in {{Vision Transformers}}},
  shorttitle = {Less Is {{More}}},
  author = {Pan, Zizheng and Zhuang, Bohan and He, Haoyu and Liu, Jing and Cai, Jianfei},
  date = {2021-12-23},
  number = {arXiv:2105.14217},
  eprint = {arXiv:2105.14217},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.14217},
  urldate = {2022-06-09},
  abstract = {Transformers have become one of the dominant architectures in deep learning, particularly as a powerful alternative to convolutional neural networks (CNNs) in computer vision. However, Transformer training and inference in previous works can be prohibitively expensive due to the quadratic complexity of self-attention over a long sequence of representations, especially for high-resolution dense prediction tasks. To this end, we present a novel Less attention vIsion Transformer (LIT), building upon the fact that the early self-attention layers in Transformers still focus on local patterns and bring minor benefits in recent hierarchical vision Transformers. Specifically, we propose a hierarchical Transformer where we use pure multi-layer perceptrons (MLPs) to encode rich local patterns in the early stages while applying self-attention modules to capture longer dependencies in deeper layers. Moreover, we further propose a learned deformable token merging module to adaptively fuse informative patches in a non-uniform manner. The proposed LIT achieves promising performance on image recognition tasks, including image classification, object detection and instance segmentation, serving as a strong backbone for many vision tasks. Code is available at: https://github.com/zhuang-group/LIT},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/9D9NAKFI/Pan et al. - 2021 - Less is More Pay Less Attention in Vision Transfo.pdf}
}

@online{panSTAdapterParameterEfficientImagetoVideo2022,
  title = {{{ST-Adapter}}: {{Parameter-Efficient Image-to-Video Transfer Learning}}},
  shorttitle = {{{ST-Adapter}}},
  author = {Pan, Junting and Lin, Ziyi and Zhu, Xiatian and Shao, Jing and Li, Hongsheng},
  date = {2022-10-13},
  number = {arXiv:2206.13559},
  eprint = {arXiv:2206.13559},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.13559},
  urldate = {2023-03-03},
  abstract = {Capitalizing on large pre-trained models for various downstream tasks of interest have recently emerged with promising performance. Due to the ever-growing model size, the standard full fine-tuning based task adaptation strategy becomes prohibitively costly in terms of model training and storage. This has led to a new research direction in parameter-efficient transfer learning. However, existing attempts typically focus on downstream tasks from the same modality (e.g., image understanding) of the pre-trained model. This creates a limit because in some specific modalities, (e.g., video understanding) such a strong pre-trained model with sufficient knowledge is less or not available. In this work, we investigate such a novel cross-modality transfer learning setting, namely parameter-efficient image-to-video transfer learning. To solve this problem, we propose a new Spatio-Temporal Adapter (ST-Adapter) for parameter-efficient fine-tuning per video task. With a built-in spatio-temporal reasoning capability in a compact design, ST-Adapter enables a pre-trained image model without temporal knowledge to reason about dynamic video content at a small (\textasciitilde 8\%) per-task parameter cost, requiring approximately 20 times fewer updated parameters compared to previous work. Extensive experiments on video action recognition tasks show that our ST-Adapter can match or even outperform the strong full fine-tuning strategy and state-of-the-art video models, whilst enjoying the advantage of parameter efficiency. The code and model are available at https://github.com/linziyi96/st-adapter},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/MVNGAL76/Pan et al. - 2022 - ST-Adapter Parameter-Efficient Image-to-Video Tra.pdf;/Users/lukakuma/Zotero/storage/6UEYAIQK/2206.html}
}

@online{papalampidiHierarchical3DAdaptersLong2022,
  title = {{{Hierarchical3D Adapters}} for {{Long Video-to-text Summarization}}},
  author = {Papalampidi, Pinelopi and Lapata, Mirella},
  date = {2022-10-10},
  number = {arXiv:2210.04829},
  eprint = {arXiv:2210.04829},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.04829},
  urldate = {2023-03-03},
  abstract = {In this paper, we focus on video-to-text summarization and investigate how to best utilize multimodal information for summarizing long inputs (e.g., an hour-long TV show) into long outputs (e.g., a multi-sentence summary). We extend SummScreen (Chen et al., 2021), a dialogue summarization dataset consisting of transcripts of TV episodes with reference summaries, and create a multimodal variant by collecting corresponding full-length videos. We incorporate multimodal information into a pre-trained textual summarizer efficiently using adapter modules augmented with a hierarchical structure while tuning only 3.8\textbackslash\% of model parameters. Our experiments demonstrate that multimodal information offers superior performance over more memory-heavy and fully fine-tuned textual summarization methods.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/LQG6NX2E/Papalampidi and Lapata - 2022 - Hierarchical3D Adapters for Long Video-to-text Sum.pdf;/Users/lukakuma/Zotero/storage/LVUXAD4X/2210.html}
}

@online{parisiTALMToolAugmented2022,
  title = {{{TALM}}: {{Tool Augmented Language Models}}},
  shorttitle = {{{TALM}}},
  author = {Parisi, Aaron and Zhao, Yao and Fiedel, Noah},
  date = {2022-05-24},
  number = {arXiv:2205.12255},
  eprint = {arXiv:2205.12255},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.12255},
  urldate = {2023-02-20},
  abstract = {Transformer based language models (LMs) demonstrate increasing performance with scale across a wide variety of tasks. Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time. Many useful tasks may also benefit from LMs being able to access APIs that read or modify state. In this work, we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative "self-play" technique to bootstrap performance starting from few tool demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA task and a reasoning oriented math task with simple tools. At a given model scale, TALM significantly outperforms non-augmented LMs. We further demonstrate that TALM successfully performs out-of-distribution inferences on both QA and math tasks, where non-augmented LMs fail. Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale.},
  pubstate = {preprint},
  keywords = {self-play},
  file = {/Users/lukakuma/Zotero/storage/DPWGC2WG/Parisi et al. - 2022 - TALM Tool Augmented Language Models.pdf;/Users/lukakuma/Zotero/storage/FWUR6NAA/2205.html}
}

@unpublished{parisiUnsurprisingEffectivenessPreTrained2022,
  title = {The {{Unsurprising Effectiveness}} of {{Pre-Trained Vision Models}} for {{Control}}},
  author = {Parisi, Simone and Rajeswaran, Aravind and Purushwalkam, Senthil and Gupta, Abhinav},
  date = {2022-03-07},
  number = {arXiv:2203.03580},
  eprint = {2203.03580},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.03580},
  urldate = {2022-05-13},
  abstract = {Recent years have seen the emergence of pre-trained representations as a powerful abstraction for AI applications in computer vision, natural language, and speech. However, policy learning for control is still dominated by a tabula-rasa learning paradigm, with visuo-motor policies often trained from scratch using data from deployment environments. In this context, we revisit and study the role of pre-trained visual representations for control, and in particular representations trained on large-scale computer vision datasets. Through extensive empirical evaluation in diverse control domains (Habitat, DeepMind Control, Adroit, Franka Kitchen), we isolate and study the importance of different representation training methods, data augmentations, and feature hierarchies. Overall, we find that pre-trained visual representations can be competitive or even better than ground-truth state representations to train control policies. This is in spite of using only out-of-domain data from standard vision datasets, without any in-domain data from the deployment environments. Additional details and source code is available at https://sites.google.com/view/pvr-control},
  keywords = {(ext) Gato},
  file = {/Users/lukakuma/Zotero/storage/EERLS2M4/Parisi et al. - 2022 - The Unsurprising Effectiveness of Pre-Trained Visi.pdf;/Users/lukakuma/Zotero/storage/THCQ2M5C/2203.html}
}

@online{parkDeepSDFLearningContinuous2019,
  title = {{{DeepSDF}}: {{Learning Continuous Signed Distance Functions}} for {{Shape Representation}}},
  shorttitle = {{{DeepSDF}}},
  author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
  date = {2019-01-15},
  number = {arXiv:1901.05103},
  eprint = {arXiv:1901.05103},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.05103},
  urldate = {2022-08-29},
  abstract = {Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.},
  pubstate = {preprint},
  keywords = {functa},
  file = {/Users/lukakuma/Zotero/storage/U8NGP9GI/Park et al. - 2019 - DeepSDF Learning Continuous Signed Distance Funct.pdf;/Users/lukakuma/Zotero/storage/ZAZPL2D3/1901.html}
}

@online{parker-holderEvolvingCurriculaRegretBased2022,
  title = {Evolving {{Curricula}} with {{Regret-Based Environment Design}}},
  author = {Parker-Holder, Jack and Jiang, Minqi and Dennis, Michael and Samvelyan, Mikayel and Foerster, Jakob and Grefenstette, Edward and Rockt\"aschel, Tim},
  date = {2022-03-08},
  number = {arXiv:2203.01302},
  eprint = {arXiv:2203.01302},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.01302},
  urldate = {2022-11-18},
  abstract = {It remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent's capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agent's capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of the paper is available at accelagent.github.io.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/A7GESL4M/Parker-Holder et al. - 2022 - Evolving Curricula with Regret-Based Environment D.pdf;/Users/lukakuma/Zotero/storage/C33V6SWA/2203.html}
}

@online{parrishSingleTurnDebateDoes2022,
  title = {Single-{{Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions}}},
  author = {Parrish, Alicia and Trivedi, Harsh and Perez, Ethan and Chen, Angelica and Nangia, Nikita and Phang, Jason and Bowman, Samuel R.},
  date = {2022-04-13},
  number = {arXiv:2204.05212},
  eprint = {arXiv:2204.05212},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.05212},
  urldate = {2022-12-28},
  abstract = {Current QA systems can generate reasonable-sounding yet false answers without explanation or evidence for the generated answer, which is especially problematic when humans cannot readily check the model's answers. This presents a challenge for building trust in machine learning systems. We take inspiration from real-world situations where difficult questions are answered by considering opposing sides (see Irving et al., 2018). For multiple-choice QA examples, we build a dataset of single arguments for both a correct and incorrect answer option in a debate-style set-up as an initial step in training models to produce explanations for two candidate answers. We use long contexts -- humans familiar with the context write convincing explanations for pre-selected correct and incorrect answers, and we test if those explanations allow humans who have not read the full context to more accurately determine the correct answer. We do not find that explanations in our set-up improve human accuracy, but a baseline condition shows that providing human-selected text snippets does improve accuracy. We use these findings to suggest ways of improving the debate set up for future data collection efforts.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/LAP9T6LI/Parrish et al. - 2022 - Single-Turn Debate Does Not Help Humans Answer Har.pdf;/Users/lukakuma/Zotero/storage/I3JPULLR/2204.html}
}

@unpublished{pathakFourCastNetGlobalDatadriven2022,
  title = {{{FourCastNet}}: {{A Global Data-driven High-resolution Weather Model}} Using {{Adaptive Fourier Neural Operators}}},
  shorttitle = {{{FourCastNet}}},
  author = {Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and Hassanzadeh, Pedram and Kashinath, Karthik and Anandkumar, Animashree},
  date = {2022-02-22},
  eprint = {2202.11214},
  eprinttype = {arxiv},
  eprintclass = {physics},
  url = {http://arxiv.org/abs/2202.11214},
  urldate = {2022-04-20},
  abstract = {FourCastNet, short for Fourier Forecasting Neural Network, is a global data-driven weather forecasting model that provides accurate short to medium-range global predictions at \$0.25\^\{\textbackslash circ\}\$ resolution. FourCastNet accurately forecasts high-resolution, fast-timescale variables such as the surface wind speed, precipitation, and atmospheric water vapor. It has important implications for planning wind energy resources, predicting extreme weather events such as tropical cyclones, extra-tropical cyclones, and atmospheric rivers. FourCastNet matches the forecasting accuracy of the ECMWF Integrated Forecasting System (IFS), a state-of-the-art Numerical Weather Prediction (NWP) model, at short lead times for large-scale variables, while outperforming IFS for variables with complex fine-scale structure, including precipitation. FourCastNet generates a week-long forecast in less than 2 seconds, orders of magnitude faster than IFS. The speed of FourCastNet enables the creation of rapid and inexpensive large-ensemble forecasts with thousands of ensemble-members for improving probabilistic forecasting. We discuss how data-driven deep learning models such as FourCastNet are a valuable addition to the meteorology toolkit to aid and augment NWP models.},
  file = {/Users/lukakuma/Zotero/storage/DLPBIWIC/Pathak et al. - 2022 - FourCastNet A Global Data-driven High-resolution .pdf;/Users/lukakuma/Zotero/storage/AIA4D89K/2202.html}
}

@article{patrickCompositionsTransformationsContrastive2020,
  title = {On {{Compositions}} of {{Transformations}} in {{Contrastive Self-Supervised Learning}}},
  author = {Patrick, Mandela and Asano, Yuki M. and Kuznetsova, Polina and Fong, Ruth and Henriques, Jo\~ao F. and Zweig, Geoffrey and Vedaldi, Andrea},
  date = {2020-03-09},
  url = {https://arxiv.org/abs/2003.04298v3},
  urldate = {2022-04-25},
  abstract = {In the image domain, excellent representations can be learned by inducing invariance to content-preserving transformations via noise contrastive learning. In this paper, we generalize contrastive learning to a wider set of transformations, and their compositions, for which either invariance or distinctiveness is sought. We show that it is not immediately obvious how existing methods such as SimCLR can be extended to do so. Instead, we introduce a number of formal requirements that all contrastive formulations must satisfy, and propose a practical construction which satisfies these requirements. In order to maximise the reach of this analysis, we express all components of noise contrastive formulations as the choice of certain generalized transformations of the data (GDTs), including data sampling. We then consider videos as an example of data in which a large variety of transformations are applicable, accounting for the extra modalities -- for which we analyze audio and text -- and the dimension of time. We find that being invariant to certain transformations and distinctive to others is critical to learning effective video representations, improving the state-of-the-art for multiple benchmarks by a large margin, and even surpassing supervised pretraining.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/9Y36KFBE/Patrick et al. - 2020 - On Compositions of Transformations in Contrastive .pdf;/Users/lukakuma/Zotero/storage/RIDLNW2D/2003.html}
}

@online{pengBEiTV2Masked2022,
  title = {{{BEiT}} v2: {{Masked Image Modeling}} with {{Vector-Quantized Visual Tokenizers}}},
  shorttitle = {{{BEiT}} V2},
  author = {Peng, Zhiliang and Dong, Li and Bao, Hangbo and Ye, Qixiang and Wei, Furu},
  date = {2022-08-12},
  number = {arXiv:2208.06366},
  eprint = {arXiv:2208.06366},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.06366},
  urldate = {2022-08-23},
  abstract = {Masked image modeling (MIM) has demonstrated impressive results in self-supervised representation learning by recovering corrupted image patches. However, most methods still operate on low-level image pixels, which hinders the exploitation of high-level semantics for representation models. In this study, we propose to use a semantic-rich visual tokenizer as the reconstruction target for masked prediction, providing a systematic way to promote MIM from pixel-level to semantic-level. Specifically, we introduce vector-quantized knowledge distillation to train the tokenizer, which discretizes a continuous semantic space to compact codes. We then pretrain vision Transformers by predicting the original visual tokens for the masked image patches. Moreover, we encourage the model to explicitly aggregate patch information into a global image representation, which facilities linear probing. Experiments on image classification and semantic segmentation show that our approach outperforms all compared MIM methods. On ImageNet-1K (224 size), the base-size BEiT v2 achieves 85.5\% top-1 accuracy for fine-tuning and 80.1\% top-1 accuracy for linear probing. The large-size BEiT v2 obtains 87.3\% top-1 accuracy for ImageNet-1K (224 size) fine-tuning, and 56.7\% mIoU on ADE20K for semantic segmentation. The code and pretrained models are available at https://aka.ms/beit.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/GWWKF7JX/Peng et al. - 2022 - BEiT v2 Masked Image Modeling with Vector-Quantiz.pdf;/Users/lukakuma/Zotero/storage/Z9L6UM4A/2208.html}
}

@online{pengCheckYourFacts2023,
  title = {Check {{Your Facts}} and {{Try Again}}: {{Improving Large Language Models}} with {{External Knowledge}} and {{Automated Feedback}}},
  shorttitle = {Check {{Your Facts}} and {{Try Again}}},
  author = {Peng, Baolin and Galley, Michel and He, Pengcheng and Cheng, Hao and Xie, Yujia and Hu, Yu and Huang, Qiuyuan and Liden, Lars and Yu, Zhou and Chen, Weizhu and Gao, Jianfeng},
  date = {2023-02-24},
  number = {arXiv:2302.12813},
  eprint = {arXiv:2302.12813},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.12813},
  urldate = {2023-02-28},
  abstract = {Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and inability to use external knowledge.This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in consolidated external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of mission-critical scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.},
  pubstate = {preprint},
  keywords = {8-chat},
  file = {/Users/lukakuma/Zotero/storage/S6TU3BED/Peng et al. - 2023 - Check Your Facts and Try Again Improving Large La.pdf;/Users/lukakuma/Zotero/storage/Y64UI2M3/2302.html}
}

@online{pengGODELLargeScalePreTraining2022,
  title = {{{GODEL}}: {{Large-Scale Pre-Training}} for {{Goal-Directed Dialog}}},
  shorttitle = {{{GODEL}}},
  author = {Peng, Baolin and Galley, Michel and He, Pengcheng and Brockett, Chris and Liden, Lars and Nouri, Elnaz and Yu, Zhou and Dolan, Bill and Gao, Jianfeng},
  date = {2022-06-22},
  number = {arXiv:2206.11309},
  eprint = {arXiv:2206.11309},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.11309},
  urldate = {2022-12-27},
  abstract = {We introduce GODEL (Grounded Open Dialogue Language Model), a large pre-trained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot fine-tuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available.},
  langid = {english},
  pubstate = {preprint},
  keywords = {8-chat,read},
  file = {/Users/lukakuma/Zotero/storage/QSXFWZ8X/Peng et al. - 2022 - GODEL Large-Scale Pre-Training for Goal-Directed .pdf}
}

@online{perezDiscoveringLanguageModel2022,
  title = {Discovering {{Language Model Behaviors}} with {{Model-Written Evaluations}}},
  author = {Perez, Ethan and Ringer, Sam and Luko\v{s}i\=ut\.e, Kamil\.e and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noem\'i and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
  date = {2022-12-19},
  number = {arXiv:2212.09251},
  eprint = {arXiv:2212.09251},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.09251},
  urldate = {2022-12-23},
  abstract = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/MK2ZCLPH/Perez et al. - 2022 - Discovering Language Model Behaviors with Model-Wr.pdf;/Users/lukakuma/Zotero/storage/U7DHQTN4/2212.html}
}

@inproceedings{perezFindingGeneralizableEvidence2019,
  title = {Finding {{Generalizable Evidence}} by {{Learning}} to {{Convince Q}}\&{{A Models}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Perez, Ethan and Karamcheti, Siddharth and Fergus, Rob and Weston, Jason and Kiela, Douwe and Cho, Kyunghyun},
  date = {2019-11},
  pages = {2402--2411},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  url = {https://aclanthology.org/D19-1244},
  urldate = {2022-12-23},
  abstract = {We propose a system that finds the strongest supporting evidence for a given answer to a question, using passage-based question-answering (QA) as a testbed. We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer, if the QA model received those sentences instead of the full passage. Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other QA models and humans. Given its general nature, this approach improves QA in a robust manner: using agent-selected evidence (i) humans can correctly answer questions with only \textbackslash textasciitilde20\% of the full passage and (ii) QA models can generalize to longer passages and harder questions.},
  eventtitle = {{{EMNLP-IJCNLP}} 2019},
  keywords = {8-chat},
  file = {/Users/lukakuma/Zotero/storage/8W96VQ9G/Perez et al. - 2019 - Finding Generalizable Evidence by Learning to Conv.pdf}
}

@article{perezRedTeamingLanguage,
  title = {Red {{Teaming Language Models}} with {{Language Models}}},
  author = {Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  date = {2022-02-07},
  pages = {31},
  abstract = {Language Models (LMs) often cannot be deployed because of their potential to harm users in ways that are hard to predict in advance. Prior work identifies harmful behaviors before deployment by using human annotators to handwrite test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (``red teaming'') using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LMgenerated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.},
  langid = {english},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/3T4PVLBI/Perez et al. - Red Teaming Language Models with Language Models.pdf}
}

@online{perezTrueFewShotLearning2021,
  title = {True {{Few-Shot Learning}} with {{Language Models}}},
  author = {Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  date = {2021-05-24},
  number = {arXiv:2105.11447},
  eprint = {arXiv:2105.11447},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.11447},
  urldate = {2022-11-02},
  abstract = {Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (``prompts''). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/645CPFY3/Perez et al. - 2021 - True Few-Shot Learning with Language Models.pdf}
}

@article{perolatMasteringGameStratego2022,
  title = {Mastering the {{Game}} of {{Stratego}} with {{Model-Free Multiagent Reinforcement Learning}}},
  author = {Perolat, Julien and family=Vylder, given=Bart, prefix=de, useprefix=true and Hennes, Daniel and Tarassov, Eugene and Strub, Florian and family=Boer, given=Vincent, prefix=de, useprefix=true and Muller, Paul and Connor, Jerome T. and Burch, Neil and Anthony, Thomas and McAleer, Stephen and Elie, Romuald and Cen, Sarah H. and Wang, Zhe and Gruslys, Audrunas and Malysheva, Aleksandra and Khan, Mina and Ozair, Sherjil and Timbers, Finbarr and Pohlen, Toby and Eccles, Tom and Rowland, Mark and Lanctot, Marc and Lespiau, Jean-Baptiste and Piot, Bilal and Omidshafiei, Shayegan and Lockhart, Edward and Sifre, Laurent and Beauguerlange, Nathalie and Munos, Remi and Silver, David and Singh, Satinder and Hassabis, Demis and Tuyls, Karl},
  date = {2022-06-30},
  url = {https://arxiv.org/abs/2206.15378v1},
  urldate = {2022-12-30},
  abstract = {We introduce DeepNash, an autonomous agent capable of learning to play the imperfect information game Stratego from scratch, up to a human expert level. Stratego is one of the few iconic board games that Artificial Intelligence (AI) has not yet mastered. This popular game has an enormous game tree on the order of \$10\^\{535\}\$ nodes, i.e., \$10\^\{175\}\$ times larger than that of Go. It has the additional complexity of requiring decision-making under imperfect information, similar to Texas hold'em poker, which has a significantly smaller game tree (on the order of \$10\^\{164\}\$ nodes). Decisions in Stratego are made over a large number of discrete actions with no obvious link between action and outcome. Episodes are long, with often hundreds of moves before a player wins, and situations in Stratego can not easily be broken down into manageably-sized sub-problems as in poker. For these reasons, Stratego has been a grand challenge for the field of AI for decades, and existing AI methods barely reach an amateur level of play. DeepNash uses a game-theoretic, model-free deep reinforcement learning method, without search, that learns to master Stratego via self-play. The Regularised Nash Dynamics (R-NaD) algorithm, a key component of DeepNash, converges to an approximate Nash equilibrium, instead of 'cycling' around it, by directly modifying the underlying multi-agent learning dynamics. DeepNash beats existing state-of-the-art AI methods in Stratego and achieved a yearly (2022) and all-time top-3 rank on the Gravon games platform, competing with human expert players.},
  langid = {english},
  keywords = {DeepMind,read},
  file = {/Users/lukakuma/Zotero/storage/QPZBV8MI/Perolat et al. - 2022 - Mastering the Game of Stratego with Model-Free Mul.pdf}
}

@video{petarvelickovicTheoreticalFoundationsGraph2021,
  title = {Theoretical {{Foundations}} of {{Graph Neural Networks}}},
  editor = {{Petar Veli\v{c}kovi\'c}},
  date = {2021-02-22},
  url = {https://www.youtube.com/watch?v=uF53xsT7mjc},
  urldate = {2022-03-23},
  abstract = {Deriving graph neural networks (GNNs) from first principles, motivating their use, and explaining how they have emerged along several related research lines. Computer Laboratory Wednesday Seminar, 17 February 2021 Slide deck: https://petar-v.com/talks/GNN-Wednesd... Link at Talks.cam: https://talks.cam.ac.uk/talk/index/15...},
  editortype = {director},
  keywords = {GNN}
}

@unpublished{petersCausalInferenceUsing2015,
  title = {Causal Inference Using Invariant Prediction: Identification and Confidence Intervals},
  shorttitle = {Causal Inference Using Invariant Prediction},
  author = {Peters, Jonas and B\"uhlmann, Peter and Meinshausen, Nicolai},
  date = {2015-11-24},
  eprint = {1501.01332},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/1501.01332},
  urldate = {2022-05-12},
  abstract = {What is the difference of a prediction that is made with a causal model and a non-causal model? Suppose we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (for example various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.},
  file = {/Users/lukakuma/Zotero/storage/ZU9TSXP2/Peters et al. - 2015 - Causal inference using invariant prediction ident.pdf;/Users/lukakuma/Zotero/storage/99I53F9L/1501.html}
}

@online{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  date = {2018-03-22},
  number = {arXiv:1802.05365},
  eprint = {arXiv:1802.05365},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.05365},
  urldate = {2022-10-31},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/EBYWWSHW/Peters et al. - 2018 - Deep contextualized word representations.pdf}
}

@online{petroniLanguageModelsKnowledge2019,
  title = {Language {{Models}} as {{Knowledge Bases}}?},
  author = {Petroni, Fabio and Rockt\"aschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
  date = {2019-09-04},
  number = {arXiv:1909.01066},
  eprint = {arXiv:1909.01066},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.01066},
  urldate = {2022-11-02},
  abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as ``fillin-the-blank'' cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-theart pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https: //github.com/facebookresearch/LAMA.},
  langid = {english},
  pubstate = {preprint},
  keywords = {knowledge},
  file = {/Users/lukakuma/Zotero/storage/BUKTZTDE/Petroni et al. - 2019 - Language Models as Knowledge Bases.pdf}
}

@inproceedings{pfeifferAdapterFusionNonDestructiveTask2021,
  title = {{{AdapterFusion}}: {{Non-Destructive Task Composition}} for {{Transfer Learning}}},
  shorttitle = {{{AdapterFusion}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  author = {Pfeiffer, Jonas and Kamath, Aishwarya and R\"uckl\'e, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  date = {2021-04},
  pages = {487--503},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  url = {https://aclanthology.org/2021.eacl-main.39},
  urldate = {2023-03-01},
  abstract = {Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.},
  eventtitle = {{{EACL}} 2021},
  keywords = {2-adapter},
  file = {/Users/lukakuma/Zotero/storage/X3LAQUIF/Pfeiffer et al. - 2021 - AdapterFusion Non-Destructive Task Composition fo.pdf}
}

@online{pfeifferAdapterHubFrameworkAdapting2020,
  title = {{{AdapterHub}}: {{A Framework}} for {{Adapting Transformers}}},
  shorttitle = {{{AdapterHub}}},
  author = {Pfeiffer, Jonas and R\"uckl\'e, Andreas and Poth, Clifton and Kamath, Aishwarya and Vuli\'c, Ivan and Ruder, Sebastian and Cho, Kyunghyun and Gurevych, Iryna},
  date = {2020-10-06},
  number = {arXiv:2007.07779},
  eprint = {arXiv:2007.07779},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2007.07779},
  urldate = {2022-09-05},
  abstract = {The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters -- small learnt bottleneck layers inserted within each layer of a pre-trained model -- ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic "stitching-in" of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at https://AdapterHub.ml.},
  pubstate = {preprint},
  keywords = {*open source AI},
  file = {/Users/lukakuma/Zotero/storage/S6XPSP9G/Pfeiffer et al. - 2020 - AdapterHub A Framework for Adapting Transformers.pdf;/Users/lukakuma/Zotero/storage/ZHL7QI7R/2007.html}
}

@online{pfeifferMADXAdapterBasedFramework2020,
  title = {{{MAD-X}}: {{An Adapter-Based Framework}} for {{Multi-Task Cross-Lingual Transfer}}},
  shorttitle = {{{MAD-X}}},
  author = {Pfeiffer, Jonas and Vuli\'c, Ivan and Gurevych, Iryna and Ruder, Sebastian},
  date = {2020-10-06},
  number = {arXiv:2005.00052},
  eprint = {arXiv:2005.00052},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.00052},
  urldate = {2023-02-17},
  abstract = {The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml},
  pubstate = {preprint},
  keywords = {2-adapter},
  file = {/Users/lukakuma/Zotero/storage/C8G96I65/Pfeiffer et al. - 2020 - MAD-X An Adapter-Based Framework for Multi-Task C.pdf;/Users/lukakuma/Zotero/storage/LCZ2EJKY/2005.html}
}

@inproceedings{pfeifferMADXAdapterBasedFramework2020a,
  title = {{{MAD-X}}: {{An Adapter-Based Framework}} for {{Multi-Task Cross-Lingual Transfer}}},
  shorttitle = {{{MAD-X}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pfeiffer, Jonas and Vuli\'c, Ivan and Gurevych, Iryna and Ruder, Sebastian},
  date = {2020-11},
  pages = {7654--7673},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  url = {https://aclanthology.org/2020.emnlp-main.617},
  urldate = {2023-03-01},
  abstract = {The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.},
  eventtitle = {{{EMNLP}} 2020},
  keywords = {2-adapter},
  file = {/Users/lukakuma/Zotero/storage/VBMPPAUT/Pfeiffer et al. - 2020 - MAD-X An Adapter-Based Framework for Multi-Task C.pdf}
}

@online{pfeifferModularDeepLearning2023,
  title = {Modular {{Deep Learning}}},
  author = {Pfeiffer, Jonas and Ruder, Sebastian and Vuli\'c, Ivan and Ponti, Edoardo Maria},
  date = {2023-02-22},
  number = {arXiv:2302.11529},
  eprint = {arXiv:2302.11529},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.11529},
  urldate = {2023-02-28},
  abstract = {Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/3RXKTIYC/Pfeiffer et al. - 2023 - Modular Deep Learning.pdf;/Users/lukakuma/Zotero/storage/EAUARW66/2302.html}
}

@inproceedings{pfeifferXGQACrossLingualVisual2022,
  title = {{{xGQA}}: {{Cross-Lingual Visual Question Answering}}},
  shorttitle = {{{xGQA}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Pfeiffer, Jonas and Geigle, Gregor and Kamath, Aishwarya and Steitz, Jan-Martin and Roth, Stefan and Vuli\'c, Ivan and Gurevych, Iryna},
  date = {2022-05},
  pages = {2497--2511},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  url = {https://aclanthology.org/2022.findings-acl.196},
  urldate = {2023-03-03},
  abstract = {Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and\textemdash vice versa\textemdash multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling.},
  eventtitle = {Findings 2022},
  file = {/Users/lukakuma/Zotero/storage/Y5B6R45H/Pfeiffer et al. - 2022 - xGQA Cross-Lingual Visual Question Answering.pdf}
}

@online{phangInvestigatingEfficientlyExtending2022,
  title = {Investigating {{Efficiently Extending Transformers}} for {{Long Input Summarization}}},
  author = {Phang, Jason and Zhao, Yao and Liu, Peter J.},
  date = {2022-08-08},
  number = {arXiv:2208.04347},
  eprint = {arXiv:2208.04347},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.04347},
  urldate = {2022-08-19},
  abstract = {While large pretrained Transformer models have proven highly capable at tackling natural language tasks, handling long sequence inputs continues to be a significant challenge. One such task is long input summarization, where inputs are longer than the maximum input context of most pretrained models. Through an extensive set of experiments, we investigate what model architectural changes and pretraining paradigms can most efficiently adapt a pretrained Transformer for long input summarization. We find that a staggered, blocklocal Transformer with global encoder tokens strikes a good balance of performance and efficiency, and that an additional pretraining phase on long sequences meaningfully improves downstream summarization performance. Based on our findings, we introduce PEGASUS-X, an extension of the PEGASUS model with additional long input pretraining to handle inputs of up to 16K tokens. PEGASUSX achieves strong performance on long input summarization tasks comparable with much larger models while adding few additional parameters and not requiring model parallelism to train.},
  langid = {english},
  pubstate = {preprint},
  keywords = {summarization},
  file = {/Users/lukakuma/Zotero/storage/MBWEHSFL/Phang et al. - 2022 - Investigating Efficiently Extending Transformers f.pdf}
}

@online{phuongFormalAlgorithmsTransformers2022,
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  date = {2022-07-19},
  number = {arXiv:2207.09238},
  eprint = {arXiv:2207.09238},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.09238},
  urldate = {2022-08-08},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/5MLGIARS/Phuong and Hutter - 2022 - Formal Algorithms for Transformers.pdf;/Users/lukakuma/Zotero/storage/LXFY9XFA/2207.html}
}

@online{piantasodiMeaningReferenceLarge2022,
  title = {Meaning without Reference in Large Language Models},
  author = {Piantasodi, Steven T. and Hill, Felix},
  date = {2022-08-04},
  number = {arXiv:2208.02957},
  eprint = {arXiv:2208.02957},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.02957},
  urldate = {2022-08-10},
  abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/FZW585ZZ/Piantasodi and Hill - 2022 - Meaning without reference in large language models.pdf;/Users/lukakuma/Zotero/storage/7YNXJGM5/2208.html}
}

@online{piergiovanniVideoQuestionAnswering2022,
  title = {Video {{Question Answering}} with {{Iterative Video-Text Co-Tokenization}}},
  author = {Piergiovanni, A. J. and Morton, Kairo and Kuo, Weicheng and Ryoo, Michael S. and Angelova, Anelia},
  date = {2022-08-01},
  number = {arXiv:2208.00934},
  eprint = {arXiv:2208.00934},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.00934},
  urldate = {2023-02-07},
  abstract = {Video question answering is a challenging task that requires understanding jointly the language input, the visual information in individual video frames, as well as the temporal information about the events occurring in the video. In this paper, we propose a novel multi-stream video encoder for video question answering that uses multiple video inputs and a new video-text iterative co-tokenization approach to answer a variety of questions related to videos. We experimentally evaluate the model on several datasets, such as MSRVTT-QA, MSVD-QA, IVQA, outperforming the previous state-of-the-art by large margins. Simultaneously, our model reduces the required GFLOPs from 150-360 to only 67, producing a highly efficient video question answering model.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/LWNC8BL2/Piergiovanni et al. - 2022 - Video Question Answering with Iterative Video-Text.pdf;/Users/lukakuma/Zotero/storage/48PTCS3N/2208.html}
}

@video{pieterabbeelCVPR2021Keynote2021,
  title = {{{CVPR}} 2021 {{Keynote}} -- {{Pieter Abbeel}} -- {{Towards}} a {{General Solution}} for {{Robotics}}.},
  editor = {{Pieter Abbeel}},
  date = {2021-06-28},
  url = {https://www.youtube.com/watch?v=l9pwlXXsi7Q},
  urldate = {2022-06-23},
  abstract = {In this talk I share my thoughts on how we might be able to achieve large pre-trained neural networks for robotics, much the way pre-trained models like GPT-x/BERT are standardly used in NLP.  I lay out how we might get there substantial research progress in unsupervised representation learning, in unsupervised (reward-free) reinforcement learning (RL) pre-training, human-in-the-loop RL, and few shot imitation learning.   Talk slides: https://www.dropbox.com/s/r4r3l9ga3u2...},
  editortype = {director}
}

@video{pieterabbeelL2DeepQLearning2021,
  title = {L2 {{Deep Q-Learning}} ({{Foundations}} of {{Deep RL Series}})},
  editor = {{Pieter Abbeel}},
  date = {2021-08-25},
  url = {https://www.youtube.com/watch?v=Psrhxy88zww},
  urldate = {2022-07-19},
  abstract = {Lecture 2 of a 6-lecture series on the Foundations of Deep RL  Topic: Deep Q-Learning Instructor: Pieter Abbeel Slides: https://www.dropbox.com/s/c6v5r6a8lsa...},
  editortype = {director}
}

@online{piktusWebYourOyster2022,
  title = {The {{Web Is Your Oyster}} - {{Knowledge-Intensive NLP}} against a {{Very Large Web Corpus}}},
  author = {Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Okhonko, Dmytro and Broscheit, Samuel and Izacard, Gautier and Lewis, Patrick and O\u{g}uz, Barlas and Grave, Edouard and Yih, Wen-tau and Riedel, Sebastian},
  date = {2022-05-24},
  number = {arXiv:2112.09924},
  eprint = {arXiv:2112.09924},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.09924},
  urldate = {2022-11-21},
  abstract = {In order to address increasing demands of real-world applications, the research for knowledge-intensive NLP (KI-NLP) should advance by capturing the challenges of a truly open-domain environment: web-scale knowledge, lack of structure, inconsistent quality and noise. To this end, we propose a new setup for evaluating existing knowledge intensive tasks in which we generalize the background corpus to a universal web snapshot. We investigate a slate of NLP tasks which rely on knowledge - either factual or common sense, and ask systems to use a subset of CCNet - the Sphere corpus - as a knowledge source. In contrast to Wikipedia, otherwise a common background corpus in KI-NLP, Sphere is orders of magnitude larger and better reflects the full diversity of knowledge on the web. Despite potential gaps in coverage, challenges of scale, lack of structure and lower quality, we find that retrieval from Sphere enables a state of the art system to match and even outperform Wikipedia-based models on several tasks. We also observe that while a dense index can outperform a sparse BM25 baseline on Wikipedia, on Sphere this is not yet possible. To facilitate further research and minimise the community's reliance on proprietary, black-box search engines, we share our indices, evaluation metrics and infrastructure.},
  pubstate = {preprint},
  keywords = {4-document retrieval,read},
  file = {/Users/lukakuma/Zotero/storage/3CUYH2A5/Piktus et al. - 2022 - The Web Is Your Oyster - Knowledge-Intensive NLP a.pdf;/Users/lukakuma/Zotero/storage/TUJNU4T9/2112.html}
}

@online{pilaultInteractiveChainPromptingAmbiguityResolution2023,
  title = {Interactive-{{Chain-Prompting}}: {{Ambiguity Resolution}} for {{Crosslingual Conditional Generation}} with {{Interaction}}},
  shorttitle = {Interactive-{{Chain-Prompting}}},
  author = {Pilault, Jonathan and Garcia, Xavier and Bra\v{z}inskas, Arthur and Firat, Orhan},
  date = {2023-01-24},
  number = {arXiv:2301.10309},
  eprint = {arXiv:2301.10309},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.10309},
  urldate = {2023-01-26},
  abstract = {Crosslingual conditional generation (e.g., machine translation) has long enjoyed the benefits of scaling. Nonetheless, there are still issues that scale alone may not overcome. A source query in one language, for instance, may yield several translation options in another language without any extra context. Only one translation could be acceptable however, depending on the translator's preferences and goals. Choosing the incorrect option might significantly affect translation usefulness and quality. We propose a novel method interactive-chain prompting -- a series of question, answering and generation intermediate steps between a Translator model and a User model -- that reduces translations into a list of subproblems addressing ambiguities and then resolving such subproblems before producing the final text to be translated. To check ambiguity resolution capabilities and evaluate translation quality, we create a dataset exhibiting different linguistic phenomena which leads to ambiguities at inference for four languages. To encourage further exploration in this direction, we release all datasets. We note that interactive-chain prompting, using eight interactions as exemplars, consistently surpasses prompt-based methods with direct access to background information to resolve ambiguities.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/7H5R72DV/Pilault et al. - 2023 - Interactive-Chain-Prompting Ambiguity Resolution .pdf;/Users/lukakuma/Zotero/storage/HSZANDQY/2301.html}
}

@inproceedings{pislarWhenShouldAgents2021,
  title = {When Should Agents Explore?},
  author = {Pislar, Miruna and Szepesvari, David and Ostrovski, Georg and Borsa, Diana L. and Schaul, Tom},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=dEwfxt14bca},
  urldate = {2022-05-15},
  abstract = {Exploration remains a central challenge for reinforcement learning (RL). Virtually all existing methods share the feature of a *monolithic* behaviour policy that changes only gradually (at best)....},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/M2B6CS2B/Pislar et al. - 2021 - When should agents explore.pdf;/Users/lukakuma/Zotero/storage/GLL3TM6F/forum.html}
}

@online{plappertMultiGoalReinforcementLearning2018,
  title = {Multi-{{Goal Reinforcement Learning}}: {{Challenging Robotics Environments}} and {{Request}} for {{Research}}},
  shorttitle = {Multi-{{Goal Reinforcement Learning}}},
  author = {Plappert, Matthias and Andrychowicz, Marcin and Ray, Alex and McGrew, Bob and Baker, Bowen and Powell, Glenn and Schneider, Jonas and Tobin, Josh and Chociej, Maciek and Welinder, Peter and Kumar, Vikash and Zaremba, Wojciech},
  date = {2018-03-10},
  number = {arXiv:1802.09464},
  eprint = {arXiv:1802.09464},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.09464},
  urldate = {2022-07-23},
  abstract = {The purpose of this technical report is two-fold. First of all, it introduces a suite of challenging continuous control tasks (integrated with OpenAI Gym) based on currently existing robotics hardware. The tasks include pushing, sliding and pick \& place with a Fetch robotic arm as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent is told what to do using an additional input. The second part of the paper presents a set of concrete research ideas for improving RL algorithms, most of which are related to Multi-Goal RL and Hindsight Experience Replay.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/EJ46S6KL/Plappert et al. - 2018 - Multi-Goal Reinforcement Learning Challenging Rob.pdf;/Users/lukakuma/Zotero/storage/S5DH9LE6/1802.html}
}

@online{PoliticalIdeologies21st2022,
  title = {Political {{Ideologies}} for the 21st {{Century}}},
  date = {2022-02-17},
  url = {https://www.radicalxchange.org/media/blog/political-ideologies-for-the-21st-century/},
  urldate = {2022-03-08},
  abstract = {We are a community of activists, artists, entrepreneurs, and scholars committed to using mechanism design to inspire radical social change.},
  langid = {english},
  organization = {{RadicalxChange}},
  file = {/Users/lukakuma/Zotero/storage/MPLVDM3L/political-ideologies-for-the-21st-century.html}
}

@online{poluFormalMathematicsStatement2022a,
  title = {Formal {{Mathematics Statement Curriculum Learning}}},
  author = {Polu, Stanislas and Han, Jesse Michael and Zheng, Kunhao and Baksys, Mantas and Babuschkin, Igor and Sutskever, Ilya},
  date = {2022-02-02},
  number = {arXiv:2202.01344},
  eprint = {arXiv:2202.01344},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.01344},
  urldate = {2022-11-19},
  abstract = {We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.},
  pubstate = {preprint},
  keywords = {formal reasoning,OpenAI,read},
  file = {/Users/lukakuma/Zotero/storage/X58TMMV4/Polu et al. - 2022 - Formal Mathematics Statement Curriculum Learning.pdf;/Users/lukakuma/Zotero/storage/8XG97FSQ/2202.html}
}

@online{poluGenerativeLanguageModeling2020,
  title = {Generative {{Language Modeling}} for {{Automated Theorem Proving}}},
  author = {Polu, Stanislas and Sutskever, Ilya},
  date = {2020-09-07},
  number = {arXiv:2009.03393},
  eprint = {arXiv:2009.03393},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.03393},
  urldate = {2022-11-19},
  abstract = {We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.},
  pubstate = {preprint},
  keywords = {formal reasoning,read},
  file = {/Users/lukakuma/Zotero/storage/GLF6DN6L/Polu and Sutskever - 2020 - Generative Language Modeling for Automated Theorem.pdf;/Users/lukakuma/Zotero/storage/9X2DX5V8/2009.html}
}

@online{pontiCombiningModularSkills2022,
  title = {Combining {{Modular Skills}} in {{Multitask Learning}}},
  author = {Ponti, Edoardo M. and Sordoni, Alessandro and Bengio, Yoshua and Reddy, Siva},
  date = {2022-03-01},
  number = {arXiv:2202.13914},
  eprint = {arXiv:2202.13914},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.13914},
  urldate = {2023-03-03},
  abstract = {A modular design encourages neural models to disentangle and recombine different facets of knowledge to generalise more systematically to new tasks. In this work, we assume that each task is associated with a subset of latent discrete skills from a (potentially small) inventory. In turn, skills correspond to parameter-efficient (sparse / low-rank) model parameterisations. By jointly learning these and a task-skill allocation matrix, the network for each task is instantiated as the average of the parameters of active skills. To favour non-trivial soft partitions of skills across tasks, we experiment with a series of inductive biases, such as an Indian Buffet Process prior and a two-speed learning rate. We evaluate our latent-skill model on two main settings: 1) multitask reinforcement learning for grounded instruction following on 8 levels of the BabyAI platform; and 2) few-shot adaptation of pre-trained text-to-text generative models on CrossFit, a benchmark comprising 160 NLP tasks. We find that the modular design of a network significantly increases sample efficiency in reinforcement learning and few-shot generalisation in supervised learning, compared to baselines with fully shared, task-specific, or conditionally generated parameters where knowledge is entangled across tasks. In addition, we show how discrete skills help interpretability, as they yield an explicit hierarchy of tasks.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/QDC8HIBH/Ponti et al. - 2022 - Combining Modular Skills in Multitask Learning.pdf;/Users/lukakuma/Zotero/storage/2ZWL32ZL/2202.html}
}

@online{popeEfficientlyScalingTransformer2022,
  title = {Efficiently {{Scaling Transformer Inference}}},
  author = {Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  date = {2022-11-09},
  number = {arXiv:2211.05102},
  eprint = {arXiv:2211.05102},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.05102},
  urldate = {2023-01-31},
  abstract = {We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32x larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76\% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/2P868YVR/Pope et al. - 2022 - Efficiently Scaling Transformer Inference.pdf;/Users/lukakuma/Zotero/storage/Y4JADM8H/2211.html}
}

@online{portelasAutomaticCurriculumLearning2020,
  title = {Automatic {{Curriculum Learning For Deep RL}}: {{A Short Survey}}},
  shorttitle = {Automatic {{Curriculum Learning For Deep RL}}},
  author = {Portelas, R\'emy and Colas, C\'edric and Weng, Lilian and Hofmann, Katja and Oudeyer, Pierre-Yves},
  date = {2020-05-28},
  number = {arXiv:2003.04664},
  eprint = {arXiv:2003.04664},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2003.04664},
  urldate = {2022-07-20},
  abstract = {Automatic Curriculum Learning (ACL) has become a cornerstone of recent successes in Deep Reinforcement Learning (DRL).These methods shape the learning trajectories of agents by challenging them with tasks adapted to their capacities. In recent years, they have been used to improve sample efficiency and asymptotic performance, to organize exploration, to encourage generalization or to solve sparse reward problems, among others. The ambition of this work is dual: 1) to present a compact and accessible introduction to the Automatic Curriculum Learning literature and 2) to draw a bigger picture of the current state of the art in ACL to encourage the cross-breeding of existing concepts and the emergence of new ideas.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/XYN7RA2H/Portelas et al. - 2020 - Automatic Curriculum Learning For Deep RL A Short.pdf;/Users/lukakuma/Zotero/storage/CNYXNGNY/2003.html}
}

@article{posnerPropertyOnlyAnother2017,
  title = {Property {{Is Only Another Name}} for {{Monopoly}}},
  author = {Posner, Eric A. and Weyl, E. Glen},
  date = {2017},
  journaltitle = {Journal of Legal Analysis},
  volume = {9},
  number = {1},
  pages = {51--123},
  issn = {2161-7201, 1946-5319},
  url = {https://academic.oup.com/jla/article-lookup/doi/10.1093/jla/lax001},
  urldate = {2022-03-16},
  abstract = {The existing system of private property interferes with allocative efficiency by giving owners the power to hold out for excessive prices. We propose a remedy in the form of a tax on property, based on the value self-assessed by its owner at intervals, along with a requirement that the owner sell the property to any third party willing to pay a price equal to the self-assessed value. The tax rate would reflect a tradeoff between gains from allocative efficiency and losses to investment efficiency, likely in the range of 5 to 10 percent annually for most assets. We discuss the detailed design of this system from an economic and legal perspective.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/B22GGUJ5/Posner and Weyl - 2017 - Property Is Only Another Name for Monopoly.pdf}
}

@online{pressMeasuringNarrowingCompositionality2022a,
  title = {Measuring and {{Narrowing}} the {{Compositionality Gap}} in {{Language Models}}},
  author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A. and Lewis, Mike},
  date = {2022-10-07},
  number = {arXiv:2210.03350},
  eprint = {arXiv:2210.03350},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.03350},
  urldate = {2022-11-20},
  abstract = {We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly instead of implicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/M4C98QD7/Press et al. - 2022 - Measuring and Narrowing the Compositionality Gap i.pdf;/Users/lukakuma/Zotero/storage/BPMKGZY6/2210.html}
}

@online{pressTrainShortTest2022,
  title = {Train {{Short}}, {{Test Long}}: {{Attention}} with {{Linear Biases Enables Input Length Extrapolation}}},
  shorttitle = {Train {{Short}}, {{Test Long}}},
  author = {Press, Ofir and Smith, Noah A. and Lewis, Mike},
  date = {2022-04-22},
  number = {arXiv:2108.12409},
  eprint = {arXiv:2108.12409},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.12409},
  urldate = {2022-09-06},
  abstract = {Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11\% faster and using 11\% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/K2V92N5U/Press et al. - 2022 - Train Short, Test Long Attention with Linear Bias.pdf;/Users/lukakuma/Zotero/storage/T8AVXJXI/2108.html}
}

@online{ProgrammingLanguageFuture,
  title = {A {{Programming Language}} for {{Future Interests}} | {{Yale Journal}} of {{Law}} \& {{Technology}}},
  date = {2022},
  url = {https://yjolt.org/programming-language-future-interests},
  urldate = {2023-02-08},
  file = {/Users/lukakuma/Zotero/storage/WDXKU6MT/grimmelmann_-_a_programming_language_for_future_interests.pdf;/Users/lukakuma/Zotero/storage/H2P6PPHD/programming-language-future-interests.html}
}

@inproceedings{punyFrameAveragingInvariant2021,
  title = {Frame {{Averaging}} for {{Invariant}} and {{Equivariant Network Design}}},
  author = {Puny, Omri and Atzmon, Matan and Smith, Edward J. and Misra, Ishan and Grover, Aditya and Ben-Hamu, Heli and Lipman, Yaron},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=zIUyj55nXR},
  urldate = {2022-05-15},
  abstract = {Many machine learning tasks involve learning functions that are known to be invariant or equivariant to certain symmetries of the input data. However, it is often challenging to design neural...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/JRRPKZ7Q/Puny et al. - 2021 - Frame Averaging for Invariant and Equivariant Netw.pdf;/Users/lukakuma/Zotero/storage/TPP3U6W5/forum.html}
}

@online{qiaoReasoningLanguageModel2022,
  title = {Reasoning with {{Language Model Prompting}}: {{A Survey}}},
  shorttitle = {Reasoning with {{Language Model Prompting}}},
  author = {Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  date = {2022-12-19},
  number = {arXiv:2212.09597},
  eprint = {arXiv:2212.09597},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.09597},
  urldate = {2023-02-20},
  abstract = {Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/M4BPFHHT/Qiao et al. - 2022 - Reasoning with Language Model Prompting A Survey.pdf;/Users/lukakuma/Zotero/storage/YSNPFXKI/2212.html}
}

@article{radfordImprovingLanguageUnderstanding2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  date = {2018},
  pages = {12},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  keywords = {OpenAI,read},
  file = {/Users/lukakuma/Zotero/storage/RJK92AWI/language_understanding_paper.pdf}
}

@article{radfordLanguageModelsAre2018,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  date = {2018},
  pages = {24},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  keywords = {OpenAI,read},
  file = {/Users/lukakuma/Zotero/storage/U8KN6G57/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@online{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  number = {arXiv:2103.00020},
  eprint = {arXiv:2103.00020},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2022-11-15},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/85QCYMXK/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;/Users/lukakuma/Zotero/storage/D8ZPXFQE/2103.html}
}

@article{radfordRobustSpeechRecognition2022,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  date = {2022-09-21},
  pages = {28},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  langid = {english},
  keywords = {OpenAI,read},
  file = {/Users/lukakuma/Zotero/storage/LDQ327ZZ/Radford et al. - Robust Speech Recognition via Large-Scale Weak Sup.pdf}
}

@unpublished{raeCompressiveTransformersLongRange2019,
  title = {Compressive {{Transformers}} for {{Long-Range Sequence Modelling}}},
  author = {Rae, Jack W. and Potapenko, Anna and Jayakumar, Siddhant M. and Lillicrap, Timothy P.},
  date = {2019-11-13},
  number = {arXiv:1911.05507},
  eprint = {1911.05507},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1911.05507},
  urldate = {2022-06-07},
  abstract = {We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.},
  file = {/Users/lukakuma/Zotero/storage/GRVL2VG4/Rae et al. - 2019 - Compressive Transformers for Long-Range Sequence M.pdf;/Users/lukakuma/Zotero/storage/FW9BRLIX/1911.html}
}

@unpublished{raeScalingLanguageModels2022,
  title = {Scaling {{Language Models}}: {{Methods}}, {{Analysis}} \& {{Insights}} from {{Training Gopher}}},
  shorttitle = {Scaling {{Language Models}}},
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and family=Driessche, given=George, prefix=van den, useprefix=false and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and family=Autume, given=Cyprien de Masson, prefix=d', useprefix=true and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  date = {2022-01-21},
  eprint = {2112.11446},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.11446},
  urldate = {2022-05-09},
  abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
  keywords = {(ext) Flamingo,DeepMind,read,scaling law},
  file = {/Users/lukakuma/Zotero/storage/IWPQFUSP/Rae et al. - 2022 - Scaling Language Models Methods, Analysis & Insig.pdf;/Users/lukakuma/Zotero/storage/IHY9X9UM/2112.html}
}

@online{raffelCallBuildModels2021,
  title = {A {{Call}} to {{Build Models Like We Build Open-Source Software}}},
  author = {Raffel, Colin},
  date = {2021-12-08},
  url = {https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html},
  urldate = {2022-09-05},
  keywords = {*open source AI},
  file = {/Users/lukakuma/Zotero/storage/35VBYDQW/a-call-to-build-models-like-we-build-open-source-software.html}
}

@online{raffelExploringLimitsTransfer2020,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  date = {2020-07-28},
  number = {arXiv:1910.10683},
  eprint = {arXiv:1910.10683},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.10683},
  urldate = {2022-08-09},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/L7XU5HAZ/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf;/Users/lukakuma/Zotero/storage/KGWEPB26/1910.html}
}

@inproceedings{rajaniExplainYourselfLeveraging2019,
  title = {Explain {{Yourself}}! {{Leveraging Language Models}} for {{Commonsense Reasoning}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  date = {2019-07},
  pages = {4932--4942},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  url = {https://aclanthology.org/P19-1487},
  urldate = {2022-12-23},
  abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10\% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
  eventtitle = {{{ACL}} 2019},
  keywords = {self-explain},
  file = {/Users/lukakuma/Zotero/storage/AQV7DYPK/Rajani et al. - 2019 - Explain Yourself! Leveraging Language Models for C.pdf}
}

@online{rajbhandariDeepSpeedMoEAdvancingMixtureofExperts2022,
  title = {{{DeepSpeed-MoE}}: {{Advancing Mixture-of-Experts Inference}} and {{Training}} to {{Power Next-Generation AI Scale}}},
  shorttitle = {{{DeepSpeed-MoE}}},
  author = {Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  date = {2022-07-21},
  number = {arXiv:2201.05596},
  eprint = {arXiv:2201.05596},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.05596},
  urldate = {2022-07-29},
  abstract = {As the training of giant dense models hits the boundary on the availability and capability of the hardware resources today, Mixture-of-Experts (MoE) models become one of the most promising model architectures due to their significant training cost reduction compared to a quality-equivalent dense model. Its training cost saving is demonstrated from encoder-decoder models (prior works) to a 5x saving for auto-aggressive language models (this work along with parallel explorations). However, due to the much larger model size and unique architecture, how to provide fast MoE model inference remains challenging and unsolved, limiting its practical usage. To tackle this, we present DeepSpeed-MoE, an end-to-end MoE training and inference solution as part of the DeepSpeed library, including novel MoE architecture designs and model compression techniques that reduce MoE model size by up to 3.7x, and a highly optimized inference system that provides 7.3x better latency and cost compared to existing MoE inference solutions. DeepSpeed-MoE offers an unprecedented scale and efficiency to serve massive MoE models with up to 4.5x faster and 9x cheaper inference compared to quality-equivalent dense models. We hope our innovations and systems help open a promising path to new directions in the large model landscape, a shift from dense to sparse MoE models, where training and deploying higher-quality models with fewer resources becomes more widely possible.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/II7BIAHG/Rajbhandari et al. - 2022 - DeepSpeed-MoE Advancing Mixture-of-Experts Infere.pdf;/Users/lukakuma/Zotero/storage/JIVZIE78/2201.html}
}

@online{rajbhandariZeROMemoryOptimizations2020,
  title = {{{ZeRO}}: {{Memory Optimizations Toward Training Trillion Parameter Models}}},
  shorttitle = {{{ZeRO}}},
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  date = {2020-05-13},
  number = {arXiv:1910.02054},
  eprint = {arXiv:1910.02054},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.02054},
  urldate = {2022-08-09},
  abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.},
  pubstate = {preprint},
  keywords = {distributed training,read},
  file = {/Users/lukakuma/Zotero/storage/J9WIT6QG/Rajbhandari et al. - 2020 - ZeRO Memory Optimizations Toward Training Trillio.pdf;/Users/lukakuma/Zotero/storage/E9WRRJTJ/1910.html}
}

@inproceedings{ramachandranStandAloneSelfAttentionVision2019,
  title = {Stand-{{Alone Self-Attention}} in {{Vision Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jon},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html},
  urldate = {2022-08-03},
  abstract = {Convolutions are a fundamental building block of modern  computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention to ResNet-50 produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12\% fewer FLOPS and 29\% fewer parameters. On COCO object detection, a fully self-attention model matches the mAP of a baseline RetinaNet while having 39\% fewer FLOPS and 34\% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.},
  file = {/Users/lukakuma/Zotero/storage/DQAB5LSV/Ramachandran et al. - 2019 - Stand-Alone Self-Attention in Vision Models.pdf}
}

@inproceedings{ramakrishnanEnvironmentPredictiveCoding2021,
  title = {Environment {{Predictive Coding}} for {{Visual Navigation}}},
  author = {Ramakrishnan, Santhosh Kumar and Nagarajan, Tushar and Al-Halah, Ziad and Grauman, Kristen},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=DBiQQYWykyy},
  urldate = {2022-05-15},
  abstract = {We introduce environment predictive coding, a self-supervised approach to learn environment-level representations for embodied agents. In contrast to prior work on self-supervised learning for...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/VZ64SAG6/Ramakrishnan et al. - 2021 - Environment Predictive Coding for Visual Navigatio.pdf}
}

@online{ramamurthyReinforcementLearningNot2022,
  title = {Is {{Reinforcement Learning}} ({{Not}}) for {{Natural Language Processing}}?: {{Benchmarks}}, {{Baselines}}, and {{Building Blocks}} for {{Natural Language Policy Optimization}}},
  shorttitle = {Is {{Reinforcement Learning}} ({{Not}}) for {{Natural Language Processing}}?},
  author = {Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kiant\'e and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin},
  date = {2022-11-24},
  number = {arXiv:2210.01241},
  eprint = {arXiv:2210.01241},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.01241},
  urldate = {2023-02-20},
  abstract = {We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization)\vphantom\{\} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluation.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/KNDU3V76/Ramamurthy et al. - 2022 - Is Reinforcement Learning (Not) for Natural Langua.pdf;/Users/lukakuma/Zotero/storage/MJIA8VHV/2210.html}
}

@article{rameshHierarchicalTextConditionalImage2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  date = {2022},
  pages = {26},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  langid = {english},
  keywords = {*diffusion,image generation,OpenAI,read},
  file = {/Users/lukakuma/Zotero/storage/QGHTZ3NS/Ramesh et al. - Hierarchical Text-Conditional Image Generation wit.pdf}
}

@unpublished{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2102.12092},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.12092},
  urldate = {2021-03-15},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  keywords = {OpenAI,read},
  file = {/Users/lukakuma/Zotero/storage/GZN4252Z/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf;/Users/lukakuma/Zotero/storage/DJW95QYA/2102.html}
}

@online{rampasFastTextConditionalDiscrete2022,
  title = {Fast {{Text-Conditional Discrete Denoising}} on {{Vector-Quantized Latent Spaces}}},
  author = {Rampas, Dominic and Pernias, Pablo and Zhong, Elea and Aubreville, Marc},
  date = {2022-11-14},
  number = {arXiv:2211.07292},
  eprint = {arXiv:2211.07292},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.07292},
  urldate = {2022-11-16},
  abstract = {Conditional text-to-image generation has seen countless recent improvements in terms of quality, diversity and fidelity. Nevertheless, most state-of-the-art models require numerous inference steps to produce faithful generations, resulting in performance bottlenecks for end-user applications. In this paper we introduce Paella, a novel text-to-image model requiring less than 10 steps to sample high-fidelity images, using a speed-optimized architecture allowing to sample a single image in less than 500 ms, while having 573M parameters. The model operates on a compressed \& quantized latent space, it is conditioned on CLIP embeddings and uses an improved sampling function over previous works. Aside from text-conditional image generation, our model is able to do latent space interpolation and image manipulations such as inpainting, outpainting, and structural editing. We release all of our code and pretrained models at https://github.com/dome272/Paella},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/Q3BTQ483/Rampas et al. - 2022 - Fast Text-Conditional Discrete Denoising on Vector.pdf;/Users/lukakuma/Zotero/storage/QR3QSULD/2211.html}
}

@online{rasExplainableDeepLearning2021,
  title = {Explainable {{Deep Learning}}: {{A Field Guide}} for the {{Uninitiated}}},
  shorttitle = {Explainable {{Deep Learning}}},
  author = {Ras, Gabrielle and Xie, Ning and family=Gerven, given=Marcel, prefix=van, useprefix=true and Doran, Derek},
  date = {2021-09-13},
  number = {arXiv:2004.14545},
  eprint = {arXiv:2004.14545},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.14545},
  urldate = {2022-12-28},
  abstract = {Deep neural networks (DNNs) have become a proven and indispensable machine learning tool. As a black-box model, it remains difficult to diagnose what aspects of the model's input drive the decisions of a DNN. In countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that DNN decisions are driven by aspects appropriate in the context of its use. The development of methods and studies enabling the explanation of a DNN's decisions has thus blossomed into an active, broad area of research. A practitioner wanting to study explainable deep learning may be intimidated by the plethora of orthogonal directions the field has taken. This complexity is further exacerbated by competing definitions of what it means ``to explain'' the actions of a DNN and to evaluate an approach's ``ability to explain''. This article offers a field guide to explore the space of explainable deep learning aimed at those uninitiated in the field. The field guide: i) Introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning, ii) discusses the evaluations for model explanations, iii) places explainability in the context of other related deep learning research areas, and iv) finally elaborates on user-oriented explanation designing and potential future directions on explainable deep learning. We hope the guide is used as an easy-to-digest starting point for those just embarking on research in this field.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/33WPMP97/Ras et al. - 2021 - Explainable Deep Learning A Field Guide for the U.pdf;/Users/lukakuma/Zotero/storage/IQQ6BVJC/2004.html}
}

@online{rashkinMeasuringAttributionNatural2022,
  title = {Measuring {{Attribution}} in {{Natural Language Generation Models}}},
  author = {Rashkin, Hannah and Nikolaev, Vitaly and Lamm, Matthew and Aroyo, Lora and Collins, Michael and Das, Dipanjan and Petrov, Slav and Tomar, Gaurav Singh and Turc, Iulia and Reitter, David},
  date = {2022-08-02},
  number = {arXiv:2112.12870},
  eprint = {arXiv:2112.12870},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.12870},
  urldate = {2022-12-05},
  abstract = {With recent improvements in natural language generation (NLG) models for various applications, it has become imperative to have the means to identify and evaluate whether NLG output is only sharing verifiable information about the external world. In this work, we present a new evaluation framework entitled Attributable to Identified Sources (AIS) for assessing the output of natural language generation models, when such output pertains to the external world. We first define AIS and introduce a two-stage annotation pipeline for allowing annotators to appropriately evaluate model output according to AIS guidelines. We empirically validate this approach on generation datasets spanning three tasks (two conversational QA datasets, a summarization dataset, and a table-to-text dataset) via human evaluation studies that suggest that AIS could serve as a common framework for measuring whether model-generated statements are supported by underlying sources. We release guidelines for the human evaluation studies.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/6NHIRPFW/Rashkin et al. - 2022 - Measuring Attribution in Natural Language Generati.pdf;/Users/lukakuma/Zotero/storage/INB4VKKI/2112.html}
}

@online{razdaibiedinaProgressivePromptsContinual2023,
  title = {Progressive {{Prompts}}: {{Continual Learning}} for {{Language Models}}},
  shorttitle = {Progressive {{Prompts}}},
  author = {Razdaibiedina, Anastasia and Mao, Yuning and Hou, Rui and Khabsa, Madian and Lewis, Mike and Almahairi, Amjad},
  date = {2023-01-28},
  number = {arXiv:2301.12314},
  eprint = {arXiv:2301.12314},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.12314},
  urldate = {2023-02-03},
  abstract = {We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement {$>$}20\% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.},
  pubstate = {preprint},
  keywords = {*peft,3-prefix tuning},
  file = {/Users/lukakuma/Zotero/storage/PM3CSZ8Q/Razdaibiedina et al. - 2023 - Progressive Prompts Continual Learning for Langua.pdf;/Users/lukakuma/Zotero/storage/JY7G5394/2301.html}
}

@online{rebuffiLearningMultipleVisual2017,
  title = {Learning Multiple Visual Domains with Residual Adapters},
  author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  date = {2017-11-27},
  number = {arXiv:1705.08045},
  eprint = {arXiv:1705.08045},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.08045},
  urldate = {2023-03-01},
  abstract = {There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to recognize well uniformly.},
  pubstate = {preprint},
  keywords = {2-adapter},
  file = {/Users/lukakuma/Zotero/storage/74G45YMX/Rebuffi et al. - 2017 - Learning multiple visual domains with residual ada.pdf;/Users/lukakuma/Zotero/storage/BNHCNQBT/1705.html}
}

@online{reedGeneralistAgent2022,
  title = {A {{Generalist Agent}}},
  author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2022-05-19},
  number = {arXiv:2205.06175},
  eprint = {arXiv:2205.06175},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.06175},
  urldate = {2022-07-28},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  pubstate = {preprint},
  keywords = {7-multimodal,action/physical,DeepMind,read},
  file = {/Users/lukakuma/Zotero/storage/LN9J7DGT/A Generalist Agent.pdf;/Users/lukakuma/Zotero/storage/Z827VKIU/Reed et al. - 2022 - A Generalist Agent.pdf;/Users/lukakuma/Zotero/storage/UY38NDN5/2205.html}
}

@online{reidCanWikipediaHelp2022,
  title = {Can {{Wikipedia Help Offline Reinforcement Learning}}?},
  author = {Reid, Machel and Yamada, Yutaro and Gu, Shixiang Shane},
  date = {2022-03-14},
  number = {arXiv:2201.12122},
  eprint = {arXiv:2201.12122},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.12122},
  urldate = {2022-07-23},
  abstract = {Fine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/I3VBMM7Z/Reid et al. - 2022 - Can Wikipedia Help Offline Reinforcement Learning.pdf;/Users/lukakuma/Zotero/storage/XAB3ZWUV/2201.html}
}

@online{renOutofDistributionDetectionSelective2022,
  title = {Out-of-{{Distribution Detection}} and {{Selective Generation}} for {{Conditional Language Models}}},
  author = {Ren, Jie and Luo, Jiaming and Zhao, Yao and Krishna, Kundan and Saleh, Mohammad and Lakshminarayanan, Balaji and Liu, Peter J.},
  date = {2022-09-30},
  number = {arXiv:2209.15558},
  eprint = {arXiv:2209.15558},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.15558},
  urldate = {2023-02-02},
  abstract = {Machine learning algorithms typically assume independent and identically distributed samples in training and at test time. Much work has shown that high-performing ML classifiers can degrade significantly and provide overly-confident, wrong classification predictions, particularly for out-of-distribution (OOD) inputs. Conditional language models (CLMs) are predominantly trained to classify the next token in an output sequence, and may suffer even worse degradation on OOD inputs as the prediction is done auto-regressively over many steps. Furthermore, the space of potential low-quality outputs is larger as arbitrary text can be generated and it is important to know when to trust the generated output. We present a highly accurate and lightweight OOD detection method for CLMs, and demonstrate its effectiveness on abstractive summarization and translation. We also show how our method can be used under the common and realistic setting of distribution shift for selective generation (analogous to selective prediction for classification) of high-quality outputs, while automatically abstaining from low-quality ones, enabling safer deployment of generative language models.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/TERHDBSG/Ren et al. - 2022 - Out-of-Distribution Detection and Selective Genera.pdf;/Users/lukakuma/Zotero/storage/8GWS59PJ/2209.html}
}

@online{reynoldsPromptProgrammingLarge2021,
  title = {Prompt {{Programming}} for {{Large Language Models}}: {{Beyond}} the {{Few-Shot Paradigm}}},
  shorttitle = {Prompt {{Programming}} for {{Large Language Models}}},
  author = {Reynolds, Laria and McDonell, Kyle},
  date = {2021-02-15},
  number = {arXiv:2102.07350},
  eprint = {arXiv:2102.07350},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.07350},
  urldate = {2022-12-12},
  abstract = {Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. In this work, we discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/48XW9DYK/Reynolds and McDonell - 2021 - Prompt Programming for Large Language Models Beyo.pdf;/Users/lukakuma/Zotero/storage/MLLLICK4/2102.html}
}

@article{rifaiogluRecentApplicationsDeep2019,
  title = {Recent Applications of Deep Learning and Machine Intelligence on in Silico Drug Discovery: Methods, Tools and Databases},
  shorttitle = {Recent Applications of Deep Learning and Machine Intelligence on in Silico Drug Discovery},
  author = {Rifaioglu, Ahmet Sureyya and Atas, Heval and Martin, Maria Jesus and Cetin-Atalay, Rengul and Atalay, Volkan and Do\u{g}an, Tunca},
  date = {2019-09-27},
  journaltitle = {Briefings in Bioinformatics},
  shortjournal = {Briefings in Bioinformatics},
  volume = {20},
  number = {5},
  pages = {1878--1912},
  issn = {1477-4054},
  url = {https://doi.org/10.1093/bib/bby061},
  urldate = {2022-05-01},
  abstract = {The identification of interactions between drugs/compounds and their targets is crucial for the development of new drugs. In vitro screening experiments (i.e. bioassays) are frequently used for this purpose; however, experimental approaches are insufficient to explore novel drug-target interactions, mainly because of feasibility problems, as they are labour intensive, costly and time consuming. A computational field known as `virtual screening' (VS) has emerged in the past decades to aid experimental drug discovery studies by statistically estimating unknown bio-interactions between compounds and biological targets. These methods use the physico-chemical and structural properties of compounds and/or target proteins along with the experimentally verified bio-interaction information to generate predictive models. Lately, sophisticated machine learning techniques are applied in VS to elevate the predictive performance.The objective of this study is to examine and discuss the recent applications of machine learning techniques in VS, including deep learning, which became highly popular after giving rise to epochal developments in the fields of computer vision and natural language processing. The past 3 years have witnessed an unprecedented amount of research studies considering the application of deep learning in biomedicine, including computational drug discovery. In this review, we first describe the main instruments of VS methods, including compound and protein features (i.e. representations and descriptors), frequently used libraries and toolkits for VS, bioactivity databases and gold-standard data sets for system training and benchmarking. We subsequently review recent VS studies with a strong emphasis on deep learning applications. Finally, we discuss the present state of the field, including the current challenges and suggest future directions. We believe that this survey will provide insight to the researchers working in the field of computational drug discovery in terms of comprehending and developing novel bio-prediction methods.},
  file = {/Users/lukakuma/Zotero/storage/755HUAT8/Rifaioglu et al. - 2019 - Recent applications of deep learning and machine i.pdf;/Users/lukakuma/Zotero/storage/85JQW7A6/5062947.html}
}

@online{ritaRolePopulationHeterogeneity2022,
  title = {On the Role of Population Heterogeneity in Emergent Communication},
  author = {Rita, Mathieu and Strub, Florian and Grill, Jean-Bastien and Pietquin, Olivier and Dupoux, Emmanuel},
  date = {2022-04-27},
  number = {arXiv:2204.12982},
  eprint = {arXiv:2204.12982},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.12982},
  urldate = {2023-02-20},
  abstract = {Populations have often been perceived as a structuring component for language to emerge and evolve: the larger the population, the more structured the language. While this observation is widespread in the sociolinguistic literature, it has not been consistently reproduced in computer simulations with neural agents. In this paper, we thus aim to clarify this apparent contradiction. We explore emergent language properties by varying agent population size in the speaker-listener Lewis Game. After reproducing the experimental difference, we challenge the simulation assumption that the agent community is homogeneous. We first investigate how speaker-listener asymmetry alters language structure to examine two potential diversity factors: training speed and network capacity. We find out that emergent language properties are only altered by the relative difference of learning speeds between speaker and listener, and not by their absolute values. From then, we leverage this observation to control population heterogeneity without introducing confounding factors. We finally show that introducing such training speed heterogeneities naturally sort out the initial contradiction: larger simulated communities start developing more stable and structured languages.},
  pubstate = {preprint},
  keywords = {multiagent},
  file = {/Users/lukakuma/Zotero/storage/666N4Q7Y/Rita et al. - 2022 - On the role of population heterogeneity in emergen.pdf;/Users/lukakuma/Zotero/storage/RPT74AUR/2204.html}
}

@online{robertsHowMuchKnowledge2020,
  title = {How {{Much Knowledge Can You Pack Into}} the {{Parameters}} of a {{Language Model}}?},
  author = {Roberts, Adam and Raffel, Colin and Shazeer, Noam},
  date = {2020-10-05},
  number = {arXiv:2002.08910},
  eprint = {arXiv:2002.08910},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.08910},
  urldate = {2022-11-02},
  abstract = {It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.1 President Franklin {$<$}M{$>$} born {$<$}M{$>$} January 1882.},
  langid = {english},
  pubstate = {preprint},
  keywords = {knowledge},
  file = {/Users/lukakuma/Zotero/storage/UP5FIPH2/Roberts et al. - 2020 - How Much Knowledge Can You Pack Into the Parameter.pdf}
}

@article{robertsonProbabilisticRelevanceFramework2009,
  title = {The {{Probabilistic Relevance Framework}}: {{BM25}} and {{Beyond}}},
  shorttitle = {The {{Probabilistic Relevance Framework}}},
  author = {Robertson, Stephen and Zaragoza, Hugo},
  date = {2009},
  journaltitle = {Foundations and Trends\textregistered{} in Information Retrieval},
  shortjournal = {FNT in Information Retrieval},
  volume = {3},
  number = {4},
  pages = {333--389},
  issn = {1554-0669, 1554-0677},
  url = {http://www.nowpublishers.com/article/Details/INR-019},
  urldate = {2022-12-22},
  abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\textendash 1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
  langid = {english},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/PYPH4MY3/Robertson and Zaragoza - 2009 - The Probabilistic Relevance Framework BM25 and Be.pdf}
}

@article{robertsScalingModelsData2022,
  title = {Scaling {{Up Models}} and {{Data}} with \$\textbackslash texttt\{t5x\}\$ and \$\textbackslash texttt\{seqio\}\$},
  author = {Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and Mishra, Gaurav and Bradbury, James and Andor, Daniel and Narang, Sharan and Lester, Brian and Gaffney, Colin and Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor and Salcianu, Alex and family=Zee, given=Marc, prefix=van, useprefix=true and Austin, Jacob and Goodman, Sebastian and Soares, Livio Baldini and Hu, Haitang and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Clark, Jonathan H. and Lee, Stephan and Garrette, Dan and Lee-Thorp, James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard, Jeremy and Fiedel, Noah and Omernick, Mark and Saeta, Brennan and Sepassi, Ryan and Spiridonov, Alexander and Newlan, Joshua and Gesmundo, Andrea},
  date = {2022-03-31},
  url = {https://arxiv.org/abs/2203.17189v1},
  urldate = {2022-10-24},
  abstract = {Recent neural network-based language models have benefited greatly from scaling up the size of training datasets and the number of parameters in the models themselves. Scaling can be complicated due to various factors including the need to distribute computation on supercomputer clusters (e.g., TPUs), prevent bottlenecks when infeeding data, and ensure reproducible results. In this work, we present two software libraries that ease these issues: \$\textbackslash texttt\{t5x\}\$ simplifies the process of building and training large language models at scale while maintaining ease of use, and \$\textbackslash texttt\{seqio\}\$ provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines. These open-source libraries have been used to train models with hundreds of billions of parameters on datasets with multiple terabytes of training data. Along with the libraries, we release configurations and instructions for T5-like encoder-decoder models as well as GPT-like decoder-only architectures. \$\textbackslash texttt\{t5x\}\$ and \$\textbackslash texttt\{seqio\}\$ are open source and available at https://github.com/google-research/t5x and https://github.com/google/seqio, respectively.},
  langid = {english},
  keywords = {distributed training,read},
  file = {/Users/lukakuma/Zotero/storage/FNHVGGJV/Roberts et al. - 2022 - Scaling Up Models and Data with $texttt t5x $ and.pdf;/Users/lukakuma/Zotero/storage/63P59DIV/2203.html}
}

@article{rocktaschelReasoningEntailmentNeural2015,
  title = {Reasoning about {{Entailment}} with {{Neural Attention}}},
  author = {Rockt\"aschel, Tim and Grefenstette, Edward and Hermann, Karl Moritz and Ko\v{c}isk\'y, Tom\'a\v{s} and Blunsom, Phil},
  date = {2015-09-22},
  url = {https://arxiv.org/abs/1509.06664v4},
  urldate = {2022-08-01},
  abstract = {While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/4W7JW8T2/Rockt√§schel et al. - 2015 - Reasoning about Entailment with Neural Attention.pdf;/Users/lukakuma/Zotero/storage/CPAWC6CL/1509.html}
}

@online{rogersPrimerBERTologyWhat2020,
  title = {A {{Primer}} in {{BERTology}}: {{What}} We Know about How {{BERT}} Works},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  date = {2020-11-09},
  number = {arXiv:2002.12327},
  eprint = {arXiv:2002.12327},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.12327},
  urldate = {2022-11-01},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/LZ2CLIY9/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf}
}

@online{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},
  date = {2022-04-13},
  number = {arXiv:2112.10752},
  eprint = {arXiv:2112.10752},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.10752},
  urldate = {2022-08-08},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  pubstate = {preprint},
  keywords = {*diffusion,image generation},
  file = {/Users/lukakuma/Zotero/storage/5GZ3ISRI/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf;/Users/lukakuma/Zotero/storage/27IPG4SG/2112.html}
}

@article{rossiTemporalGraphNetworks2020,
  title = {Temporal {{Graph Networks}} for {{Deep Learning}} on {{Dynamic Graphs}}},
  author = {Rossi, Emanuele and Chamberlain, Ben and Frasca, Fabrizio and Eynard, Davide and Monti, Federico and Bronstein, Michael},
  date = {2020-06-18},
  url = {https://arxiv.org/abs/2006.10637v3},
  urldate = {2022-03-30},
  abstract = {Graph Neural Networks (GNNs) have recently become increasingly popular due to their ability to learn complex systems of relations or interactions arising in a broad spectrum of problems ranging from biology and particle physics to social networks and recommendation systems. Despite the plethora of different models for deep learning on graphs, few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time). In this paper, we present Temporal Graph Networks (TGNs), a generic, efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators, TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient. We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs.},
  langid = {english},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/777N2LI7/Rossi et al. - 2020 - Temporal Graph Networks for Deep Learning on Dynam.pdf;/Users/lukakuma/Zotero/storage/7DM6TTEH/2006.html}
}

@online{royExplainabilityCausalSelfTalk2022,
  title = {Explainability {{Via Causal Self-Talk}}},
  author = {Roy, Nicholas A. and Kim, Junkyung and Rabinowitz, Neil},
  date = {2022-11-17},
  number = {arXiv:2211.09937},
  eprint = {arXiv:2211.09937},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.09937},
  urldate = {2023-03-02},
  abstract = {Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided. While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations. We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning. We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself. We develop an instance of this solution for Deep RL agents: Causal Self-Talk. CST operates by training the agent to communicate with itself across time. We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior. Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.},
  pubstate = {preprint},
  keywords = {calibration},
  file = {/Users/lukakuma/Zotero/storage/RDWSPEUS/Roy et al. - 2022 - Explainability Via Causal Self-Talk.pdf;/Users/lukakuma/Zotero/storage/AH7LUT3P/2211.html}
}

@inproceedings{roziereLeveragingAutomatedUnit2021,
  title = {Leveraging {{Automated Unit Tests}} for {{Unsupervised Code Translation}}},
  author = {Roziere, Baptiste and Zhang, Jie and Charton, Francois and Harman, Mark and Synnaeve, Gabriel and Lample, Guillaume},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=cmt-6KtR4c4},
  urldate = {2022-05-15},
  abstract = {With little to no parallel data available for programming languages, unsupervised methods are well-suited to source code translation. However, the majority of unsupervised machine translation...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/LBA83VVM/Roziere et al. - 2021 - Leveraging Automated Unit Tests for Unsupervised C.pdf;/Users/lukakuma/Zotero/storage/YVV8SN6V/forum.html}
}

@online{ruderACL2022Highlights2022,
  title = {{{ACL}} 2022 {{Highlights}}},
  author = {Ruder, Sebastian},
  date = {2022-06-06T10:33:47},
  url = {https://ruder.io/acl2022/},
  urldate = {2022-06-10},
  abstract = {This post discusses my highlights of ACL 2022, including language diversity and multimodality, prompting, the next big ideas and keynotes, my favorite papers, and the hybrid conference experience.},
  langid = {english},
  organization = {{Sebastian Ruder}},
  file = {/Users/lukakuma/Zotero/storage/XGZEKX35/acl2022.html}
}

@online{ruizDreamBoothFineTuning2022,
  title = {{{DreamBooth}}: {{Fine Tuning Text-to-Image Diffusion Models}} for {{Subject-Driven Generation}}},
  shorttitle = {{{DreamBooth}}},
  author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  date = {2022-08-25},
  number = {arXiv:2208.12242},
  eprint = {arXiv:2208.12242},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.12242},
  urldate = {2022-08-27},
  abstract = {Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models (specializing them to users' needs). Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model (Imagen, although our method is not limited to a specific model) such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views, and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, appearance modification, and artistic rendering (all while preserving the subject's key features). Project page: https://dreambooth.github.io/},
  pubstate = {preprint},
  keywords = {*diffusion,finetuning,image generation},
  file = {/Users/lukakuma/Zotero/storage/62AYILAX/Ruiz et al. - 2022 - DreamBooth Fine Tuning Text-to-Image Diffusion Mo.pdf;/Users/lukakuma/Zotero/storage/NYIFM9QU/2208.html}
}

@online{ryabininCrowdsourcedTrainingLarge2020,
  title = {Towards {{Crowdsourced Training}} of {{Large Neural Networks}} Using {{Decentralized Mixture-of-Experts}}},
  author = {Ryabinin, Max and Gusev, Anton},
  date = {2020-10-21},
  number = {arXiv:2002.04013},
  eprint = {arXiv:2002.04013},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.04013},
  urldate = {2022-09-05},
  abstract = {Many recent breakthroughs in deep learning were achieved by training increasingly larger models on massive datasets. However, training such models can be prohibitively expensive. For instance, the cluster used to train GPT-3 costs over \textbackslash\$250 million. As a result, most researchers cannot afford to train state of the art models and contribute to their development. Hypothetically, a researcher could crowdsource the training of large neural networks with thousands of regular PCs provided by volunteers. The raw computing power of a hundred thousand \textbackslash\$2500 desktops dwarfs that of a \textbackslash\$250M server pod, but one cannot utilize that power efficiently with conventional distributed training methods. In this work, we propose Learning@home: a novel neural network training paradigm designed to handle large amounts of poorly connected participants. We analyze the performance, reliability, and architectural constraints of this paradigm and compare it against existing distributed training techniques.},
  pubstate = {preprint},
  keywords = {*open source AI},
  file = {/Users/lukakuma/Zotero/storage/F6VZYJEZ/Ryabinin and Gusev - 2020 - Towards Crowdsourced Training of Large Neural Netw.pdf;/Users/lukakuma/Zotero/storage/AX7RAY6D/2002.html}
}

@article{ryabininSWARMParallelismTraining2021,
  title = {{{SWARM Parallelism}}: {{Training Large Models Can Be Surprisingly Communication-Efficient}}},
  shorttitle = {{{SWARM Parallelism}}},
  author = {Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander},
  date = {2021-11-23},
  url = {https://openreview.net/forum?id=U1edbV4kNu_},
  urldate = {2022-09-02},
  abstract = {Many deep learning applications benefit from using large models with billions of parameters. These models can only be trained with specialized distributed training algorithms that require...},
  langid = {english},
  keywords = {*open source AI},
  file = {/Users/lukakuma/Zotero/storage/5E3X7Z89/Ryabinin et al. - 2021 - SWARM Parallelism Training Large Models Can Be Su.pdf;/Users/lukakuma/Zotero/storage/M5PIFC3Y/forum.html}
}

@online{ryabininSWARMParallelismTraining2023,
  title = {{{SWARM Parallelism}}: {{Training Large Models Can Be Surprisingly Communication-Efficient}}},
  shorttitle = {{{SWARM Parallelism}}},
  author = {Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander},
  date = {2023-01-27},
  number = {arXiv:2301.11913},
  eprint = {arXiv:2301.11913},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.11913},
  urldate = {2023-01-31},
  abstract = {Many deep learning applications benefit from using large models with billions of parameters. Training these models is notoriously expensive due to the need for specialized HPC clusters. In this work, we consider alternative setups for training large models: using cheap "preemptible" instances or pooling existing resources from multiple regions. We analyze the performance of existing model-parallel algorithms in these conditions and find configurations where training larger models becomes less communication-intensive. Based on these findings, we propose SWARM parallelism, a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices. SWARM creates temporary randomized pipelines between nodes that are rebalanced in case of failure. We empirically validate our findings and compare SWARM parallelism with existing large-scale training approaches. Finally, we combine our insights with compression strategies to train a large Transformer language model with 1B shared parameters (approximately 13B before sharing) on preemptible T4 GPUs with less than 200Mb/s network.},
  pubstate = {preprint},
  keywords = {*open source AI},
  file = {/Users/lukakuma/Zotero/storage/WTW9GNFP/Ryabinin et al. - 2023 - SWARM Parallelism Training Large Models Can Be Su.pdf;/Users/lukakuma/Zotero/storage/F4R6BVYH/2301.html}
}

@online{sachanEndtoEndTrainingMultiDocument2021,
  title = {End-to-{{End Training}} of {{Multi-Document Reader}} and {{Retriever}} for {{Open-Domain Question Answering}}},
  author = {Sachan, Devendra Singh and Reddy, Siva and Hamilton, William and Dyer, Chris and Yogatama, Dani},
  date = {2021-12-04},
  number = {arXiv:2106.05346},
  eprint = {arXiv:2106.05346},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.05346},
  urldate = {2023-02-06},
  abstract = {We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than staged-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3\% absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.},
  pubstate = {preprint},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/HYYF7ZM3/Sachan et al. - 2021 - End-to-End Training of Multi-Document Reader and R.pdf;/Users/lukakuma/Zotero/storage/KSC37V4X/2106.html}
}

@online{saeidiCrossPolicyComplianceDetection2021,
  title = {Cross-{{Policy Compliance Detection}} via {{Question Answering}}},
  author = {Saeidi, Marzieh and Yazdani, Majid and Vlachos, Andreas},
  date = {2021-09-08},
  number = {arXiv:2109.03731},
  eprint = {arXiv:2109.03731},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.03731},
  urldate = {2022-12-28},
  abstract = {Policy compliance detection is the task of ensuring that a scenario conforms to a policy (e.g. a claim is valid according to government rules or a post in an online platform conforms to community guidelines). This task has been previously instantiated as a form of textual entailment, which results in poor accuracy due to the complexity of the policies. In this paper we propose to address policy compliance detection via decomposing it into question answering, where questions check whether the conditions stated in the policy apply to the scenario, and an expression tree combines the answers to obtain the label. Despite the initial upfront annotation cost, we demonstrate that this approach results in better accuracy, especially in the cross-policy setup where the policies during testing are unseen in training. In addition, it allows us to use existing question answering models pre-trained on existing large datasets. Finally, it explicitly identifies the information missing from a scenario in case policy compliance cannot be determined. We conduct our experiments using a recent dataset consisting of government policies, which we augment with expert annotations and find that the cost of annotating question answering decomposition is largely offset by improved inter-annotator agreement and speed.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/MNM94FFA/Saeidi et al. - 2021 - Cross-Policy Compliance Detection via Question Ans.pdf;/Users/lukakuma/Zotero/storage/ZSF4CHBY/2109.html}
}

@unpublished{sagawaExtendingWILDSBenchmark2022,
  title = {Extending the {{WILDS Benchmark}} for {{Unsupervised Adaptation}}},
  author = {Sagawa, Shiori and Koh, Pang Wei and Lee, Tony and Gao, Irena and Xie, Sang Michael and Shen, Kendrick and Kumar, Ananya and Hu, Weihua and Yasunaga, Michihiro and Marklund, Henrik and Beery, Sara and David, Etienne and Stavness, Ian and Guo, Wei and Leskovec, Jure and Saenko, Kate and Hashimoto, Tatsunori and Levine, Sergey and Finn, Chelsea and Liang, Percy},
  date = {2022-04-23},
  eprint = {2112.05090},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2112.05090},
  urldate = {2022-04-27},
  abstract = {Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as well. However, existing distribution shift benchmarks with unlabeled data do not reflect the breadth of scenarios that arise in real-world applications. In this work, we present the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classification, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). The update maintains consistency with the original WILDS benchmark by using identical labeled training, validation, and test sets, as well as the evaluation metrics. On these datasets, we systematically benchmark state-of-the-art methods that leverage unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on WILDS is limited. To facilitate method development and evaluation, we provide an open-source package that automates data loading and contains all of the model architectures and methods used in this paper. Code and leaderboards are available at https://wilds.stanford.edu.},
  file = {/Users/lukakuma/Zotero/storage/GMPVGP4F/Sagawa et al. - 2022 - Extending the WILDS Benchmark for Unsupervised Ada.pdf;/Users/lukakuma/Zotero/storage/CWLN2UCV/2112.html}
}

@online{sahariaImageSuperResolutionIterative2021,
  title = {Image {{Super-Resolution}} via {{Iterative Refinement}}},
  author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  date = {2021-06-30},
  number = {arXiv:2104.07636},
  eprint = {arXiv:2104.07636},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.07636},
  urldate = {2022-08-08},
  abstract = {We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50\%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34\%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/W5F8PXD9/Saharia et al. - 2021 - Image Super-Resolution via Iterative Refinement.pdf;/Users/lukakuma/Zotero/storage/YITQXCHZ/2104.html}
}

@online{salgeEmpowermentIntroduction2013,
  title = {Empowerment -- an {{Introduction}}},
  author = {Salge, Christoph and Glackin, Cornelius and Polani, Daniel},
  date = {2013-10-08},
  number = {arXiv:1310.1863},
  eprint = {arXiv:1310.1863},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1310.1863},
  urldate = {2023-01-06},
  abstract = {This book chapter is an introduction to and an overview of the information-theoretic, task independent utility function "Empowerment", which is defined as the channel capacity between an agent's actions and an agent's sensors. It quantifies how much influence and control an agent has over the world it can perceive. This book chapter discusses the general idea behind empowerment as an intrinsic motivation and showcases several previous applications of empowerment to demonstrate how empowerment can be applied to different sensor-motor configuration, and how the same formalism can lead to different observed behaviors. Furthermore, we also present a fast approximation for empowerment in the continuous domain.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/YRHHIKLZ/Salge et al. - 2013 - Empowerment -- an Introduction.pdf;/Users/lukakuma/Zotero/storage/NGPHPPT4/1310.html}
}

@article{samborskaComplementaryTaskRepresentations2022,
  title = {Complementary Task Representations in Hippocampus and Prefrontal Cortex for Generalizing the Structure of Problems},
  author = {Samborska, Veronika and Butler, James L. and Walton, Mark E. and Behrens, Timothy E. J. and Akam, Thomas},
  date = {2022-09-28},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  pages = {1--13},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  url = {https://www.nature.com/articles/s41593-022-01149-8},
  urldate = {2022-09-29},
  abstract = {Humans and other animals effortlessly generalize prior knowledge to solve novel problems, by abstracting common structure and mapping it onto new sensorimotor specifics. To investigate how the brain achieves this, in this study, we trained mice on a series of reversal learning problems that shared the same structure but had different physical implementations. Performance improved across problems, indicating transfer of knowledge. Neurons in medial prefrontal cortex (mPFC) maintained similar representations across problems despite their different sensorimotor correlates, whereas hippocampal (dCA1) representations were more strongly influenced by the specifics of each problem. This was true for both representations of the events that comprised each trial and those that integrated choices and outcomes over multiple trials to guide an animal's decisions. These data suggest that prefrontal cortex and hippocampus play complementary roles in generalization of knowledge: PFC abstracts the common structure among related problems, and hippocampus maps this structure onto the specifics of the current situation.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/KGMD72WV/Samborska et al. - 2022 - Complementary task representations in hippocampus .pdf;/Users/lukakuma/Zotero/storage/R437D6SV/s41593-022-01149-8.html}
}

@article{sanchez-lengelingGentleIntroductionGraph2021,
  title = {A {{Gentle Introduction}} to {{Graph Neural Networks}}},
  author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
  date = {2021-09-02},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {6},
  number = {9},
  pages = {e33},
  issn = {2476-0757},
  url = {https://distill.pub/2021/gnn-intro},
  urldate = {2022-03-27},
  abstract = {What components are needed for building learning algorithms that leverage the structure and properties of graphs?},
  langid = {english},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/EKSQ7YBF/gnn-intro.html}
}

@online{sanhMultitaskPromptedTraining2022a,
  title = {Multitask {{Prompted Training Enables Zero-Shot Task Generalization}}},
  author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and Dey, Manan and Bari, M. Saiful and Xu, Canwen and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla, Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak, Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang, Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden, Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos and Sharma, Abheesht and Santilli, Andrea and Fevry, Thibault and Fries, Jason Alan and Teehan, Ryan and Bers, Tali and Biderman, Stella and Gao, Leo and Wolf, Thomas and Rush, Alexander M.},
  date = {2022-03-17},
  number = {arXiv:2110.08207},
  eprint = {arXiv:2110.08207},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.08207},
  urldate = {2022-12-09},
  abstract = {Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.},
  pubstate = {preprint},
  keywords = {finetuning,read},
  file = {/Users/lukakuma/Zotero/storage/BP5FLYST/Sanh et al. - 2022 - Multitask Prompted Training Enables Zero-Shot Task.pdf;/Users/lukakuma/Zotero/storage/YIJPVRYT/2110.html}
}

@online{sapATOMICAtlasMachine2019,
  title = {{{ATOMIC}}: {{An Atlas}} of {{Machine Commonsense}} for {{If-Then Reasoning}}},
  shorttitle = {{{ATOMIC}}},
  author = {Sap, Maarten and LeBras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A. and Choi, Yejin},
  date = {2019-02-07},
  number = {arXiv:1811.00146},
  eprint = {arXiv:1811.00146},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1811.00146},
  urldate = {2022-12-14},
  abstract = {We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., "if X pays Y a compliment, then Y will likely return the compliment"). We propose nine if-then relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By generatively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of if-then relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/ITQGNPFG/Sap et al. - 2019 - ATOMIC An Atlas of Machine Commonsense for If-The.pdf;/Users/lukakuma/Zotero/storage/P5ZTY99F/1811.html}
}

@online{saundersSelfcritiquingModelsAssisting2022a,
  title = {Self-Critiquing Models for Assisting Human Evaluators},
  author = {Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  date = {2022-06-13},
  number = {arXiv:2206.05802},
  eprint = {arXiv:2206.05802},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.05802},
  urldate = {2022-12-28},
  abstract = {We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow,alignment},
  file = {/Users/lukakuma/Zotero/storage/UYACPD6C/Saunders et al. - 2022 - Self-critiquing models for assisting human evaluat.pdf;/Users/lukakuma/Zotero/storage/645HEBPZ/2206.html}
}

@inproceedings{savvaHabitatPlatformEmbodied2019,
  title = {Habitat: {{A Platform}} for {{Embodied AI Research}}},
  shorttitle = {Habitat},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and Parikh, Devi and Batra, Dhruv},
  date = {2019-10},
  pages = {9338--9346},
  publisher = {{IEEE}},
  location = {{Seoul, Korea (South)}},
  url = {https://ieeexplore.ieee.org/document/9010745/},
  urldate = {2021-04-19},
  abstract = {We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast \textendash{} when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/WEC5G6EA/Savva et al. - 2019 - Habitat A Platform for Embodied AI Research.pdf}
}

@online{saxtonAnalysingMathematicalReasoning2019,
  title = {Analysing {{Mathematical Reasoning Abilities}} of {{Neural Models}}},
  author = {Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
  date = {2019-04-02},
  number = {arXiv:1904.01557},
  eprint = {arXiv:1904.01557},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.01557},
  urldate = {2022-12-15},
  abstract = {Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/288HCNRA/Saxton et al. - 2019 - Analysing Mathematical Reasoning Abilities of Neur.pdf;/Users/lukakuma/Zotero/storage/AVHPWZ6W/1904.html}
}

@online{scaoHowManyData2021,
  title = {How {{Many Data Points}} Is a {{Prompt Worth}}?},
  author = {Scao, Teven Le and Rush, Alexander M.},
  date = {2021-04-06},
  number = {arXiv:2103.08493},
  eprint = {arXiv:2103.08493},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.08493},
  urldate = {2022-11-02},
  abstract = {When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting taskspecific guidance, which is beneficial in lowdata regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/4RJ7DGL5/Scao and Rush - 2021 - How Many Data Points is a Prompt Worth.pdf}
}

@unpublished{schaarschmidtAutomapErgonomicAutomated2021,
  title = {Automap: {{Towards Ergonomic Automated Parallelism}} for {{ML Models}}},
  shorttitle = {Automap},
  author = {Schaarschmidt, Michael and Grewe, Dominik and Vytiniotis, Dimitrios and Paszke, Adam and Schmid, Georg Stefan and Norman, Tamara and Molloy, James and Godwin, Jonathan and Rink, Norman Alexander and Nair, Vinod and Belov, Dan},
  date = {2021-12-06},
  number = {arXiv:2112.02958},
  eprint = {2112.02958},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.02958},
  urldate = {2022-06-09},
  abstract = {The rapid rise in demand for training large neural network architectures has brought into focus the need for partitioning strategies, for example by using data, model, or pipeline parallelism. Implementing these methods is increasingly supported through program primitives, but identifying efficient partitioning strategies requires expensive experimentation and expertise. We present the prototype of an automated partitioner that seamlessly integrates into existing compilers and existing user workflows. Our partitioner enables SPMD-style parallelism that encompasses data parallelism and parameter/activation sharding. Through a combination of inductive tactics and search in a platform-independent partitioning IR, automap can recover expert partitioning strategies such as Megatron sharding for transformer layers.},
  file = {/Users/lukakuma/Zotero/storage/THGSNH5A/Schaarschmidt et al. - 2021 - Automap Towards Ergonomic Automated Parallelism f.pdf;/Users/lukakuma/Zotero/storage/D67BFDNB/2112.html}
}

@online{schickExploitingClozeQuestions2021,
  title = {Exploiting {{Cloze Questions}} for {{Few Shot Text Classification}} and {{Natural Language Inference}}},
  author = {Schick, Timo and Sch\"utze, Hinrich},
  date = {2021-01-25},
  number = {arXiv:2001.07676},
  eprint = {arXiv:2001.07676},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2001.07676},
  urldate = {2022-11-02},
  abstract = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/6T4ICW67/Schick and Sch√ºtze - 2021 - Exploiting Cloze Questions for Few Shot Text Class.pdf}
}

@online{schickPEERCollaborativeLanguage2022,
  title = {{{PEER}}: {{A Collaborative Language Model}}},
  shorttitle = {{{PEER}}},
  author = {Schick, Timo and Dwivedi-Yu, Jane and Jiang, Zhengbao and Petroni, Fabio and Lewis, Patrick and Izacard, Gautier and You, Qingfei and Nalmpantis, Christoforos and Grave, Edouard and Riedel, Sebastian},
  date = {2022-08-24},
  number = {arXiv:2208.11663},
  eprint = {arXiv:2208.11663},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.11663},
  urldate = {2023-02-20},
  abstract = {Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today's language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/XH28TREN/Schick et al. - 2022 - PEER A Collaborative Language Model.pdf;/Users/lukakuma/Zotero/storage/CYGD4X7G/2208.html}
}

@online{schickSelfDiagnosisSelfDebiasingProposal2021,
  title = {Self-{{Diagnosis}} and {{Self-Debiasing}}: {{A Proposal}} for {{Reducing Corpus-Based Bias}} in {{NLP}}},
  shorttitle = {Self-{{Diagnosis}} and {{Self-Debiasing}}},
  author = {Schick, Timo and Udupa, Sahana and Sch\"utze, Hinrich},
  date = {2021-09-09},
  number = {arXiv:2103.00453},
  eprint = {arXiv:2103.00453},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.00453},
  urldate = {2022-11-02},
  abstract = {When trained on large, unfiltered crawls from the internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: they often generate racist, sexist, violent or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model's parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/3M4AP86P/Schick et al. - 2021 - Self-Diagnosis and Self-Debiasing A Proposal for .pdf}
}

@online{schickToolformerLanguageModels2023,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and Dwivedi-Yu, Jane and Dess\`i, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  date = {2023-02-09},
  url = {https://arxiv.org/abs/2302.04761v1},
  urldate = {2023-02-11},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\textbackslash\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  langid = {english},
  pubstate = {preprint},
  keywords = {*general tools},
  file = {/Users/lukakuma/Zotero/storage/R7VC8XZP/Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves t.pdf}
}

@report{schmidtfuturesBioeconomyChartingCourse2022,
  title = {The {{U}}.{{S}}. {{Bioeconomy}}: {{Charting}} a {{Course}} for a {{Resilient}} and {{Competitive Future}}},
  shorttitle = {The {{U}}.{{S}}. {{Bioeconomy}}},
  author = {{Schmidt Futures} and Hodgson, Andrea and Maxon, Mary and {Schmidt Futures} and Alper, Joe},
  date = {2022-04-14},
  institution = {{Schmidt Futures}},
  url = {https://www.schmidtfutures.com/our-work/task-force-on-synthetic-biology-and-the-bioeconomy/},
  urldate = {2022-05-23},
  langid = {english},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/ZYFP4NLR/Schmidt Futures et al. - 2022 - The U.S. Bioeconomy Charting a Course for a Resil.pdf}
}

@unpublished{scholkopfCausalRepresentationLearning2021,
  title = {Towards {{Causal Representation Learning}}},
  author = {Sch\"olkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  date = {2021-02-22},
  eprint = {2102.11107},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.11107},
  urldate = {2022-05-12},
  abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from lowlevel observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
  langid = {english},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/KGATZ4SQ/Sch√∂lkopf et al. - 2021 - Towards Causal Representation Learning.pdf}
}

@report{schrepelBlockchainCodeAntitrust2020,
  type = {SSRN Scholarly Paper},
  title = {Blockchain {{Code}} as {{Antitrust}}},
  author = {Schrepel, Thibault and Buterin, Vitalik},
  date = {2020-05-18},
  number = {ID 3597399},
  institution = {{Social Science Research Network}},
  location = {{Rochester, NY}},
  url = {https://papers.ssrn.com/abstract=3597399},
  urldate = {2022-03-16},
  abstract = {In this article, the authors show that blockchain can help in reaching the goals of antitrust law in situations where the rule of law does not (fully) apply. They detail what needs to be done to this end, from both a technical and legal standpoint.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/6GBMWCIM/Schrepel and Buterin - 2020 - Blockchain Code as Antitrust.pdf;/Users/lukakuma/Zotero/storage/SDTI9FE2/papers.html}
}

@article{schrittwieserMasteringAtariGo2019,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  date = {2019-11-19},
  url = {https://arxiv.org/abs/1911.08265v2},
  urldate = {2022-12-30},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  langid = {english},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/MBWLZWL4/Schrittwieser et al. - 2019 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf}
}

@inproceedings{schuhmannLAION5BOpenLargescale2022,
  title = {{{LAION-5B}}: {{An}} Open Large-Scale Dataset for Training next Generation Image-Text Models},
  shorttitle = {{{LAION-5B}}},
  author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade W. and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa R. and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
  date = {2022-10-12},
  url = {https://openreview.net/forum?id=M3Y74vmsMcY},
  urldate = {2022-10-13},
  abstract = {Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, further large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection.},
  eventtitle = {Thirty-Sixth {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  langid = {english},
  keywords = {pretraining,read},
  file = {/Users/lukakuma/Zotero/storage/CLKPJ3MM/Schuhmann et al. - 2022 - LAION-5B An open large-scale dataset for training.pdf;/Users/lukakuma/Zotero/storage/GAKNGGDG/forum.html}
}

@online{schulmanEquivalencePolicyGradients2018,
  title = {Equivalence {{Between Policy Gradients}} and {{Soft Q-Learning}}},
  author = {Schulman, John and Chen, Xi and Abbeel, Pieter},
  date = {2018-10-14},
  number = {arXiv:1704.06440},
  eprint = {arXiv:1704.06440},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1704.06440},
  urldate = {2022-08-09},
  abstract = {Two of the leading approaches for model-free reinforcement learning are policy gradient methods and \$Q\$-learning methods. \$Q\$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the \$Q\$-values they estimate are very inaccurate. A partial explanation may be that \$Q\$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between \$Q\$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that "soft" (entropy-regularized) \$Q\$-learning is exactly equivalent to a policy gradient method. We also point out a connection between \$Q\$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of \$Q\$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a \$Q\$-learning method that closely matches the learning dynamics of A3C without using a target network or \$\textbackslash epsilon\$-greedy exploration schedule.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/2K8JJYH8/Schulman et al. - 2018 - Equivalence Between Policy Gradients and Soft Q-Le.pdf;/Users/lukakuma/Zotero/storage/6CJPQLPD/1704.html}
}

@online{schulmanHighDimensionalContinuousControl2018,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  date = {2018-10-20},
  number = {arXiv:1506.02438},
  eprint = {arXiv:1506.02438},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1506.02438},
  urldate = {2022-07-19},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/TKXYND2J/Schulman et al. - 2018 - High-Dimensional Continuous Control Using Generali.pdf;/Users/lukakuma/Zotero/storage/GU7PGSFJ/1506.html}
}

@online{schulmanOpinionatedGuideML2020,
  title = {An {{Opinionated Guide}} to {{ML Research}}},
  author = {Schulman, John},
  date = {2020},
  url = {http://joschu.net/blog/opinionated-guide-ml-research.html},
  urldate = {2021-04-19},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/77KBCG6N/opinionated-guide-ml-research.html}
}

@online{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017-08-28},
  number = {arXiv:1707.06347},
  eprint = {arXiv:1707.06347},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2022-07-19},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/9ASXZCGI/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/Users/lukakuma/Zotero/storage/D7G8ATKP/1707.html}
}

@online{schulmanTrustRegionPolicy2017,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  date = {2017-04-20},
  number = {arXiv:1502.05477},
  eprint = {arXiv:1502.05477},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1502.05477},
  urldate = {2022-07-20},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/LE57ZL4V/Schulman et al. - 2017 - Trust Region Policy Optimization.pdf;/Users/lukakuma/Zotero/storage/ICHAA3VF/1502.html}
}

@online{scialomFinetunedLanguageModels2022a,
  title = {Fine-Tuned {{Language Models}} Are {{Continual Learners}}},
  author = {Scialom, Thomas and Chakrabarty, Tuhin and Muresan, Smaranda},
  date = {2022-10-29},
  number = {arXiv:2205.12393},
  eprint = {arXiv:2205.12393},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.12393},
  urldate = {2022-11-24},
  abstract = {Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions. Language models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets. To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning we show that Language Models can be continual learners. We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn diverse new tasks, while still maintaining good performance on previous tasks, spanning remarkably through 70 datasets in total. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some compositionality.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/LCB3SH3R/Scialom et al. - 2022 - Fine-tuned Language Models are Continual Learners.pdf;/Users/lukakuma/Zotero/storage/KT68WP4P/2205.html}
}

@online{sclarRefereeReferenceFreeSentence2022,
  title = {Referee: {{Reference-Free Sentence Summarization}} with {{Sharper Controllability}} through {{Symbolic Knowledge Distillation}}},
  shorttitle = {Referee},
  author = {Sclar, Melanie and West, Peter and Kumar, Sachin and Tsvetkov, Yulia and Choi, Yejin},
  date = {2022-10-25},
  number = {arXiv:2210.13800},
  eprint = {arXiv:2210.13800},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.13800},
  urldate = {2022-12-09},
  abstract = {We present Referee, a novel framework for sentence summarization that can be trained reference-free (i.e., requiring no gold summaries for supervision), while allowing direct control for compression ratio. Our work is the first to demonstrate that reference-free, controlled sentence summarization is feasible via the conceptual framework of Symbolic Knowledge Distillation (West et al., 2022), where latent knowledge in pre-trained language models is distilled via explicit examples sampled from the teacher models, further purified with three types of filters: length, fidelity, and Information Bottleneck. Moreover, we uniquely propose iterative distillation of knowledge, where student models from the previous iteration of distillation serve as teacher models in the next iteration. Starting off from a relatively modest set of GPT3-generated summaries, we demonstrate how iterative knowledge distillation can lead to considerably smaller, but better summarizers with sharper controllability. A useful by-product of this iterative distillation process is a high-quality dataset of sentence-summary pairs with varying degrees of compression ratios. Empirical results demonstrate that the final student models vastly outperform the much larger GPT3-Instruct model in terms of the controllability of compression ratios, without compromising the quality of resulting summarization.},
  pubstate = {preprint},
  keywords = {read,self-play,summarization},
  file = {/Users/lukakuma/Zotero/storage/32K3LTUJ/Sclar et al. - 2022 - Referee Reference-Free Sentence Summarization wit.pdf;/Users/lukakuma/Zotero/storage/TZWQSNWP/2210.html}
}

@inproceedings{segalPracticalSecureAggregation2017,
  title = {Practical {{Secure Aggregation}} for {{Privacy-Preserving Machine Learning}}},
  booktitle = {{{CCS}}},
  author = {Segal, Aaron and Marcedone, Antonio and Kreuter, Benjamin and Ramage, Daniel and McMahan, H. Brendan and Seth, Karn and Bonawitz, K. A. and Patel, Sarvar and Ivanov, Vladimir},
  date = {2017},
  url = {https://eprint.iacr.org/2017/281.pdf},
  urldate = {2022-03-11},
  file = {/Users/lukakuma/Zotero/storage/7TH3I53U/Segal et al. - 2017 - Practical Secure Aggregation for Privacy-Preservin.pdf}
}

@online{shahLMNavRoboticNavigation2022,
  title = {{{LM-Nav}}: {{Robotic Navigation}} with {{Large Pre-Trained Models}} of {{Language}}, {{Vision}}, and {{Action}}},
  shorttitle = {{{LM-Nav}}},
  author = {Shah, Dhruv and Osinski, Blazej and Ichter, Brian and Levine, Sergey},
  date = {2022-07-10},
  number = {arXiv:2207.04429},
  eprint = {arXiv:2207.04429},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.04429},
  urldate = {2022-07-21},
  abstract = {Goal-conditioned policies for robotic navigation can be trained on large, unannotated datasets, providing for good generalization to real-world settings. However, particularly in vision-based settings where specifying goals requires an image, this makes for an unnatural interface. Language provides a more convenient modality for communication with robots, but contemporary methods typically require expensive supervision, in the form of trajectories annotated with language descriptions. We present a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user. Instead of utilizing a labeled instruction following dataset, we show that such a system can be constructed entirely out of pre-trained models for navigation (ViNG), image-language association (CLIP), and language modeling (GPT-3), without requiring any fine-tuning or language-annotated robot data. We instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon navigation through complex, outdoor environments from natural language instructions. For videos of our experiments, code release, and an interactive Colab notebook that runs in your browser, please check out our project page https://sites.google.com/view/lmnav},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/SYNJR4CZ/Shah et al. - 2022 - LM-Nav Robotic Navigation with Large Pre-Trained .pdf;/Users/lukakuma/Zotero/storage/YB99FRUT/2207.html}
}

@article{shanahanArtificialIntelligenceCommon2020,
  title = {Artificial {{Intelligence}} and the {{Common Sense}} of {{Animals}}},
  author = {Shanahan, Murray and Crosby, Matthew and Beyret, Benjamin and Cheke, Lucy},
  date = {2020-11},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {11},
  pages = {862--872},
  issn = {13646613},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661320302163},
  urldate = {2022-11-17},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/DCJ3HRHU/Shanahan et al. - 2020 - Artificial Intelligence and the Common Sense of An.pdf}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, C E},
  date = {1948},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/26NURBYS/Shannon - A Mathematical Theory of Communication.pdf}
}

@article{shapson-coeConnectomicStudyPetascale2021,
  title = {A Connectomic Study of a Petascale Fragment of Human Cerebral Cortex},
  author = {Shapson-Coe, Alexander and Januszewski, Micha\l{} and Berger, Daniel R. and Pope, Art and Wu, Yuelong and Blakely, Tim and Schalek, Richard L. and Li, Peter and Wang, Shuohong and Maitin-Shepard, Jeremy and Karlupia, Neha and Dorkenwald, Sven and Sjostedt, Evelina and Leavitt, Laramie and Lee, Dongil and Bailey, Luke and Fitzmaurice, Angerica and Kar, Rohin and Field, Benjamin and Wu, Hank and Wagner-Carena, Julian and Aley, David and Lau, Joanna and Lin, Zudi and Wei, Donglai and Pfister, Hanspeter and Peleg, Adi and Jain, Viren and Lichtman, Jeff W.},
  date = {2021-05-30},
  pages = {2021.05.29.446289},
  publisher = {{bioRxiv}},
  url = {https://www.biorxiv.org/content/10.1101/2021.05.29.446289v1},
  urldate = {2022-03-11},
  abstract = {We acquired a rapidly preserved human surgical sample from the temporal lobe of the cerebral cortex. We stained a 1 mm3 volume with heavy metals, embedded it in resin, cut more than 5000 slices at {$\sim$}30 nm and imaged these sections using a high-speed multibeam scanning electron microscope. We used computational methods to render the three-dimensional structure of 50,000 cells, hundreds of millions of neurites and 130 million synaptic connections. The 1.4 petabyte electron microscopy volume, the segmented cells, cell parts, blood vessels, myelin, inhibitory and excitatory synapses, and 100 manually proofread cells are available to peruse online. Despite the incompleteness of the automated segmentation caused by split and merge errors, many interesting features were evident. Glia outnumbered neurons 2:1 and oligodendrocytes were the most common cell type in the volume. The E:I balance of neurons was 69:31\%, as was the ratio of excitatory versus inhibitory synapses in the volume. The E:I ratio of synapses was significantly higher on pyramidal neurons than inhibitory interneurons. We found that deep layer excitatory cell types can be classified into subsets based on structural and connectivity differences, that chandelier interneurons not only innervate excitatory neuron initial segments as previously described, but also each other's initial segments, and that among the thousands of weak connections established on each neuron, there exist rarer highly powerful axonal inputs that establish multi-synaptic contacts (up to {$\sim$}20 synapses) with target neurons. Our analysis indicates that these strong inputs are specific, and allow small numbers of axons to have an outsized role in the activity of some of their postsynaptic partners.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/9UX47WP4/Shapson-Coe et al. - 2021 - A connectomic study of a petascale fragment of hum.pdf;/Users/lukakuma/Zotero/storage/87E83GM3/2021.05.29.html}
}

@online{shazeerOutrageouslyLargeNeural2017,
  title = {Outrageously {{Large Neural Networks}}: {{The Sparsely-Gated Mixture-of-Experts Layer}}},
  shorttitle = {Outrageously {{Large Neural Networks}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  date = {2017-01-23},
  number = {arXiv:1701.06538},
  eprint = {arXiv:1701.06538},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1701.06538},
  urldate = {2022-08-09},
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  pubstate = {preprint},
  keywords = {sparsity},
  file = {/Users/lukakuma/Zotero/storage/UVXC6KIQ/Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-G.pdf;/Users/lukakuma/Zotero/storage/4GJ3M2GG/1701.html}
}

@online{shenGenerateRankMultitask2021,
  title = {Generate \& {{Rank}}: {{A Multi-task Framework}} for {{Math Word Problems}}},
  shorttitle = {Generate \& {{Rank}}},
  author = {Shen, Jianhao and Yin, Yichun and Li, Lin and Shang, Lifeng and Jiang, Xin and Zhang, Ming and Liu, Qun},
  date = {2021-09-07},
  number = {arXiv:2109.03034},
  eprint = {arXiv:2109.03034},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.03034},
  urldate = {2022-12-22},
  abstract = {Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate \& Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7\% (78.4\% \$\textbackslash rightarrow\$ 85.4\%) higher than the state-of-the-art.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/LHMTRLFU/Shen et al. - 2021 - Generate & Rank A Multi-task Framework for Math W.pdf;/Users/lukakuma/Zotero/storage/8JUPDGB9/2109.html}
}

@article{shermanDistinctFunctionsDirect2011,
  title = {Distinct Functions for Direct and Transthalamic Corticocortical Connections},
  author = {Sherman, S. Murray and Guillery, R. W.},
  date = {2011-09},
  journaltitle = {Journal of Neurophysiology},
  volume = {106},
  number = {3},
  pages = {1068--1077},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  url = {https://journals.physiology.org/doi/full/10.1152/jn.00429.2011},
  urldate = {2022-06-18},
  abstract = {Essentially all cortical areas receive thalamic inputs and send outputs to lower motor centers. Cortical areas communicate with each other by means of direct corticocortical and corticothalamocortical pathways, often organized in parallel. We distinguish these functionally, stressing that the transthalamic pathways are class 1 (formerly known as ``driver'') pathways capable of transmitting information, whereas the direct pathways vary, being either class 2 (formerly known as ``modulator'') or class 1. The transthalamic pathways provide a thalamic gate that can be open or closed (and otherwise more subtly modulated), and these inputs to the thalamus are generally branches of axons with motor functions. Thus the transthalamic corticocortical pathways that can be gated carry information about the cortical processing in one cortical area and also about the motor instructions currently being issued from that area and copied to other cortical areas.},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/RLGLIZ44/Sherman and Guillery - 2011 - Distinct functions for direct and transthalamic co.pdf}
}

@online{shiLargeLanguageModels2023,
  title = {Large {{Language Models Can Be Easily Distracted}} by {{Irrelevant Context}}},
  author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed and Sch\"arli, Nathanael and Zhou, Denny},
  date = {2023-01-31},
  number = {arXiv:2302.00093},
  eprint = {arXiv:2302.00093},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.00093},
  urldate = {2023-02-02},
  abstract = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/5QRKLDFI/Shi et al. - 2023 - Large Language Models Can Be Easily Distracted by .pdf;/Users/lukakuma/Zotero/storage/LA7ZFHW2/2302.html}
}

@online{shinContinualLearningDeep2017,
  title = {Continual {{Learning}} with {{Deep Generative Replay}}},
  author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
  date = {2017-12-11},
  number = {arXiv:1705.08690},
  eprint = {arXiv:1705.08690},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.08690},
  urldate = {2022-12-12},
  abstract = {Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.},
  pubstate = {preprint},
  keywords = {continual learning},
  file = {/Users/lukakuma/Zotero/storage/QIIX5CP2/Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf;/Users/lukakuma/Zotero/storage/9G59ZE8X/1705.html}
}

@unpublished{shiPointGNNGraphNeural2020,
  title = {Point-{{GNN}}: {{Graph Neural Network}} for {{3D Object Detection}} in a {{Point Cloud}}},
  shorttitle = {Point-{{GNN}}},
  author = {Shi, Weijing and Ragunathan and Rajkumar},
  date = {2020-03-02},
  eprint = {2003.01251},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.01251},
  urldate = {2022-03-24},
  abstract = {In this paper, we propose a graph neural network to detect objects from a LiDAR point cloud. Towards this end, we encode the point cloud efficiently in a fixed radius near-neighbors graph. We design a graph neural network, named Point-GNN, to predict the category and shape of the object that each vertex in the graph belongs to. In Point-GNN, we propose an auto-registration mechanism to reduce translation variance, and also design a box merging and scoring operation to combine detections from multiple vertices accurately. Our experiments on the KITTI benchmark show the proposed approach achieves leading accuracy using the point cloud alone and can even surpass fusion-based algorithms. Our results demonstrate the potential of using the graph neural network as a new approach for 3D object detection. The code is available https://github.com/WeijingShi/Point-GNN.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/PKSIZJ46/Shi et al. - 2020 - Point-GNN Graph Neural Network for 3D Object Dete.pdf;/Users/lukakuma/Zotero/storage/2TLHFHU7/2003.html}
}

@online{shiREPLUGRetrievalAugmentedBlackBox2023,
  title = {{{REPLUG}}: {{Retrieval-Augmented Black-Box Language Models}}},
  shorttitle = {{{REPLUG}}},
  author = {Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  date = {2023-01-31},
  number = {arXiv:2301.12652},
  eprint = {arXiv:2301.12652},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.12652},
  urldate = {2023-02-02},
  abstract = {We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3\%, as well as the performance of Codex on five-shot MMLU by 5.1\%.},
  pubstate = {preprint},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/P6MQQSSK/Shi et al. - 2023 - REPLUG Retrieval-Augmented Black-Box Language Mod.pdf;/Users/lukakuma/Zotero/storage/U9VF2D5P/2301.html}
}

@online{shoeybiMegatronLMTrainingMultiBillion2020,
  title = {Megatron-{{LM}}: {{Training Multi-Billion Parameter Language Models Using Model Parallelism}}},
  shorttitle = {Megatron-{{LM}}},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  date = {2020-03-13},
  number = {arXiv:1909.08053},
  eprint = {arXiv:1909.08053},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.08053},
  urldate = {2022-07-29},
  abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
  pubstate = {preprint},
  keywords = {distributed training,read},
  file = {/Users/lukakuma/Zotero/storage/WYY2FXGU/Shoeybi et al. - 2020 - Megatron-LM Training Multi-Billion Parameter Lang.pdf;/Users/lukakuma/Zotero/storage/N4Q2NETD/1909.html}
}

@article{shohamIfMultiagentLearning2007,
  title = {If Multi-Agent Learning Is the Answer, What Is the Question?},
  author = {Shoham, Yoav and Powers, Rob and Grenager, Trond},
  date = {2007-05-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  series = {Foundations of {{Multi-Agent Learning}}},
  volume = {171},
  number = {7},
  pages = {365--377},
  issn = {0004-3702},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370207000495},
  urldate = {2022-06-09},
  abstract = {The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area.11This article has a long history and owes many debts. A first version was presented at the NIPS workshop, Multi-Agent Learning: Theory and Practice, in 2002. A later version was presented at the AAAI Fall Symposium in 2004 [Y. Shoham, R. Powers, T. Grenager, On the agenda(s) of research on multi-agent learning, in: AAAI 2004 Symposium on Artificial Multi-Agent Learning (FS-04-02), AAAI Press, 2004]. Over time it has gradually evolved into the current form, as a result of our own work in the area as well as the feedback of many colleagues. We thank them all collectively, with special thanks to members of the multi-agent group at Stanford in the past three years. Rakesh Vohra and Michael Wellman provided detailed comments on the latest draft which resulted in substantive improvements, although we alone are responsible for the views put forward. This work was supported by NSF ITR grant IIS-0205633 and DARPA grant HR0011-05-1.},
  langid = {english},
  keywords = {multiagent,QA},
  file = {/Users/lukakuma/Zotero/storage/9P5WMB6T/Shoham et al. - 2007 - If multi-agent learning is the answer, what is the.pdf;/Users/lukakuma/Zotero/storage/FJRCTWBZ/S0004370207000495.html}
}

@book{shohamMultiagentSystemsAlgorithmic2008,
  title = {Multiagent {{Systems}}: {{Algorithmic}}, {{Game-Theoretic}}, and {{Logical Foundations}}},
  shorttitle = {Multiagent {{Systems}}},
  author = {Shoham, Yoav and Leyton-Brown, Kevin},
  date = {2008-12-15},
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge ; New York}},
  isbn = {978-0-521-89943-7},
  langid = {english},
  pagetotal = {504},
  file = {/Users/lukakuma/Zotero/storage/YRB63BNY/Shoham and Leyton-Brown - 2008 - Multiagent Systems Algorithmic, Game-Theoretic, a.pdf}
}

@online{shridharDistillingMultiStepReasoning2022,
  title = {Distilling {{Multi-Step Reasoning Capabilities}} of {{Large Language Models}} into {{Smaller Models}} via {{Semantic Decompositions}}},
  author = {Shridhar, Kumar and Stolfo, Alessandro and Sachan, Mrinmaya},
  date = {2022-11-30},
  number = {arXiv:2212.00193},
  eprint = {arXiv:2212.00193},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.00193},
  urldate = {2023-02-20},
  abstract = {Step-by-step reasoning approaches like chain-of-thought (CoT) have proved to be a very effective technique to induce reasoning capabilities in large language models. However, the success of the CoT approach depends primarily on model size, and often billion parameter-scale models are needed to get CoT to work. In this paper, we propose a knowledge distillation approach, that leverages the step-by-step CoT reasoning capabilities of larger models and distils these reasoning abilities into smaller models. Our approach Decompositional Distillation learns a semantic decomposition of the original problem into a sequence of subproblems and uses it to train two models: a) a problem decomposer that learns to decompose the complex reasoning problem into a sequence of simpler sub-problems and b) a problem solver that uses the intermediate subproblems to solve the overall problem. On a multi-step math word problem dataset (GSM8K), we boost the performance of GPT-2 variants up to 35\% when distilled with our approach compared to CoT. We show that using our approach, it is possible to train a GPT-2-large model (775M) that can outperform a 10X larger GPT-3 (6B) model trained using CoT reasoning. Finally, we also demonstrate that our approach of problem decomposition can also be used as an alternative to CoT prompting, which boosts the GPT-3 performance by 40\% compared to CoT prompts.},
  pubstate = {preprint},
  keywords = {self-play},
  file = {/Users/lukakuma/Zotero/storage/GUD49F5N/Shridhar et al. - 2022 - Distilling Multi-Step Reasoning Capabilities of La.pdf;/Users/lukakuma/Zotero/storage/FMV4FM8G/2212.html}
}

@online{shridharPerceiverActorMultiTaskTransformer2022,
  title = {Perceiver-{{Actor}}: {{A Multi-Task Transformer}} for {{Robotic Manipulation}}},
  shorttitle = {Perceiver-{{Actor}}},
  author = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  date = {2022-09-12},
  number = {arXiv:2209.05451},
  eprint = {arXiv:2209.05451},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.05451},
  urldate = {2022-09-13},
  abstract = {Transformers have revolutionized vision and natural language processing with their ability to scale with large datasets. But in robotic manipulation, data is both limited and expensive. Can we still benefit from Transformers with the right problem formulation? We investigate this question with PerAct, a language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation. PerAct encodes language goals and RGB-D voxel observations with a Perceiver Transformer, and outputs discretized actions by "detecting the next best voxel action". Unlike frameworks that operate on 2D images, the voxelized observation and action space provides a strong structural prior for efficiently learning 6-DoF policies. With this formulation, we train a single multi-task Transformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks (with 18 variations) from just a few demonstrations per task. Our results show that PerAct significantly outperforms unstructured image-to-action agents and 3D ConvNet baselines for a wide range of tabletop tasks.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/EPVP8ZWR/Shridhar et al. - 2022 - Perceiver-Actor A Multi-Task Transformer for Robo.pdf;/Users/lukakuma/Zotero/storage/ZN9GNS55/2209.html}
}

@online{shusterBlenderBotDeployedConversational2022,
  title = {{{BlenderBot}} 3: A Deployed Conversational Agent That Continually Learns to Responsibly Engage},
  shorttitle = {{{BlenderBot}} 3},
  author = {Shuster, Kurt and Xu, Jing and Komeili, Mojtaba and Ju, Da and Smith, Eric Michael and Roller, Stephen and Ung, Megan and Chen, Moya and Arora, Kushal and Lane, Joshua and Behrooz, Morteza and Ngan, William and Poff, Spencer and Goyal, Naman and Szlam, Arthur and Boureau, Y.-Lan and Kambadur, Melanie and Weston, Jason},
  date = {2022-08-10},
  number = {arXiv:2208.03188},
  eprint = {arXiv:2208.03188},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.03188},
  urldate = {2022-11-28},
  abstract = {We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.},
  pubstate = {preprint},
  keywords = {*general tools,8-chat,read},
  file = {/Users/lukakuma/Zotero/storage/NGK8WQM7/Shuster et al. - 2022 - BlenderBot 3 a deployed conversational agent that.pdf;/Users/lukakuma/Zotero/storage/LJI5Z3UP/2208.html}
}

@online{shusterLanguageModelsThat2022,
  title = {Language {{Models}} That {{Seek}} for {{Knowledge}}: {{Modular Search}} \& {{Generation}} for {{Dialogue}} and {{Prompt Completion}}},
  shorttitle = {Language {{Models}} That {{Seek}} for {{Knowledge}}},
  author = {Shuster, Kurt and Komeili, Mojtaba and Adolphs, Leonard and Roller, Stephen and Szlam, Arthur and Weston, Jason},
  date = {2022-03-29},
  number = {arXiv:2203.13224},
  eprint = {arXiv:2203.13224},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.13224},
  urldate = {2023-02-20},
  abstract = {Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2021) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine-{$>$}Knowledge-{$>$}Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available.},
  pubstate = {preprint},
  keywords = {(ext) CICERO,(ext) Sparrow,*general tools},
  file = {/Users/lukakuma/Zotero/storage/AAUBV6Q7/Shuster et al. - 2022 - Language Models that Seek for Knowledge Modular S.pdf;/Users/lukakuma/Zotero/storage/99QVBVSB/2203.html}
}

@unpublished{siddarthWhoWatchesWatchmen2020,
  title = {Who {{Watches}} the {{Watchmen}}? {{A Review}} of {{Subjective Approaches}} for {{Sybil-resistance}} in {{Proof}} of {{Personhood Protocols}}},
  shorttitle = {Who {{Watches}} the {{Watchmen}}?},
  author = {Siddarth, Divya and Ivliev, Sergey and Siri, Santiago and Berman, Paula},
  date = {2020-10-13},
  eprint = {2008.05300},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2008.05300},
  urldate = {2022-03-15},
  abstract = {Most current self-sovereign identity systems may be categorized as strictly objective, consisting of cryptographically signed statements issued by trusted third party attestors. This failure to provide an input for subjectivity accounts for a central challenge: the inability to address the question of "Who verifies the verifier?". Instead, these protocols outsource their legitimacy to mechanisms beyond their internal structure, relying on traditional centralized institutions such as national ID issuers and KYC providers to verify the claims they hold. This reliance has been employed to safeguard applications from a vulnerability previously thought to be impossible to address in distributed systems: the Sybil attack problem, which describes the abuse of an online system by creating many illegitimate virtual personas. Inspired by the progress in cryptocurrencies and blockchain technology, there has recently been a surge in networked protocols that make use of subjective inputs such as voting, vouching, and interpreting, to arrive at a decentralized and sybil-resistant consensus for identity. In this article, we will outline the approaches of these new and natively digital sources of authentication -- their attributes, methodologies strengths, and weaknesses -- and sketch out possible directions for future developments.},
  file = {/Users/lukakuma/Zotero/storage/78GYZR49/Siddarth et al. - 2020 - Who Watches the Watchmen A Review of Subjective A.pdf;/Users/lukakuma/Zotero/storage/D7LYS7IY/2008.html}
}

@article{silverDeterministicPolicyGradient2014,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  date = {2014},
  pages = {9},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/7V2WZ5YG/Silver et al. - Deterministic Policy Gradient Algorithms.pdf}
}

@online{silverMasteringChessShogi2017,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self-Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2017-12-05},
  number = {arXiv:1712.01815},
  eprint = {arXiv:1712.01815},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1712.01815},
  urldate = {2022-12-22},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  pubstate = {preprint},
  keywords = {DeepMind,self-play},
  file = {/Users/lukakuma/Zotero/storage/AH44RQ6L/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf;/Users/lukakuma/Zotero/storage/D32WXAJ9/1712.html}
}

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  date = {2016-01},
  journaltitle = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/nature16961},
  urldate = {2022-06-09},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  issue = {7587},
  langid = {english},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/4FLI6C27/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf;/Users/lukakuma/Zotero/storage/QEB9AADT/nature16961.html}
}

@article{silverMasteringGameGo2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Graepel, Thore and Hassabis, Demis},
  date = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/nature24270},
  urldate = {2022-06-09},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100\textendash 0 against the previously published, champion-defeating AlphaGo.},
  issue = {7676},
  langid = {english},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/ZNY3ZNMC/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf;/Users/lukakuma/Zotero/storage/EIXR7UWY/nature24270.html}
}

@article{silverRewardEnough2021,
  title = {Reward Is Enough},
  author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  date = {2021-10-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {299},
  pages = {103535},
  issn = {0004-3702},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370221000862},
  urldate = {2022-06-06},
  abstract = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.},
  langid = {english},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/KRI8ANFJ/Silver et al. - 2021 - Reward is enough.pdf;/Users/lukakuma/Zotero/storage/VNEW5SSC/S0004370221000862.html}
}

@online{singhalLargeLanguageModels2022a,
  title = {Large {{Language Models Encode Clinical Knowledge}}},
  author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Scharli, Nathaneal and Chowdhery, Aakanksha and Mansfield, Philip and family=Arcas, given=Blaise Aguera, prefix=y, useprefix=false and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
  date = {2022-12-26},
  number = {arXiv:2212.13138},
  eprint = {arXiv:2212.13138},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.13138},
  urldate = {2023-01-09},
  abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this, we present MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. In addition, we evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics), including 67.6\% accuracy on MedQA (US Medical License Exam questions), surpassing prior state-of-the-art by over 17\%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.},
  pubstate = {preprint},
  keywords = {*peft,3-prefix tuning,read},
  file = {/Users/lukakuma/Zotero/storage/2I85D4BQ/Singhal et al. - 2022 - Large Language Models Encode Clinical Knowledge.pdf;/Users/lukakuma/Zotero/storage/CNYH4DBL/2212.html}
}

@online{singhFLAVAFoundationalLanguage2022,
  title = {{{FLAVA}}: {{A Foundational Language And Vision Alignment Model}}},
  shorttitle = {{{FLAVA}}},
  author = {Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  date = {2022-03-29},
  number = {arXiv:2112.04482},
  eprint = {arXiv:2112.04482},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.04482},
  urldate = {2022-07-25},
  abstract = {State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising direction would be to use a single holistic universal model, as a "foundation", that targets all modalities at once -- a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/4JP9XZ3C/Singh et al. - 2022 - FLAVA A Foundational Language And Vision Alignmen.pdf;/Users/lukakuma/Zotero/storage/ZNVUAESF/2112.html}
}

@article{smithDevelopmentalApproachMachine2017,
  title = {A {{Developmental Approach}} to {{Machine Learning}}?},
  author = {Smith, Linda B. and Slone, Lauren K.},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  volume = {8},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.02124},
  urldate = {2022-07-26},
  abstract = {Visual learning depends on both the algorithms and the training material. This essay considers the natural statistics of infant- and toddler-egocentric vision. These natural training sets for human visual object recognition are very different from the training data fed into machine vision systems. Rather than equal experiences with all kinds of things, toddlers experience extremely skewed distributions with many repeated occurrences of a very few things. And though highly variable when considered as a whole, individual views of things are experienced in a specific order \textendash{} with slow, smooth visual changes moment-to-moment, and developmentally ordered transitions in scene content. We propose that the skewed, ordered, biased visual experiences of infants and toddlers are the training data that allow human learners to develop a way to recognize everything, both the pervasively present entities and the rarely encountered ones. The joint consideration of real-world statistics for learning by researchers of human and machine learning seems likely to bring advances in both disciplines.},
  file = {/Users/lukakuma/Zotero/storage/YS83K38S/Smith and Slone - 2017 - A Developmental Approach to Machine Learning.pdf}
}

@article{smithDevelopmentEmbodiedCognition2005,
  title = {The {{Development}} of {{Embodied Cognition}}: {{Six Lessons}} from {{Babies}}},
  shorttitle = {The {{Development}} of {{Embodied Cognition}}},
  author = {Smith, Linda and Gasser, Michael},
  date = {2005-12-01},
  journaltitle = {Artificial life},
  shortjournal = {Artificial life},
  volume = {11},
  pages = {13--29},
  doi = {10.1162/1064546053278973},
  abstract = {The embodiment hypothesis is the idea that intelligence emerges in the interaction of an agent with an environment and as a result of sensorimotor activity. We offer six lessons for developing embodied intelligent agents suggested by research in developmental psychology. We argue that starting as a baby grounded in a physical, social, and linguistic world is crucial to the development of the flexible and inventive intelligence that characterizes humankind.},
  file = {/Users/lukakuma/Zotero/storage/8PSX4FSG/Smith and Gasser - 2005 - The Development of Embodied Cognition Six Lessons.pdf}
}

@online{smithUsingDeepSpeedMegatron2022,
  title = {Using {{DeepSpeed}} and {{Megatron}} to {{Train Megatron-Turing NLG 530B}}, {{A Large-Scale Generative Language Model}}},
  author = {Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and Zhang, Elton and Child, Rewon and Aminabadi, Reza Yazdani and Bernauer, Julie and Song, Xia and Shoeybi, Mohammad and He, Yuxiong and Houston, Michael and Tiwary, Saurabh and Catanzaro, Bryan},
  date = {2022-02-04},
  number = {arXiv:2201.11990},
  eprint = {arXiv:2201.11990},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.11990},
  urldate = {2022-09-06},
  abstract = {Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/LYQQ66AQ/Smith et al. - 2022 - Using DeepSpeed and Megatron to Train Megatron-Tur.pdf;/Users/lukakuma/Zotero/storage/NTHSBKQ9/2201.html}
}

@online{snellOfflineRLNatural2022a,
  title = {Offline {{RL}} for {{Natural Language Generation}} with {{Implicit Language Q Learning}}},
  author = {Snell, Charlie and Kostrikov, Ilya and Su, Yi and Yang, Mengjiao and Levine, Sergey},
  date = {2022-06-05},
  number = {arXiv:2206.11871},
  eprint = {arXiv:2206.11871},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.11871},
  urldate = {2023-02-20},
  abstract = {Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL motivated method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility optimization framework of traditional RL algorithms with supervised learning's ability to leverage existing data and its simplicity and stability. Our method, based on dynamic programming, employs a blend of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing utility. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language generation settings, demonstrating how it can be a more effective utility optimizer than prior approaches for end-to-end dialogue, and how it can effectively optimize high variance reward functions based on subjective judgement, such as whether to label a comment as an example of toxic speech or not.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/JSWLHEFH/Snell et al. - 2022 - Offline RL for Natural Language Generation with Im.pdf;/Users/lukakuma/Zotero/storage/CWTPA37S/2206.html}
}

@online{sodhaniIntroductionLifelongSupervised2022,
  title = {An {{Introduction}} to {{Lifelong Supervised Learning}}},
  author = {Sodhani, Shagun and Faramarzi, Mojtaba and Mehta, Sanket Vaibhav and Malviya, Pranshu and Abdelsalam, Mohamed and Janarthanan, Janarthanan and Chandar, Sarath},
  date = {2022-07-12},
  number = {arXiv:2207.04354},
  eprint = {arXiv:2207.04354},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.04354},
  urldate = {2022-07-26},
  abstract = {This primer is an attempt to provide a detailed summary of the different facets of lifelong learning. We start with Chapter 2 which provides a high-level overview of lifelong learning systems. In this chapter, we discuss prominent scenarios in lifelong learning (Section 2.4), provide 8 Introduction a high-level organization of different lifelong learning approaches (Section 2.5), enumerate the desiderata for an ideal lifelong learning system (Section 2.6), discuss how lifelong learning is related to other learning paradigms (Section 2.7), describe common metrics used to evaluate lifelong learning systems (Section 2.8). This chapter is more useful for readers who are new to lifelong learning and want to get introduced to the field without focusing on specific approaches or benchmarks. The remaining chapters focus on specific aspects (either learning algorithms or benchmarks) and are more useful for readers who are looking for specific approaches or benchmarks. Chapter 3 focuses on regularization-based approaches that do not assume access to any data from previous tasks. Chapter 4 discusses memory-based approaches that typically use a replay buffer or an episodic memory to save subset of data across different tasks. Chapter 5 focuses on different architecture families (and their instantiations) that have been proposed for training lifelong learning systems. Following these different classes of learning algorithms, we discuss the commonly used evaluation benchmarks and metrics for lifelong learning (Chapter 6) and wrap up with a discussion of future challenges and important research directions in Chapter 7.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/GBHZ6BYX/Sodhani et al. - 2022 - An Introduction to Lifelong Supervised Learning.pdf;/Users/lukakuma/Zotero/storage/XJHG29HC/2207.html}
}

@online{solaimanProcessAdaptingLanguage2021,
  title = {Process for {{Adapting Language Models}} to {{Society}} ({{PALMS}}) with {{Values-Targeted Datasets}}},
  author = {Solaiman, Irene and Dennison, Christy},
  date = {2021-11-23},
  number = {arXiv:2106.10328},
  eprint = {arXiv:2106.10328},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.10328},
  urldate = {2022-11-19},
  abstract = {Language models can generate harmful and biased outputs and exhibit undesirable behavior according to a given cultural context. We propose a Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets, an iterative process to significantly change model behavior by crafting and fine-tuning on a dataset that reflects a predetermined set of target values. We evaluate our process using three metrics: quantitative metrics with human evaluations that score output adherence to a target value, toxicity scoring on outputs; and qualitative metrics analyzing the most common word associated with a given social category. Through each iteration, we add additional training dataset examples based on observed shortcomings from evaluations. PALMS performs significantly better on all metrics compared to baseline and control models for a broad range of GPT-3 language model sizes without compromising capability integrity. We find that the effectiveness of PALMS increases with model size. We show that significantly adjusting language model behavior is feasible with a small, hand-curated dataset.},
  pubstate = {preprint},
  keywords = {alignment},
  file = {/Users/lukakuma/Zotero/storage/PV89LEEK/Solaiman and Dennison - 2021 - Process for Adapting Language Models to Society (P.pdf;/Users/lukakuma/Zotero/storage/8474ILC4/2106.html}
}

@online{soltanAlexaTM20BFewShot2022,
  title = {{{AlexaTM 20B}}: {{Few-Shot Learning Using}} a {{Large-Scale Multilingual Seq2Seq Model}}},
  shorttitle = {{{AlexaTM 20B}}},
  author = {Soltan, Saleh and Ananthakrishnan, Shankar and FitzGerald, Jack and Gupta, Rahul and Hamza, Wael and Khan, Haidar and Peris, Charith and Rawls, Stephen and Rosenbaum, Andy and Rumshisky, Anna and Prakash, Chandana Satya and Sridhar, Mukund and Triefenbach, Fabian and Verma, Apurv and Tur, Gokhan and Natarajan, Prem},
  date = {2022-08-03},
  number = {arXiv:2208.01448},
  eprint = {arXiv:2208.01448},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.01448},
  urldate = {2022-10-06},
  abstract = {In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/C4J7LKHF/Soltan et al. - 2022 - AlexaTM 20B Few-Shot Learning Using a Large-Scale.pdf;/Users/lukakuma/Zotero/storage/CJULVRR5/2208.html}
}

@online{songConsistencyModels2023a,
  title = {Consistency {{Models}}},
  author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  date = {2023-03-02},
  number = {arXiv:2303.01469},
  eprint = {arXiv:2303.01469},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2303.01469},
  urldate = {2023-03-04},
  abstract = {Diffusion models have made significant breakthroughs in image, audio, and video generation, but they depend on an iterative generation process that causes slow sampling speed and caps their potential for real-time applications. To overcome this limitation, we propose consistency models, a new family of generative models that achieve high sample quality without adversarial training. They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. They also support zero-shot data editing, like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either as a way to distill pre-trained diffusion models, or as standalone generative models. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step generation. For example, we achieve the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained as standalone generative models, consistency models also outperform single-step, non-adversarial generative models on standard benchmarks like CIFAR-10, ImageNet 64x64 and LSUN 256x256.},
  pubstate = {preprint},
  keywords = {image generation},
  file = {/Users/lukakuma/Zotero/storage/HD2ZMRSW/Song et al. - 2023 - Consistency Models.pdf;/Users/lukakuma/Zotero/storage/MN2V7MBB/2303.html}
}

@unpublished{songHyParHybridParallelism2020,
  title = {{{HyPar}}: {{Towards Hybrid Parallelism}} for {{Deep Learning Accelerator Array}}},
  shorttitle = {{{HyPar}}},
  author = {Song, Linghao and Mao, Jiachen and Zhuo, Youwei and Qian, Xuehai and Li, Hai and Chen, Yiran},
  date = {2020-01-16},
  eprint = {1901.02067},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1901.02067},
  urldate = {2022-04-14},
  abstract = {With the rise of artificial intelligence in recent years, Deep Neural Networks (DNNs) have been widely used in many domains. To achieve high performance and energy efficiency, hardware acceleration (especially inference) of DNNs is intensively studied both in academia and industry. However, we still face two challenges: large DNN models and datasets, which incur frequent off-chip memory accesses; and the training of DNNs, which is not well-explored in recent accelerator designs. To truly provide high throughput and energy efficient acceleration for the training of deep and large models, we inevitably need to use multiple accelerators to explore the coarse-grain parallelism, compared to the fine-grain parallelism inside a layer considered in most of the existing architectures. It poses the key research question to seek the best organization of computation and dataflow among accelerators. In this paper, inspired by recent work in machine learning systems, we propose a solution HyPar to determine layer-wise parallelism for deep neural network training with an array of DNN accelerators. HyPar partitions the feature map tensors (input and output), the kernel tensors, the gradient tensors, and the error tensors for the DNN accelerators. A partition constitutes the choice of parallelism for weighted layers. The optimization target is to search a partition that minimizes the total communication during training a complete DNN. To solve this problem, we propose a communication model to explain the source and amount of communications. Then, we use a hierarchical layer-wise dynamic programming method to search for the partition for each layer.},
  file = {/Users/lukakuma/Zotero/storage/T6IU24J9/Song et al. - 2020 - HyPar Towards Hybrid Parallelism for Deep Learnin.pdf;/Users/lukakuma/Zotero/storage/JIV5T8DG/1901.html}
}

@online{sorscherNeuralScalingLaws2022a,
  title = {Beyond Neural Scaling Laws: Beating Power Law Scaling via Data Pruning},
  shorttitle = {Beyond Neural Scaling Laws},
  author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
  date = {2022-11-15},
  number = {arXiv:2206.14486},
  eprint = {arXiv:2206.14486},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.14486},
  urldate = {2022-12-15},
  abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
  pubstate = {preprint},
  keywords = {scaling law},
  file = {/Users/lukakuma/Zotero/storage/V2L3TCMN/Sorscher et al. - 2022 - Beyond neural scaling laws beating power law scal.pdf;/Users/lukakuma/Zotero/storage/A37XWZNS/2206.html}
}

@online{srivastavaImitationGameQuantifying2022a,
  title = {Beyond the {{Imitation Game}}: {{Quantifying}} and Extrapolating the Capabilities of Language Models},
  shorttitle = {Beyond the {{Imitation Game}}},
  author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri\`a and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlm\"uller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karaka\c{s}, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bart\l omiej and \"Ozyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ram\'irez, C\'esar Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and Gonz\'alez, Daniel Mosegu\'i and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Mart\'inez-Plumed, Fernando and Happ\'e, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and family=Melo, given=Gerard, prefix=de, useprefix=true and Kruszewski, Germ\'an and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-L\'opez, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Sch\"utze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fern\'andez and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Koco\'n, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, J\"org and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Col\'on, Luis Oliveros and Metz, Luke and \c{S}enel, L\"utfi Kerem and Bosma, Maarten and Sap, Maarten and family=Hoeve, given=Maartje, prefix=ter, useprefix=true and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ram\'irez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, M\'aty\'as and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Sw\k{e}drowski, Micha\l{} and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Mi\l kowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ram\'on Risco and Milli\`ere, Rapha\"el and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Th\'eo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
  date = {2022-06-10},
  number = {arXiv:2206.04615},
  eprint = {arXiv:2206.04615},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.04615},
  urldate = {2022-12-21},
  abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/UL7ZIM7X/Srivastava et al. - 2022 - Beyond the Imitation Game Quantifying and extrapo.pdf;/Users/lukakuma/Zotero/storage/KU4RZ6C7/2206.html}
}

@online{stadieConsiderationsLearningExplore2019,
  title = {Some {{Considerations}} on {{Learning}} to {{Explore}} via {{Meta-Reinforcement Learning}}},
  author = {Stadie, Bradly C. and Yang, Ge and Houthooft, Rein and Chen, Xi and Duan, Yan and Wu, Yuhuai and Abbeel, Pieter and Sutskever, Ilya},
  date = {2019-01-11},
  number = {arXiv:1803.01118},
  eprint = {arXiv:1803.01118},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.01118},
  urldate = {2022-07-23},
  abstract = {We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-\$\textbackslash text\{RL\}\^2\$. Results are presented on a novel environment we call `Krazy World' and a set of maze environments. We show E-MAML and E-\$\textbackslash text\{RL\}\^2\$ deliver better performance on tasks where exploration is important.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/DPC5UYBF/Stadie et al. - 2019 - Some Considerations on Learning to Explore via Met.pdf;/Users/lukakuma/Zotero/storage/2CKSTV9A/1803.html}
}

@online{stadieThirdPersonImitationLearning2019,
  title = {Third-{{Person Imitation Learning}}},
  author = {Stadie, Bradly C. and Abbeel, Pieter and Sutskever, Ilya},
  date = {2019-09-22},
  number = {arXiv:1703.01703},
  eprint = {arXiv:1703.01703},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.01703},
  urldate = {2022-07-23},
  abstract = {Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/5I77JRLN/Stadie et al. - 2019 - Third-Person Imitation Learning.pdf;/Users/lukakuma/Zotero/storage/ESN53UT5/1703.html}
}

@video{stanfordhaiJackClarkPresenting2022,
  title = {Jack {{Clark Presenting}} the 2022 {{AI Index Report}}},
  editor = {{Stanford HAI}},
  date = {2022-03-31},
  url = {https://www.youtube.com/watch?v=1xLxlMdcbpM},
  urldate = {2022-04-01},
  abstract = {Please watch to hear AI Index Steering Committee Co-Chair Jack Clark share a presentation on the key findings of the 2022 AI Index Report, released on March 16. The AI Index is an independent initiative at Stanford HAI led by an interdisciplinary steering committee of experts from across academia and industry. Its goal is to be the world's most credible and authoritative source for data and insights about AI to provide policymakers, researchers, journalists, executives, and the general public a deeper understanding of the field.},
  editortype = {director}
}

@article{sterlingZINC15Ligand2015,
  title = {{{ZINC}} 15 \textendash{} {{Ligand Discovery}} for {{Everyone}}},
  author = {Sterling, Teague and Irwin, John J.},
  date = {2015-11-23},
  journaltitle = {Journal of Chemical Information and Modeling},
  shortjournal = {J. Chem. Inf. Model.},
  volume = {55},
  number = {11},
  pages = {2324--2337},
  publisher = {{American Chemical Society}},
  issn = {1549-9596},
  url = {https://doi.org/10.1021/acs.jcim.5b00559},
  urldate = {2022-05-01},
  abstract = {Many questions about the biological activity and availability of small molecules remain inaccessible to investigators who could most benefit from their answers. To narrow the gap between chemoinformatics and biology, we have developed a suite of ligand annotation, purchasability, target, and biology association tools, incorporated into ZINC and meant for investigators who are not computer specialists. The new version contains over 120 million purchasable ``drug-like'' compounds \textendash{} effectively all organic molecules that are for sale \textendash{} a quarter of which are available for immediate delivery. ZINC connects purchasable compounds to high-value ones such as metabolites, drugs, natural products, and annotated compounds from the literature. Compounds may be accessed by the genes for which they are annotated as well as the major and minor target classes to which those genes belong. It offers new analysis tools that are easy for nonspecialists yet with few limitations for experts. ZINC retains its original 3D roots \textendash{} all molecules are available in biologically relevant, ready-to-dock formats. ZINC is freely available at http://zinc15.docking.org.},
  file = {/Users/lukakuma/Zotero/storage/WYNSGXC7/Sterling and Irwin - 2015 - ZINC 15 ‚Äì Ligand Discovery for Everyone.pdf;/Users/lukakuma/Zotero/storage/EWL4DEVF/acs.jcim.html}
}

@online{stiennonLearningSummarizeHuman2022a,
  title = {Learning to Summarize from Human Feedback},
  author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
  date = {2022-02-15},
  number = {arXiv:2009.01325},
  eprint = {arXiv:2009.01325},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.01325},
  urldate = {2023-02-20},
  abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
  pubstate = {preprint},
  keywords = {RLHF},
  file = {/Users/lukakuma/Zotero/storage/RFYHSHGS/Stiennon et al. - 2022 - Learning to summarize from human feedback.pdf;/Users/lukakuma/Zotero/storage/J9FLQQ4K/2009.html}
}

@inproceedings{stookeDecouplingRepresentationLearning2021,
  title = {Decoupling {{Representation Learning}} from {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Stooke, Adam and Lee, Kimin and Abbeel, Pieter and Laskin, Michael},
  date = {2021-07-01},
  pages = {9870--9879},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/stooke21a.html},
  urldate = {2022-06-07},
  abstract = {In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at \textbackslash url\{https://github.com/astooke/rlpyt/tree/master/rlpyt/ul\}.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/6VVCL648/Stooke et al. - 2021 - Decoupling Representation Learning from Reinforcem.pdf;/Users/lukakuma/Zotero/storage/RT5EPYYB/Stooke et al. - 2021 - Decoupling Representation Learning from Reinforcem.pdf}
}

@online{strouseCollaboratingHumansHuman2022,
  title = {Collaborating with {{Humans}} without {{Human Data}}},
  author = {Strouse, D. J. and McKee, Kevin R. and Botvinick, Matt and Hughes, Edward and Everett, Richard},
  date = {2022-01-07},
  number = {arXiv:2110.08176},
  eprint = {arXiv:2110.08176},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.08176},
  urldate = {2023-02-20},
  abstract = {Collaborating with humans requires rapidly adapting to their individual strengths, weaknesses, and preferences. Unfortunately, most standard multi-agent reinforcement learning techniques, such as self-play (SP) or population play (PP), produce agents that overfit to their training partners and do not generalize well to humans. Alternatively, researchers can collect human data, train a human model using behavioral cloning, and then use that model to train "human-aware" agents ("behavioral cloning play", or BCP). While such an approach can improve the generalization of agents to new human co-players, it involves the onerous and expensive step of collecting large amounts of human data first. Here, we study the problem of how to train agents that collaborate well with human partners without using human data. We argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, we find that a surprisingly simple approach is highly effective. We train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method we call Fictitious Co-Play (FCP). Our experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. We find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also report a strong subjective preference to partnering with FCP agents over all baselines.},
  pubstate = {preprint},
  keywords = {multiagent},
  file = {/Users/lukakuma/Zotero/storage/UCDMGGYI/Strouse et al. - 2022 - Collaborating with Humans without Human Data.pdf;/Users/lukakuma/Zotero/storage/X9ULJQ49/2110.html}
}

@inproceedings{strouseLearningMoreSkills2021,
  title = {Learning More Skills through Optimistic Exploration},
  author = {Strouse, D. J. and Baumli, Kate and Warde-Farley, David and Mnih, Volodymyr and Hansen, Steven Stenberg},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=cU8rknuhxc},
  urldate = {2022-05-15},
  abstract = {Unsupervised skill learning objectives (Eysenbach et al., 2019; Gregor et al., 2016) allow agents to learn rich repertoires of behavior in the absence of extrinsic rewards. They work by...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/I32J7YCC/Strouse et al. - 2021 - Learning more skills through optimistic exploratio.pdf;/Users/lukakuma/Zotero/storage/AUWVCK7I/forum.html}
}

@online{sungTrainingNeuralNetworks2021,
  title = {Training {{Neural Networks}} with {{Fixed Sparse Masks}}},
  author = {Sung, Yi-Lin and Nair, Varun and Raffel, Colin},
  date = {2021-11-18},
  number = {arXiv:2111.09839},
  eprint = {arXiv:2111.09839},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.09839},
  urldate = {2022-12-12},
  abstract = {During typical gradient-based training of deep neural networks, all of the model's parameters are updated at each iteration. Recent work has shown that it is possible to update only a small subset of the model's parameters during training, which can alleviate storage and communication requirements. In this paper, we show that it is possible to induce a fixed sparse mask on the model's parameters that selects a subset to update over many iterations. Our method constructs the mask out of the \$k\$ parameters with the largest Fisher information as a simple approximation as to which parameters are most important for the task at hand. In experiments on parameter-efficient transfer learning and distributed training, we show that our approach matches or exceeds the performance of other methods for training with sparse updates while being more efficient in terms of memory usage and communication costs. We release our code publicly to promote further applications of our approach.},
  pubstate = {preprint},
  keywords = {*peft},
  file = {/Users/lukakuma/Zotero/storage/W7XNK742/Sung et al. - 2021 - Training Neural Networks with Fixed Sparse Masks.pdf;/Users/lukakuma/Zotero/storage/J56EU6W6/2111.html}
}

@online{sunLAMOLLAnguageMOdeling2019,
  title = {{{LAMOL}}: {{LAnguage MOdeling}} for {{Lifelong Language Learning}}},
  shorttitle = {{{LAMOL}}},
  author = {Sun, Fan-Keng and Ho, Cheng-Hao and Lee, Hung-Yi},
  date = {2019-12-22},
  number = {arXiv:1909.03329},
  eprint = {arXiv:1909.03329},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.03329},
  urldate = {2022-12-13},
  abstract = {Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language modeling. LAMOL replays pseudo-samples of previous tasks while requiring no extra memory or model capacity. Specifically, LAMOL is a language model that simultaneously learns to solve the tasks and generate training samples. When the model is trained for a new task, it generates pseudo-samples of previous tasks for training alongside data for the new task. The results show that LAMOL prevents catastrophic forgetting without any sign of intransigence and can perform five very different language tasks sequentially with only one model. Overall, LAMOL outperforms previous methods by a considerable margin and is only 2-3\% worse than multitasking, which is usually considered the LLL upper bound. The source code is available at https://github.com/jojotenya/LAMOL.},
  pubstate = {preprint},
  keywords = {continual learning},
  file = {/Users/lukakuma/Zotero/storage/CKDBFM9D/Sun et al. - 2019 - LAMOL LAnguage MOdeling for Lifelong Language Lea.pdf;/Users/lukakuma/Zotero/storage/BUHE6NYG/1909.html}
}

@unpublished{sunReasoningVirtualKnowledge2021,
  title = {Reasoning {{Over Virtual Knowledge Bases With Open Predicate Relations}}},
  author = {Sun, Haitian and Verga, Pat and Dhingra, Bhuwan and Salakhutdinov, Ruslan and Cohen, William W.},
  date = {2021-06-14},
  eprint = {2102.07043},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.07043},
  urldate = {2022-04-13},
  abstract = {We present the Open Predicate Query Language (OPQL); a method for constructing a virtual KB (VKB) trained entirely from text. Large Knowledge Bases (KBs) are indispensable for a wide-range of industry applications such as question answering and recommendation. Typically, KBs encode world knowledge in a structured, readily accessible form derived from laborious human annotation efforts. Unfortunately, while they are extremely high precision, KBs are inevitably highly incomplete and automated methods for enriching them are far too inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set of relation mentions in a way that naturally enables reasoning and can be trained without any structured supervision. We demonstrate that OPQL outperforms prior VKB methods on two different KB reasoning tasks and, additionally, can be used as an external memory integrated into a language model (OPQL-LM) leading to improvements on two open-domain question answering tasks.},
  file = {/Users/lukakuma/Zotero/storage/KDB3K3J3/Sun et al. - 2021 - Reasoning Over Virtual Knowledge Bases With Open P.pdf;/Users/lukakuma/Zotero/storage/3VHDV6JG/2102.html}
}

@online{sunRecitationAugmentedLanguageModels2022a,
  title = {Recitation-{{Augmented Language Models}}},
  author = {Sun, Zhiqing and Wang, Xuezhi and Tay, Yi and Yang, Yiming and Zhou, Denny},
  date = {2022-10-03},
  number = {arXiv:2210.01296},
  eprint = {arXiv:2210.01296},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.01296},
  urldate = {2022-12-19},
  abstract = {We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on three pre-trained models (PaLM, UL2, and OPT) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA).},
  pubstate = {preprint},
  keywords = {5-reasoning,read},
  file = {/Users/lukakuma/Zotero/storage/694JWCKT/Sun et al. - 2022 - Recitation-Augmented Language Models.pdf;/Users/lukakuma/Zotero/storage/3N64EF9F/2210.html;/Users/lukakuma/Zotero/storage/RAJAJH35/2210.html}
}

@online{suttonAlbertaPlanAI2022,
  title = {The {{Alberta Plan}} for {{AI Research}}},
  author = {Sutton, Richard S. and Bowling, Michael H. and Pilarski, Patrick M.},
  date = {2022-08-23},
  number = {arXiv:2208.11173},
  eprint = {arXiv:2208.11173},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.11173},
  urldate = {2022-09-05},
  abstract = {Herein we describe our approach to artificial intelligence research, which we call the Alberta Plan. The Alberta Plan is pursued within our research groups in Alberta and by others who are like minded throughout the world. We welcome all who would join us in this pursuit.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/U22JHRSY/Sutton et al. - 2022 - The Alberta Plan for AI Research.pdf;/Users/lukakuma/Zotero/storage/4JGQ63LW/2208.html}
}

@online{suttonBitterLesson2019,
  title = {The {{Bitter Lesson}}},
  author = {Sutton, Richard S.},
  date = {2019},
  url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  urldate = {2022-04-06},
  keywords = {(ext) Gato,read},
  file = {/Users/lukakuma/Zotero/storage/N4JJDQC3/BitterLesson.html}
}

@unpublished{suttonHistoryMetagradientGradient2022,
  title = {A {{History}} of {{Meta-gradient}}: {{Gradient Methods}} for {{Meta-learning}}},
  shorttitle = {A {{History}} of {{Meta-gradient}}},
  author = {Sutton, Richard S.},
  date = {2022-02-19},
  number = {arXiv:2202.09701},
  eprint = {2202.09701},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.09701},
  urldate = {2022-06-07},
  abstract = {The history of meta-learning methods based on gradient descent is reviewed, focusing primarily on methods that adapt step-size (learning rate) meta-parameters.},
  file = {/Users/lukakuma/Zotero/storage/PSGF8IS8/Sutton - 2022 - A History of Meta-gradient Gradient Methods for M.pdf;/Users/lukakuma/Zotero/storage/XD3P95J2/2202.html}
}

@article{suttonJohnMcCarthyDefinition2020,
  title = {John {{McCarthy}}'s {{Definition}} of {{Intelligence}}},
  author = {Sutton, Richard S.},
  date = {2020-02-01},
  journaltitle = {Journal of Artificial General Intelligence},
  volume = {11},
  number = {2},
  pages = {1--100},
  issn = {1946-0163},
  url = {https://www.sciendo.com/article/10.2478/jagi-2020-0003},
  urldate = {2022-05-29},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/JGXBM6QU/Monett et al. - 2020 - Special Issue ‚ÄúOn Defining Artificial Intelligence.pdf}
}

@unpublished{suttonQuestCommonModel2022,
  title = {The {{Quest}} for a {{Common Model}} of the {{Intelligent Decision Maker}}},
  author = {Sutton, Richard S.},
  date = {2022-04-07},
  number = {arXiv:2202.13252},
  eprint = {2202.13252},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.13252},
  urldate = {2022-05-27},
  abstract = {The premise of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the "common model of the intelligent agent". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent.},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/URXRPDLK/Sutton - 2022 - The Quest for a Common Model of the Intelligent De.pdf;/Users/lukakuma/Zotero/storage/82YEWIG8/2202.html}
}

@book{suttonReinforcementLearningIntroduction2020,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, Richard S.},
  date = {2020},
  url = {https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981},
  urldate = {2022-06-08},
  file = {/Users/lukakuma/Zotero/storage/4YWVMRAH/Reinforcement Learning An Introduction (Adaptive .pdf}
}

@article{suzgunChallengingBIGBenchTasks2022,
  title = {Challenging {{BIG-Bench Tasks}} and {{Whether Chain-of-Thought Can Solve Them}}},
  author = {Suzgun, Mirac and Scales, Nathan and Sch\"arli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Wei, Jason},
  date = {2022-10-17},
  url = {https://arxiv.org/abs/2210.09261v1},
  urldate = {2022-12-21},
  abstract = {BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65\% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.},
  langid = {english},
  keywords = {5-reasoning,read},
  file = {/Users/lukakuma/Zotero/storage/B9T3QS32/Suzgun et al. - 2022 - Challenging BIG-Bench Tasks and Whether Chain-of-T.pdf}
}

@article{takamotoUniversalNeuralNetwork2022,
  title = {Towards Universal Neural Network Potential for Material Discovery Applicable to Arbitrary Combination of 45 Elements},
  author = {Takamoto, So and Shinagawa, Chikashi and Motoki, Daisuke and Nakago, Kosuke and Li, Wenwen and Kurata, Iori and Watanabe, Taku and Yayama, Yoshihiro and Iriguchi, Hiroki and Asano, Yusuke and Onodera, Tasuku and Ishii, Takafumi and Kudo, Takao and Ono, Hideki and Sawada, Ryohto and Ishitani, Ryuichiro and Ong, Marc and Yamaguchi, Taiki and Kataoka, Toshiki and Hayashi, Akihide and Charoenphakdee, Nontawat and Ibuka, Takeshi},
  date = {2022-05-30},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {2991},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  url = {https://www.nature.com/articles/s41467-022-30687-9},
  urldate = {2022-05-30},
  abstract = {Computational material discovery is under intense study owing to its ability to explore the vast space of chemical systems. Neural network potentials (NNPs) have been shown to be particularly effective in conducting atomistic simulations for such purposes. However, existing NNPs are generally designed for narrow target materials, making them unsuitable for broader applications in material discovery. Here we report a development of universal NNP called PreFerred Potential (PFP), which is able to handle any combination of 45 elements. Particular emphasis is placed on the datasets, which include a diverse set of virtual structures used to attain the universality. We demonstrated the applicability of PFP in selected domains: lithium diffusion in LiFeSO4F, molecular adsorption in metal-organic frameworks, an order\textendash disorder transition of Cu-Au alloys, and material discovery for a Fischer\textendash Tropsch catalyst. They showcase the power of PFP, and this technology provides a highly useful tool for material discovery.},
  issue = {1},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/HLHZEWZL/Takamoto et al. - 2022 - Towards universal neural network potential for mat.pdf;/Users/lukakuma/Zotero/storage/4PINWKNV/s41467-022-30687-9.html}
}

@report{tambeDigitalCapitalSuperstar2020,
  type = {Working Paper},
  title = {Digital {{Capital}} and {{Superstar Firms}}},
  author = {Tambe, Prasanna and Hitt, Lorin and Rock, Daniel and Brynjolfsson, Erik},
  date = {2020-12},
  series = {Working {{Paper Series}}},
  number = {28285},
  institution = {{National Bureau of Economic Research}},
  url = {https://www.nber.org/papers/w28285},
  urldate = {2022-03-09},
  abstract = {General purpose technologies like information technology typically require complementary firm-specific investments to create value. These complementary investments produce a form of capital, which is typically intangible and which we call digital capital. We create an extended firm-level panel on IT labor investments (1990-2016) using data from LinkedIn. We then apply Hall's Quantity Revelation Theorem to compute both prices and quantities of digital capital over recent decades. We find that 1) digital capital prices vary significantly over time, peaking around the dot-com boom in 2000, 2) significant digital capital quantities have accumulated since the 1990s, with digital capital accounting for at least 25\% of firms' assets by the end of our panel, 3) that digital capital has disproportionately accumulated in a small subset of ``superstar'' firms and its concentration is much greater than the concentration of other assets, and 4) that digital capital accumulation predicts firm-level productivity about three years in the future.},
  file = {/Users/lukakuma/Zotero/storage/CVHQRWS7/Tambe et al. - 2020 - Digital Capital and Superstar Firms.pdf}
}

@online{tamEvaluatingFactualConsistency2022,
  title = {Evaluating the {{Factual Consistency}} of {{Large Language Models Through Summarization}}},
  author = {Tam, Derek and Mascarenhas, Anisha and Zhang, Shiyue and Kwan, Sarah and Bansal, Mohit and Raffel, Colin},
  date = {2022-11-15},
  number = {arXiv:2211.08412},
  eprint = {arXiv:2211.08412},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.08412},
  urldate = {2022-11-17},
  abstract = {While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\textbackslash{} the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.},
  pubstate = {preprint},
  keywords = {knowledge,read,summarization},
  file = {/Users/lukakuma/Zotero/storage/H5MCS6N6/Tam et al. - 2022 - Evaluating the Factual Consistency of Large Langua.pdf;/Users/lukakuma/Zotero/storage/6QG5E8FV/2211.html}
}

@online{tamkinUnderstandingCapabilitiesLimitations2021a,
  title = {Understanding the {{Capabilities}}, {{Limitations}}, and {{Societal Impact}} of {{Large Language Models}}},
  author = {Tamkin, Alex and Brundage, Miles and Clark, Jack and Ganguli, Deep},
  date = {2021-02-04},
  number = {arXiv:2102.02503},
  eprint = {arXiv:2102.02503},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.02503},
  urldate = {2022-11-19},
  abstract = {On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.},
  pubstate = {preprint},
  keywords = {OpenAI},
  file = {/Users/lukakuma/Zotero/storage/IYLHZSRA/Tamkin et al. - 2021 - Understanding the Capabilities, Limitations, and S.pdf;/Users/lukakuma/Zotero/storage/9222H29V/2102.html}
}

@unpublished{tangSparseMLPImage2022,
  title = {Sparse {{MLP}} for {{Image Recognition}}: {{Is Self-Attention Really Necessary}}?},
  shorttitle = {Sparse {{MLP}} for {{Image Recognition}}},
  author = {Tang, Chuanxin and Zhao, Yucheng and Wang, Guangting and Luo, Chong and Xie, Wenxuan and Zeng, Wenjun},
  date = {2022-05-29},
  number = {arXiv:2109.05422},
  eprint = {2109.05422},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.05422},
  urldate = {2022-06-09},
  abstract = {Transformers have sprung up in the field of computer vision. In this work, we explore whether the core self-attention module in Transformer is the key to achieving excellent performance in image recognition. To this end, we build an attention-free network called sMLPNet based on the existing MLP-based vision models. Specifically, we replace the MLP module in the token-mixing step with a novel sparse MLP (sMLP) module. For 2D image tokens, sMLP applies 1D MLP along the axial directions and the parameters are shared among rows or columns. By sparse connection and weight sharing, sMLP module significantly reduces the number of model parameters and computational complexity, avoiding the common over-fitting problem that plagues the performance of MLP-like models. When only trained on the ImageNet-1K dataset, the proposed sMLPNet achieves 81.9\% top-1 accuracy with only 24M parameters, which is much better than most CNNs and vision Transformers under the same model size constraint. When scaling up to 66M parameters, sMLPNet achieves 83.4\% top-1 accuracy, which is on par with the state-of-the-art Swin Transformer. The success of sMLPNet suggests that the self-attention mechanism is not necessarily a silver bullet in computer vision. The code and models are publicly available at https://github.com/microsoft/SPACH},
  version = {2},
  file = {/Users/lukakuma/Zotero/storage/AR58XMYG/Tang et al. - 2022 - Sparse MLP for Image Recognition Is Self-Attentio.pdf;/Users/lukakuma/Zotero/storage/AQMLYTES/2109.html}
}

@article{taylorGalacticaLargeLanguage2022,
  title = {Galactica: {{A Large Language Model}} for {{Science}}},
  author = {Taylor, Ross and Scialom, Thomas and Poulton, Andrew and Kardas, Marcin and Hartshorn, Anthony and Kerkez, Viktor and Cucurull, Guillem and Saravia, Elvis and Stojnic, Robert},
  date = {2022-11-16},
  pages = {58},
  abstract = {Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community1.},
  langid = {english},
  keywords = {5-reasoning,read},
  file = {/Users/lukakuma/Zotero/storage/E4F9VCFV/Taylor et al. - Galactica A Large Language Model for Science.pdf}
}

@online{tayScaleEfficientlyInsights2022,
  title = {Scale {{Efficiently}}: {{Insights}} from {{Pre-training}} and {{Fine-tuning Transformers}}},
  shorttitle = {Scale {{Efficiently}}},
  author = {Tay, Yi and Dehghani, Mostafa and Rao, Jinfeng and Fedus, William and Abnar, Samira and Chung, Hyung Won and Narang, Sharan and Yogatama, Dani and Vaswani, Ashish and Metzler, Donald},
  date = {2022-01-30},
  number = {arXiv:2109.10686},
  eprint = {arXiv:2109.10686},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.10686},
  urldate = {2022-11-02},
  abstract = {There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. (2020) presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\% fewer parameters and training 40\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis.},
  langid = {english},
  pubstate = {preprint},
  keywords = {finetuning},
  file = {/Users/lukakuma/Zotero/storage/T64MBAZF/Tay et al. - 2022 - Scale Efficiently Insights from Pre-training and .pdf}
}

@online{tayScalingLawsVs2022,
  title = {Scaling {{Laws}} vs {{Model Architectures}}: {{How}} Does {{Inductive Bias Influence Scaling}}?},
  shorttitle = {Scaling {{Laws}} vs {{Model Architectures}}},
  author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Chung, Hyung Won and Fedus, William and Rao, Jinfeng and Narang, Sharan and Tran, Vinh Q. and Yogatama, Dani and Metzler, Donald},
  date = {2022-07-21},
  number = {arXiv:2207.10551},
  eprint = {arXiv:2207.10551},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.10551},
  urldate = {2022-07-30},
  abstract = {There have been a lot of interest in the scaling properties of Transformer models. However, not much has been done on the front of investigating the effect of scaling properties of different inductive biases and model architectures. Do model architectures scale differently? If so, how does inductive bias affect scaling behaviour? How does this influence upstream (pretraining) and downstream (transfer)? This paper conducts a systematic study of scaling behaviour of ten diverse model architectures such as Transformers, Switch Transformers, Universal Transformers, Dynamic convolutions, Performers, and recently proposed MLP-Mixers. Via extensive experiments, we show that (1) architecture is an indeed an important consideration when performing scaling and (2) the best performing model can fluctuate at different scales. We believe that the findings outlined in this work has significant implications to how model architectures are currently evaluated in the community.},
  pubstate = {preprint},
  keywords = {scaling law},
  file = {/Users/lukakuma/Zotero/storage/XFZJ2YAF/Tay et al. - 2022 - Scaling Laws vs Model Architectures How does Indu.pdf;/Users/lukakuma/Zotero/storage/K4Z9QBZC/2207.html}
}

@online{tayTranscendingScalingLaws2022,
  title = {Transcending {{Scaling Laws}} with 0.1\% {{Extra Compute}}},
  author = {Tay, Yi and Wei, Jason and Chung, Hyung Won and Tran, Vinh Q. and So, David R. and Shakeri, Siamak and Garcia, Xavier and Zheng, Huaixiu Steven and Rao, Jinfeng and Chowdhery, Aakanksha and Zhou, Denny and Metzler, Donald and Petrov, Slav and Houlsby, Neil and Le, Quoc V. and Dehghani, Mostafa},
  date = {2022-10-20},
  number = {arXiv:2210.11399},
  eprint = {arXiv:2210.11399},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.11399},
  urldate = {2022-10-24},
  abstract = {Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving \$\textbackslash sim\$4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.},
  pubstate = {preprint},
  keywords = {compute efficient method,finetuning,read,scaling law},
  file = {/Users/lukakuma/Zotero/storage/9F5FJC8G/Tay et al. - 2022 - Transcending Scaling Laws with 0.1% Extra Compute.pdf;/Users/lukakuma/Zotero/storage/4A42TZPZ/2210.html}
}

@online{tayTransformerMemoryDifferentiable2022,
  title = {Transformer {{Memory}} as a {{Differentiable Search Index}}},
  author = {Tay, Yi and Tran, Vinh Q. and Dehghani, Mostafa and Ni, Jianmo and Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe and Gupta, Jai and Schuster, Tal and Cohen, William W. and Metzler, Donald},
  date = {2022-10-21},
  number = {arXiv:2202.06991},
  eprint = {arXiv:2202.06991},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.06991},
  urldate = {2022-12-03},
  abstract = {In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.},
  pubstate = {preprint},
  keywords = {memory},
  file = {/Users/lukakuma/Zotero/storage/XWDRIIPE/Tay et al. - 2022 - Transformer Memory as a Differentiable Search Inde.pdf;/Users/lukakuma/Zotero/storage/EUWTR2US/2202.html}
}

@online{tayUL2UnifyingLanguage2022,
  title = {{{UL2}}: {{Unifying Language Learning Paradigms}}},
  shorttitle = {{{UL2}}},
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
  date = {2022-10-08},
  number = {arXiv:2205.05131},
  eprint = {arXiv:2205.05131},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.05131},
  urldate = {2022-10-17},
  abstract = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. Finally, we show that UL2 20B works well with chain-of-thought prompting and reasoning. We release Flax-based T5X model checkpoints for the 20B model at \textbackslash url\{https://github.com/google-research/google-research/tree/master/ul2\}.},
  pubstate = {preprint},
  keywords = {objective fn,read},
  file = {/Users/lukakuma/Zotero/storage/KNZRBYDF/Tay et al. - 2022 - UL2 Unifying Language Learning Paradigms.pdf;/Users/lukakuma/Zotero/storage/M4A9FUIY/2205.05131.pdf;/Users/lukakuma/Zotero/storage/77E2LEAV/2205.html}
}

@unpublished{tayUnifyingLanguageLearning2022,
  title = {Unifying {{Language Learning Paradigms}}},
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  date = {2022-05-10},
  number = {arXiv:2205.05131},
  eprint = {2205.05131},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.05131},
  urldate = {2022-05-18},
  abstract = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based T5X model checkpoints for the 20B model at \textbackslash url\{https://github.com/google-research/google-research/tree/master/ul2\}.},
  file = {/Users/lukakuma/Zotero/storage/M4CNA8ZG/Tay et al. - 2022 - Unifying Language Learning Paradigms.pdf;/Users/lukakuma/Zotero/storage/A6TZPS8E/2205.html}
}

@article{teamNoLanguageLeft2022,
  title = {No {{Language Left Behind}}: {{Scaling Human-Centered Machine Translation}}},
  author = {family=Team, given=NLLB, given-i=NLLB and Costa-juss\`a, Marta R and Cross, James and \c{C}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and Sun, Anna and Wang, Skyler and Wenzek, Guillaume and Youngblood, Al and Akula, Bapi and Barrault, Loic and Gonzalez, Gabriel Mejia and Hansanti, Prangthip and Hoffman, John and Jarrett, Semarley and Sadagopan, Kaushik Ram and Rowe, Dirk and Spruit, Shannon and Tran, Chau and Andrews, Pierre and Ayan, Necip Fazil and Bhosale, Shruti and Edunov, Sergey and Fan, Angela and Gao, Cynthia and Goswami, Vedanuj and Guzm\'an, Francisco and Koehn, Philipp and Mourachko, Alexandre and Ropers, Christophe and Saleem, Safiyyah and Schwenk, Holger and Wang, Jeff and Ai, Meta},
  date = {2022-07-06},
  pages = {190},
  abstract = {Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44\% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/KZFK7IWZ/Team et al. - No Language Left Behind Scaling Human-Centered Ma.pdf}
}

@online{TechnologyBLOOMTraining2022,
  title = {The {{Technology Behind BLOOM Training}}},
  date = {2022-06-14},
  url = {https://huggingface.co/blog/bloom-megatron-deepspeed},
  urldate = {2022-09-06},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/Users/lukakuma/Zotero/storage/5XS3B2FM/bloom-megatron-deepspeed.html}
}

@inproceedings{teehanEmergentStructuresTraining2022,
  title = {Emergent {{Structures}} and {{Training Dynamics}} in {{Large Language Models}}},
  author = {Teehan, Ryan and Clinciu, Miruna and Serikov, Oleg and Szczechla, Eliza and Seelam, Natasha and Mirkin, Shachar and Gokaslan, Aaron},
  date = {2022-03-09},
  url = {https://openreview.net/forum?id=SbgL3zrIWc},
  urldate = {2022-05-24},
  abstract = {Large language models have achieved success on a number of downstream tasks, particularly in a few and zero-shot manner. As a consequence, researchers have been investigating both the kind of...},
  eventtitle = {Challenges \{\textbackslash\&\vphantom\}},
  langid = {english},
  keywords = {emergence},
  file = {/Users/lukakuma/Zotero/storage/PUAYTV8A/Teehan et al. - 2022 - Emergent Structures and Training Dynamics in Large.pdf;/Users/lukakuma/Zotero/storage/2PB539VQ/forum.html}
}

@article{tenenbaumHowGrowMind2011,
  title = {How to {{Grow}} a {{Mind}}: {{Statistics}}, {{Structure}}, and {{Abstraction}}},
  shorttitle = {How to {{Grow}} a {{Mind}}},
  author = {Tenenbaum, Joshua B. and Kemp, Charles and Griffiths, Thomas L. and Goodman, Noah D.},
  date = {2011-03-11},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {331},
  number = {6022},
  pages = {1279--1285},
  issn = {0036-8075, 1095-9203},
  url = {https://www.science.org/doi/10.1126/science.1192788},
  urldate = {2022-07-26},
  abstract = {In coming to understand the world\textemdash in learning concepts, acquiring language, and grasping causal relations\textemdash our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/PFXZIW5A/Tenenbaum et al. - 2011 - How to Grow a Mind Statistics, Structure, and Abs.pdf}
}

@video{therobotbrainspodcastSeasonEp222022,
  title = {Season 2 {{Ep}} 22 {{Geoff Hinton}} on Revolutionizing Artificial Intelligence... Again},
  editor = {{The Robot Brains Podcast}},
  date = {2022-06-01},
  url = {https://www.youtube.com/watch?v=2EDP4v-9TUA},
  urldate = {2022-06-14},
  abstract = {Over the past ten years, AI has experienced breakthrough after breakthrough in everything from computer vision to speech recognition, protein folding prediction, and so much more. Many of these advancements hinge on the deep learning work conducted by our guest, Geoff Hinton, who has fundamentally changed the focus and direction of the field. A recipient of the Turing Award, the equivalent of the Nobel prize for computer science, he has over half a million citations of his work.  Hinton has spent about half a century on deep learning, most of the time researching in relative obscurity. But that all changed in 2012 when Hinton and his students showed deep learning is better at image recognition than any other approaches to computer vision, and by a very large margin. That result, that moment, known as the ImageNet moment, changed the whole AI field. Pretty much everyone dropped what they had been doing and switched to deep learning. Geoff joins Pieter in our two-part season finale for a wide-ranging discussion inspired by insights gleaned from Hinton's journey from academia to Google Brain. The episode covers how existing neural networks and backpropagation models operate differently than how the brain actually works; the purpose of sleep; and why it's better to grow our computers than manufacture them. What's in this episode: 00:00:00 - Introduction 00:02:48 - Understanding how the brain works 00:06:59 - Why we need unsupervised local objective functions  00:09:39 - Masked auto-encoders  00:10:55 - Current methods in end to end learning 00:18:36 - Spiking neural networks 00:23:00 - Leveraging spike times  00:29:55 - The story behind AlexNet 00:36:15 - Transition from pure academia to Google 00:40:23 - The secret auction of Hinton's company at NuerIPS 00:44:18 - Hinton's start in psychology and carpentry 00:54:34 - Why computers should be grown rather than manufactured 01:06:57 - The function of sleep and Boltzmann Machines  01:11:49 - Need for negative data  01:19:35 - Visualizing data using t-SNE Links:  Geoff's Bio: https://en.wikipedia.org/wiki/Geoffre... Geoff's Twitter: https://twitter.com/geoffreyhinton?la... Research and Publications: https://bit.ly/3z3M54e Google Scholar Citations: https://bit.ly/3N892HJ Story Behind the 2012 NIPS Auction: https://bit.ly/3t9xsIN GLOM: https://bit.ly/3lYgWr6 Vector Institute: https://vectorinstitute.ai/ SUBSCRIBE TODAY:  Apple: https://apple.co/3NLtQED Spotify: https://spoti.fi/3GBDpDM Amazon: https://amzn.to/3NHlQoa Google: https://bit.ly/3aD7ZkN Acast: https://bit.ly/3x6ZYfw Host: Pieter Abbeel Executive Producers: Alice Patel \& Henry Tobias Jones  Production: Fresh Air Production},
  editortype = {director}
}

@video{therobotbrainspodcastSeasonEp232022,
  title = {Season 2 {{Ep}} 23 {{Twitter Q}}\&{{A}} with {{Geoff Hinton}}},
  editor = {{The Robot Brains Podcast}},
  date = {2022-06-08},
  url = {https://www.youtube.com/watch?v=4Otcau-C_Yc},
  urldate = {2022-06-14},
  abstract = {Last week, we were honored to have Professor Geoff Hinton join the show for a wide-ranging discussion inspired by insights gleaned from Geoff's journey in academia, as well as past 10 years with Google Brain. The episode covers how existing neural networks and backpropagation models operate differently than how the brain actually works; the ImageNet/AlexNet breakthrough moment; the purpose of sleep; and why it's better to grow our computers than manufacture them. As you might recall, we also gave our audience an opportunity to contribute questions for Geoff via Twitter. We received so many amazing questions from our audience that we had to break down our time with Geoff into two parts! In this episode, we'll discuss some of these questions with Geoff. Tune in to get Geoff's answers to the following questions AND MORE:  Are you concerned with AI becoming too successful?  What is the connection between mania and genius?  What childhood experiences shaped him the most?  What is next in AI?  What should PhD students focus on?  How conscious do you think today's neural nets are? How important is embodiment for intelligence?  How does the brain work?  Links:  Geoff's Bio: https://en.wikipedia.org/wiki/Geoffre... Geoff's Twitter: https://twitter.com/geoffreyhinton?la... Research and Publications: https://bit.ly/3z3M54e Google Scholar Citations: https://bit.ly/3N892HJ Story Behind the 2012 NIPS Auction: https://bit.ly/3t9xsIN GLOM: https://bit.ly/3lYgWr6 Vector Institute: https://vectorinstitute.ai/ SUBSCRIBE TODAY:  Apple: https://apple.co/3xhtNKa Spotify: https://spoti.fi/3NMANGd Google: https://bit.ly/3azFyE8 Amazon: https://amzn.to/3aF9EG8 Acast: https://bit.ly/390bNMi Host: Pieter Abbeel Executive Producers: Alice Patel \& Henry Tobias Jones  Production: Fresh Air Production},
  editortype = {director}
}

@article{thomsonFunctionalMapsNeocortical2007,
  title = {Functional Maps of Neocortical Local Circuitry},
  author = {Thomson, Alex and Lamy, Christophe},
  date = {2007},
  journaltitle = {Frontiers in Neuroscience},
  volume = {1},
  issn = {1662-453X},
  url = {https://www.frontiersin.org/article/10.3389/neuro.01.1.1.002.2007},
  urldate = {2022-06-18},
  keywords = {(ext) J.Hawkins},
  file = {/Users/lukakuma/Zotero/storage/MXX2FDY7/Thomson and Lamy - 2007 - Functional maps of neocortical local circuitry.pdf}
}

@online{thoppilanLaMDALanguageModels2022a,
  title = {{{LaMDA}}: {{Language Models}} for {{Dialog Applications}}},
  shorttitle = {{{LaMDA}}},
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  date = {2022-02-10},
  number = {arXiv:2201.08239},
  eprint = {arXiv:2201.08239},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.08239},
  urldate = {2022-11-17},
  abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
  pubstate = {preprint},
  keywords = {*general tools,8-chat,multiagent,read},
  file = {/Users/lukakuma/Zotero/storage/76WZKCWD/Thoppilan et al. - 2022 - LaMDA Language Models for Dialog Applications.pdf;/Users/lukakuma/Zotero/storage/7TFMUHZM/2201.html}
}

@online{tianAmosAdamstyleOptimizer2022,
  title = {Amos: {{An Adam-style Optimizer}} with {{Adaptive Weight Decay}} towards {{Model-Oriented Scale}}},
  shorttitle = {Amos},
  author = {Tian, Ran and Parikh, Ankur P.},
  date = {2022-10-20},
  number = {arXiv:2210.11693},
  eprint = {arXiv:2210.11693},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.11693},
  urldate = {2022-10-27},
  abstract = {We present Amos, a stochastic gradient-based optimizer designed for training deep neural networks. It can be viewed as an Adam optimizer with theoretically supported, adaptive learning-rate decay and weight decay. A key insight behind Amos is that it leverages model-specific information to determine the initial learning-rate and decaying schedules. When used for pre-training BERT variants and T5, Amos consistently converges faster than the state-of-the-art settings of AdamW, achieving better validation loss within {$<$}=70\% training steps and time, while requiring {$<$}=51\% memory for slot variables. Our code is open-sourced at: https://github.com/google-research/jestimator},
  pubstate = {preprint},
  keywords = {optimizer},
  file = {/Users/lukakuma/Zotero/storage/Y232DGXP/Tian and Parikh - 2022 - Amos An Adam-style Optimizer with Adaptive Weight.pdf;/Users/lukakuma/Zotero/storage/K36ABMDW/2210.html}
}

@unpublished{tolstikhinMLPMixerAllMLPArchitecture2021,
  title = {{{MLP-Mixer}}: {{An}} All-{{MLP Architecture}} for {{Vision}}},
  shorttitle = {{{MLP-Mixer}}},
  author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  date = {2021-06-11},
  number = {arXiv:2105.01601},
  eprint = {2105.01601},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.01601},
  urldate = {2022-06-09},
  abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/U6XHXVN9/Tolstikhin et al. - 2021 - MLP-Mixer An all-MLP Architecture for Vision.pdf;/Users/lukakuma/Zotero/storage/NUXSLVKL/2105.html}
}

@inproceedings{toppingUnderstandingOversquashingBottlenecks2021,
  title = {Understanding Over-Squashing and Bottlenecks on Graphs via Curvature},
  author = {Topping, Jake and Giovanni, Francesco Di and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=7UmjRGzp-A},
  urldate = {2022-05-15},
  abstract = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/TMPG7USW/Topping et al. - 2021 - Understanding over-squashing and bottlenecks on gr.pdf;/Users/lukakuma/Zotero/storage/248CZJJF/forum.html}
}

@article{torresWhyHowWhen2021,
  title = {The {{Why}}, {{How}}, and {{When}} of {{Representations}} for {{Complex Systems}}},
  author = {Torres, Leo and Blevins, Ann S. and Bassett, Danielle and Eliassi-Rad, Tina},
  date = {2021-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {63},
  number = {3},
  pages = {435--485},
  issn = {0036-1445},
  url = {https://epubs.siam.org/doi/10.1137/20M1355896},
  urldate = {2022-05-18},
  abstract = {Complex systems, composed at the most basic level of units and their interactions, describe phenomena in a wide variety of domains, from neuroscience to computer science and economics. The wide variety of applications has resulted in two key challenges: the generation of many domain-specific strategies for complex systems analyses that are seldom revisited, and the compartmentalization of representation and analysis ideas within a domain due to inconsistency in complex systems language. In this work we propose basic, domain-agnostic language in order to advance toward a more cohesive vocabulary. We use this language to evaluate each step of the complex systems analysis pipeline, beginning with the system under study and data collected, then moving through different mathematical frameworks for encoding the observed data (i.e., graphs, simplicial complexes, and hypergraphs), and relevant computational methods for each framework. At each step we consider different types of dependencies; these are properties of the system that describe how the existence of an interaction among a set of units in a system may affect the possibility of the existence of another relation. We discuss how dependencies may arise and how they may alter the interpretation of results or the entirety of the analysis pipeline. We close with two real-world examples using coauthorship data and email communications data that illustrate how the system under study, the dependencies therein, the research question, and the choice of mathematical representation influence the results. We hope this work can serve as an opportunity for reflection for experienced complex systems scientists, as well as an introductory resource for new researchers.},
  file = {/Users/lukakuma/Zotero/storage/PKU6QZMM/Torres et al. - 2021 - The Why, How, and When of Representations for Complex Systems.pdf;/Users/lukakuma/Zotero/storage/ZCK4TCP5/Torres et al. - 2020 - The why, how, and when of representations for comp.pdf;/Users/lukakuma/Zotero/storage/RWWDRVJC/2006.html}
}

@article{touvronLLaMAOpenEfficient,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothee and Rozi\`ere, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  date = {2023-02-25},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla70B and PaLM-540B. We release all our models to the research community1.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/PWBQ6JJN/Touvron et al. - LLaMA Open and Efficient Foundation Language Mode.pdf}
}

@online{touvronTrainingDataefficientImage2021,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J\'egou, Herv\'e},
  date = {2021-01-15},
  number = {arXiv:2012.12877},
  eprint = {arXiv:2012.12877},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.12877},
  urldate = {2022-08-10},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/D8TXIAWE/Touvron et al. - 2021 - Training data-efficient image transformers & disti.pdf;/Users/lukakuma/Zotero/storage/4457GVVG/2012.html}
}

@online{toyamaAndroidEnvReinforcementLearning2021,
  title = {{{AndroidEnv}}: {{A Reinforcement Learning Platform}} for {{Android}}},
  shorttitle = {{{AndroidEnv}}},
  author = {Toyama, Daniel and Hamel, Philippe and Gergely, Anita and Comanici, Gheorghe and Glaese, Amelia and Ahmed, Zafarali and Jackson, Tyler and Mourad, Shibl and Precup, Doina},
  date = {2021-05-27},
  number = {arXiv:2105.13231},
  eprint = {arXiv:2105.13231},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.13231},
  urldate = {2023-02-20},
  abstract = {We introduce AndroidEnv, an open-source platform for Reinforcement Learning (RL) research built on top of the Android ecosystem. AndroidEnv allows RL agents to interact with a wide variety of apps and services commonly used by humans through a universal touchscreen interface. Since agents train on a realistic simulation of an Android device, they have the potential to be deployed on real devices. In this report, we give an overview of the environment, highlighting the significant features it provides for research, and we present an empirical evaluation of some popular reinforcement learning agents on a set of tasks built on this platform.},
  pubstate = {preprint},
  keywords = {*general tools},
  file = {/Users/lukakuma/Zotero/storage/B6S6RTN3/Toyama et al. - 2021 - AndroidEnv A Reinforcement Learning Platform for .pdf;/Users/lukakuma/Zotero/storage/LRE7FGXC/2105.html}
}

@online{TransformersAreGraph2020,
  title = {Transformers Are {{Graph Neural Networks}}},
  date = {2020-09-12T15:13:31},
  url = {https://thegradient.pub/transformers-are-graph-neural-networks/},
  urldate = {2022-03-28},
  abstract = {My engineering friends often ask me: deep learning on graphs sounds great, but are there any real applications?  While Graph Neural Networks are used in recommendation systems at Pinterest, Alibaba and Twitter, a more subtle success story is the Transformer architecture, which has taken the NLP world by storm. Through},
  langid = {english},
  organization = {{The Gradient}},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/LD8ES65Y/transformers-are-graph-neural-networks.html}
}

@online{trivediInterleavingRetrievalChainofThought2022,
  title = {Interleaving {{Retrieval}} with {{Chain-of-Thought Reasoning}} for {{Knowledge-Intensive Multi-Step Questions}}},
  author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  date = {2022-12-20},
  number = {arXiv:2212.10509},
  eprint = {arXiv:2212.10509},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.10509},
  urldate = {2023-02-20},
  abstract = {Recent work has shown that large language models are capable of generating natural language reasoning steps or Chains-of-Thoughts (CoT) to answer a multi-step question when prompted to do so. This is insufficient, however, when the necessary knowledge is not available or up-to-date within a model's parameters. A straightforward approach to address this is to retrieve text from an external knowledge source using the question as a query and prepend it as context to the model's input. This, however, is also insufficient for multi-step QA where \textbackslash textit\{what to retrieve\} depends on \textbackslash textit\{what has already been derived\}. To address this issue we propose IRCoT, a new approach that interleaves retrieval with CoT for multi-step QA, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Our experiments with GPT3 show substantial improvements in retrieval (up to 22 points) and downstream QA (up to 16 points) over the baselines on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. Notably, our method also works well for much smaller models such as T5-Flan-large (0.7B) without any additional training.},
  pubstate = {preprint},
  keywords = {4-document retrieval},
  file = {/Users/lukakuma/Zotero/storage/7MNY8CMD/Trivedi et al. - 2022 - Interleaving Retrieval with Chain-of-Thought Reaso.pdf;/Users/lukakuma/Zotero/storage/H93GPXYJ/2212.html}
}

@unpublished{trottBuildingFoundationDataDriven2021,
  title = {Building a {{Foundation}} for {{Data-Driven}}, {{Interpretable}}, and {{Robust Policy Design}} Using the {{AI Economist}}},
  author = {Trott, Alexander and Srinivasa, Sunil and family=Wal, given=Douwe, prefix=van der, useprefix=true and Haneuse, Sebastien and Zheng, Stephan},
  date = {2021-08-05},
  eprint = {2108.02904},
  eprinttype = {arxiv},
  eprintclass = {cs, econ, q-fin},
  url = {http://arxiv.org/abs/2108.02904},
  urldate = {2022-01-08},
  abstract = {Optimizing economic and public policy is critical to address socioeconomic issues and trade-offs, e.g., improving equality, productivity, or wellness, and poses a complex mechanism design problem. A policy designer needs to consider multiple objectives, policy levers, and behavioral responses from strategic actors who optimize for their individual objectives. Moreover, real-world policies should be explainable and robust to simulation-to-reality gaps, e.g., due to calibration issues. Existing approaches are often limited to a narrow set of policy levers or objectives that are hard to measure, do not yield explicit optimal policies, or do not consider strategic behavior, for example. Hence, it remains challenging to optimize policy in real-world scenarios. Here we show that the AI Economist framework enables effective, flexible, and interpretable policy design using two-level reinforcement learning (RL) and data-driven simulations. We validate our framework on optimizing the stringency of US state policies and Federal subsidies during a pandemic, e.g., COVID-19, using a simulation fitted to real data. We find that log-linear policies trained using RL significantly improve social welfare, based on both public health and economic outcomes, compared to past outcomes. Their behavior can be explained, e.g., well-performing policies respond strongly to changes in recovery and vaccination rates. They are also robust to calibration errors, e.g., infection rates that are over or underestimated. As of yet, real-world policymaking has not seen adoption of machine learning methods at large, including RL and AI-driven simulations. Our results show the potential of AI to guide policy design and improve social welfare amidst the complexity of the real world.},
  keywords = {policty design},
  file = {/Users/lukakuma/Zotero/storage/JEQCI8EQ/Trott et al. - 2021 - Building a Foundation for Data-Driven, Interpretab.pdf;/Users/lukakuma/Zotero/storage/3PURYP7S/2108.html}
}

@online{truongRethinkingSim2RealLower2022,
  title = {Rethinking {{Sim2Real}}: {{Lower Fidelity Simulation Leads}} to {{Higher Sim2Real Transfer}} in {{Navigation}}},
  shorttitle = {Rethinking {{Sim2Real}}},
  author = {Truong, Joanne and Rudolph, Max and Yokoyama, Naoki and Chernova, Sonia and Batra, Dhruv and Rai, Akshara},
  date = {2022-07-21},
  number = {arXiv:2207.10821},
  eprint = {arXiv:2207.10821},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.10821},
  urldate = {2022-08-10},
  abstract = {If we want to train robots in simulation before deploying them in reality, it seems natural and almost self-evident to presume that reducing the sim2real gap involves creating simulators of increasing fidelity (since reality is what it is). We challenge this assumption and present a contrary hypothesis -- sim2real transfer of robots may be improved with lower (not higher) fidelity simulation. We conduct a systematic large-scale evaluation of this hypothesis on the problem of visual navigation -- in the real world, and on 2 different simulators (Habitat and iGibson) using 3 different robots (A1, AlienGo, Spot). Our results show that, contrary to expectation, adding fidelity does not help with learning; performance is poor due to slow simulation speed (preventing large-scale learning) and overfitting to inaccuracies in simulation physics. Instead, building simple models of the robot motion using real-world data can improve learning and generalization.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/FLUKL755/Truong et al. - 2022 - Rethinking Sim2Real Lower Fidelity Simulation Lea.pdf;/Users/lukakuma/Zotero/storage/QCQRMSA2/2207.html}
}

@online{tschannenImageandLanguageUnderstandingPixels2022,
  title = {Image-and-{{Language Understanding}} from {{Pixels Only}}},
  author = {Tschannen, Michael and Mustafa, Basil and Houlsby, Neil},
  date = {2022-12-15},
  number = {arXiv:2212.08045},
  eprint = {arXiv:2212.08045},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.08045},
  urldate = {2022-12-18},
  abstract = {Multimodal models are becoming increasingly effective, in part due to unified components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-specific pieces and training procedures. For example, CLIP (Radford et al., 2021) trains independent text and image towers via a contrastive loss. We explore an additional unification: the use of a pure pixel-based model to perform image, text, and multimodal tasks. Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as retrieval and zero-shot image classification almost as well as CLIP, with half the number of parameters and no text-specific tower or embedding. When trained jointly via image-text contrastive learning and next-sentence contrastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Finally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/ZIX7H4KU/Tschannen et al. - 2022 - Image-and-Language Understanding from Pixels Only.pdf;/Users/lukakuma/Zotero/storage/KYEWINNW/2212.html}
}

@online{tsimpoukelliMultimodalFewShotLearning2021,
  title = {Multimodal {{Few-Shot Learning}} with {{Frozen Language Models}}},
  author = {Tsimpoukelli, Maria and Menick, Jacob and Cabi, Serkan and Eslami, S. M. Ali and Vinyals, Oriol and Hill, Felix},
  date = {2021-07-03},
  number = {arXiv:2106.13884},
  eprint = {arXiv:2106.13884},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.13884},
  urldate = {2023-02-15},
  abstract = {When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model prompted with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of multiple interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks.},
  pubstate = {preprint},
  keywords = {2-adapter,7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/XSNY2W3N/Tsimpoukelli et al. - 2021 - Multimodal Few-Shot Learning with Frozen Language .pdf;/Users/lukakuma/Zotero/storage/5AYI4DVY/2106.html}
}

@article{tunyasuvunakoolHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction for the Human Proteome},
  author = {Tunyasuvunakool, Kathryn and Adler, Jonas and Wu, Zachary and Green, Tim and Zielinski, Michal and \v{Z}\'idek, Augustin and Bridgland, Alex and Cowie, Andrew and Meyer, Clemens and Laydon, Agata and Velankar, Sameer and Kleywegt, Gerard J. and Bateman, Alex and Evans, Richard and Pritzel, Alexander and Figurnov, Michael and Ronneberger, Olaf and Bates, Russ and Kohl, Simon A. A. and Potapenko, Anna and Ballard, Andrew J. and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Clancy, Ellen and Reiman, David and Petersen, Stig and Senior, Andrew W. and Kavukcuoglu, Koray and Birney, Ewan and Kohli, Pushmeet and Jumper, John and Hassabis, Demis},
  date = {2021-08},
  journaltitle = {Nature},
  volume = {596},
  number = {7873},
  pages = {590--596},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/s41586-021-03828-1},
  urldate = {2022-05-12},
  abstract = {Protein structures can provide invaluable information, both for reasoning about biological processes and for enabling interventions such as structure-based drug development or targeted mutagenesis. After decades of effort, 17\% of the total residues in human protein sequences are covered by an experimentally determined structure1. Here we markedly expand the structural coverage of the proteome by applying the state-of-the-art machine learning method, AlphaFold2, at a scale that covers almost the entire human proteome (98.5\% of human proteins). The resulting dataset covers 58\% of residues with a confident prediction, of which a subset (36\% of all residues) have very high confidence. We introduce several metrics developed by building on the AlphaFold model and use them to interpret the dataset, identifying strong multi-domain predictions as well as regions that are likely to be disordered. Finally, we provide some case studies to illustrate how high-quality predictions could be used to generate biological hypotheses. We are making our predictions freely available to the community and anticipate that routine large-scale and high-accuracy structure prediction will become an important tool~that will allow new questions to be addressed from a structural perspective.},
  issue = {7873},
  langid = {english},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/CCAFFIRB/Tunyasuvunakool et al. - 2021 - Highly accurate protein structure prediction for t.pdf;/Users/lukakuma/Zotero/storage/SC85WP45/s41586-021-03828-1.html}
}

@article{uc-cetinaSurveyReinforcementLearning2022,
  title = {Survey on Reinforcement Learning for Language Processing},
  author = {Uc-Cetina, Victor and Navarro-Guerrero, Nicolas and Martin-Gonzalez, Anabel and Weber, Cornelius and Wermter, Stefan},
  date = {2022-06-03},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  eprint = {2104.05565},
  eprinttype = {arxiv},
  eprintclass = {cs},
  issn = {0269-2821, 1573-7462},
  url = {http://arxiv.org/abs/2104.05565},
  urldate = {2022-06-09},
  abstract = {In recent years some researchers have explored the use of reinforcement learning (RL) algorithms as key components in the solution of various natural language processing tasks. For instance, some of these algorithms leveraging deep neural learning have found their way into conversational systems. This paper reviews the state of the art of RL methods for their possible use for different problems of natural language processing, focusing primarily on conversational systems, mainly due to their growing relevance. We provide detailed descriptions of the problems as well as discussions of why RL is well-suited to solve them. Also, we analyze the advantages and limitations of these methods. Finally, we elaborate on promising research directions in natural language processing that might benefit from reinforcement learning.},
  file = {/Users/lukakuma/Zotero/storage/HWYDG6DC/Uc-Cetina et al. - 2022 - Survey on reinforcement learning for language proc.pdf;/Users/lukakuma/Zotero/storage/VBJ7QT6R/2104.html}
}

@online{UnderstandLanguageUnderstand2021,
  title = {To {{Understand Language}} Is to {{Understand Generalization}}},
  date = {2021-12-17T00:00:00+00:00},
  url = {https://evjang.com/2021/12/17/lang-generalization.html},
  urldate = {2022-07-21},
  abstract = {In my essay ``Just ask for Generalization'', I argued that some optimization capabilities, such as reinforcement learning from sub-optimal trajectories, might be better implemented by generalization than by construction. We have to generalize to unseen situations at deployment time anyway, so why not focus on generalization capability as the first class citizen, and then ``just ask for optimality'' as an unseen case? A corollary to this design philosophy is that we should discard inductive biases that introduce optimization bottlenecks for ``the data sponge'': if an inductive bias turns out to be merely ``data in disguise'', it may not only cease to provide a benefit in the high data regime, but actually hinder the model on examples where the inductive bias no longer applies.},
  langid = {english},
  organization = {{Eric Jang}},
  file = {/Users/lukakuma/Zotero/storage/DLWQL6WS/lang-generalization.html}
}

@online{ungSaFeRDialoguesTakingFeedback2022,
  title = {{{SaFeRDialogues}}: {{Taking Feedback Gracefully}} after {{Conversational Safety Failures}}},
  shorttitle = {{{SaFeRDialogues}}},
  author = {Ung, Megan and Xu, Jing and Boureau, Y.-Lan},
  date = {2022-05-04},
  number = {arXiv:2110.07518},
  eprint = {arXiv:2110.07518},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.07518},
  urldate = {2022-11-28},
  abstract = {Current open-domain conversational models can easily be made to talk in inadequate ways. Online learning from conversational feedback given by the conversation partner is a promising avenue for a model to improve and adapt, so as to generate fewer of these safety failures. However, current state-of-the-art models tend to react to feedback with defensive or oblivious responses. This makes for an unpleasant experience and may discourage conversation partners from giving feedback in the future. This work proposes SaFeRDialogues, a task and dataset of graceful responses to conversational feedback about safety failures. We collect a dataset of 10k dialogues demonstrating safety failures, feedback signaling them, and a response acknowledging the feedback. We show how fine-tuning on this dataset results in conversations that human raters deem considerably more likely to lead to a civil conversation, without sacrificing engagingness or general conversational ability.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/IUQFNI48/Ung et al. - 2022 - SaFeRDialogues Taking Feedback Gracefully after C.pdf;/Users/lukakuma/Zotero/storage/XAQKEPIL/2110.html}
}

@online{ustunHyperXUnifiedHypernetwork2022,
  title = {Hyper-{{X}}: {{A Unified Hypernetwork}} for {{Multi-Task Multilingual Transfer}}},
  shorttitle = {Hyper-{{X}}},
  author = {\"Ust\"un, Ahmet and Bisazza, Arianna and Bouma, Gosse and family=Noord, given=Gertjan, prefix=van, useprefix=true and Ruder, Sebastian},
  date = {2022-10-25},
  number = {arXiv:2205.12148},
  eprint = {arXiv:2205.12148},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.12148},
  urldate = {2022-12-17},
  abstract = {Massively multilingual models are promising for transfer learning across tasks and languages. However, existing methods are unable to fully leverage training data when it is available in different task-language combinations. To exploit such heterogeneous supervision, we propose Hyper-X, a single hypernetwork that unifies multi-task and multilingual learning with efficient adaptation. This model generates weights for adapter modules conditioned on both tasks and language embeddings. By learning to combine task and language-specific knowledge, our model enables zero-shot transfer for unseen languages and task-language combinations. Our experiments on a diverse set of languages demonstrate that Hyper-X achieves the best or competitive gain when a mixture of multiple resources is available, while being on par with strong baselines in the standard scenario. Hyper-X is also considerably more efficient in terms of parameters and resources compared to methods that train separate adapters. Finally, Hyper-X consistently produces strong results in few-shot scenarios for new languages, showing the versatility of our approach beyond zero-shot transfer.},
  pubstate = {preprint},
  keywords = {multilingual,read},
  file = {/Users/lukakuma/Zotero/storage/TSEQBFWN/√úst√ºn et al. - 2022 - Hyper-X A Unified Hypernetwork for Multi-Task Mul.pdf;/Users/lukakuma/Zotero/storage/TALS7UUW/2205.html}
}

@online{valmeekamLargeLanguageModels2022,
  title = {Large {{Language Models Still Can}}'t {{Plan}} ({{A Benchmark}} for {{LLMs}} on {{Planning}} and {{Reasoning}} about {{Change}})},
  author = {Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  date = {2022-10-29},
  number = {arXiv:2206.10498},
  eprint = {arXiv:2206.10498},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.10498},
  urldate = {2023-01-05},
  abstract = {Recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a significant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs' reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LLMs on reasoning about actions and change, a central aspect of human intelligence. We provide multiple test cases that are more involved than any of the previously established benchmarks and each test case evaluates a different aspect of reasoning about actions and change. Results on GPT-3 (davinci), Instruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performance on such reasoning tasks.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/TCRYNHEZ/Valmeekam et al. - 2022 - Large Language Models Still Can't Plan (A Benchmar.pdf;/Users/lukakuma/Zotero/storage/CRE5PEDG/2206.html}
}

@unpublished{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-04-20},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/GA2RS543/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/lukakuma/Zotero/storage/AERY6PT4/1706.html}
}

@online{velickovicCLRSAlgorithmicReasoning2022,
  title = {The {{CLRS Algorithmic Reasoning Benchmark}}},
  author = {Veli\v{c}kovi\'c, Petar and Badia, Adri\`a Puigdom\`enech and Budden, David and Pascanu, Razvan and Banino, Andrea and Dashevskiy, Misha and Hadsell, Raia and Blundell, Charles},
  date = {2022-06-04},
  number = {arXiv:2205.15659},
  eprint = {arXiv:2205.15659},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.15659},
  urldate = {2022-11-14},
  abstract = {Learning representations of algorithms is an emerging area of machine learning, seeking to bridge concepts from neural networks with classical algorithms. Several important works have investigated whether neural networks can effectively reason like algorithms, typically by learning to execute them. The common trend in the area, however, is to generate targeted kinds of algorithmic data to evaluate specific hypotheses, making results hard to transfer across publications, and increasing the barrier of entry. To consolidate progress and work towards unified evaluation, we propose the CLRS Algorithmic Reasoning Benchmark, covering classical algorithms from the Introduction to Algorithms textbook. Our benchmark spans a variety of algorithmic reasoning procedures, including sorting, searching, dynamic programming, graph algorithms, string algorithms and geometric algorithms. We perform extensive experiments to demonstrate how several popular algorithmic reasoning baselines perform on these tasks, and consequently, highlight links to several open challenges. Our library is readily available at https://github.com/deepmind/clrs.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/WZ5QP5D2/Veliƒçkoviƒá et al. - 2022 - The CLRS Algorithmic Reasoning Benchmark.pdf;/Users/lukakuma/Zotero/storage/HNH7TGUW/2205.html}
}

@online{velickovicEverythingConnectedGraph2023,
  title = {Everything Is {{Connected}}: {{Graph Neural Networks}}},
  shorttitle = {Everything Is {{Connected}}},
  author = {Veli\v{c}kovi\'c, Petar},
  date = {2023-01-19},
  number = {arXiv:2301.08210},
  eprint = {arXiv:2301.08210},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.08210},
  urldate = {2023-01-24},
  abstract = {In many ways, graphs are the main modality of data we receive from nature. This is due to the fact that most of the patterns we see, both in natural and artificial systems, are elegantly representable using the language of graph structures. Prominent examples include molecules (represented as graphs of atoms and bonds), social networks and transportation networks. This potential has already been seen by key scientific and industrial groups, with already-impacted application areas including traffic forecasting, drug discovery, social network analysis and recommender systems. Further, some of the most successful domains of application for machine learning in previous years -- images, text and speech processing -- can be seen as special cases of graph representation learning, and consequently there has been significant exchange of information between these areas. The main aim of this short survey is to enable the reader to assimilate the key concepts in the area, and position graph representation learning in a proper context with related fields.},
  pubstate = {preprint},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/8RD4CBYF/Veliƒçkoviƒá - 2023 - Everything is Connected Graph Neural Networks.pdf;/Users/lukakuma/Zotero/storage/AM77427A/2301.html}
}

@article{velickovicGraphAttentionNetworks2017,
  title = {Graph {{Attention Networks}}},
  author = {Veli\v{c}kovi\'c, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li\`o, Pietro and Bengio, Yoshua},
  date = {2017-10-30},
  url = {https://arxiv.org/abs/1710.10903v3},
  urldate = {2022-03-30},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  langid = {english},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/9ZDHDQRM/Veliƒçkoviƒá et al. - 2017 - Graph Attention Networks.pdf}
}

@unpublished{velickovicMessagePassingAll2022,
  title = {Message Passing All the Way Up},
  author = {Veli\v{c}kovi\'c, Petar},
  date = {2022-02-22},
  eprint = {2202.11097},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2202.11097},
  urldate = {2022-03-23},
  abstract = {The message passing framework is the foundation of the immense success enjoyed by graph neural networks (GNNs) in recent years. In spite of its elegance, there exist many problems it provably cannot solve over given input graphs. This has led to a surge of research on going "beyond message passing", building GNNs which do not suffer from those limitations -- a term which has become ubiquitous in regular discourse. However, have those methods truly moved beyond message passing? In this position paper, I argue about the dangers of using this term -- especially when teaching graph representation learning to newcomers. I show that any function of interest we want to compute over graphs can, in all likelihood, be expressed using pairwise message passing -- just over a potentially modified graph, and argue how most practical implementations subtly do this kind of trick anyway. Hoping to initiate a productive discussion, I propose replacing "beyond message passing" with a more tame term, "augmented message passing".},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/9RAEQS9Y/Veliƒçkoviƒá - 2022 - Message passing all the way up.pdf;/Users/lukakuma/Zotero/storage/ZYHPVYVI/2202.html}
}

@online{velingkerAffinityAwareGraphNetworks2022,
  title = {Affinity-{{Aware Graph Networks}}},
  author = {Velingker, Ameya and Sinop, Ali Kemal and Ktena, Ira and Veli\v{c}kovi\'c, Petar and Gollapudi, Sreenivas},
  date = {2022-06-23},
  number = {arXiv:2206.11941},
  eprint = {arXiv:2206.11941},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.11941},
  urldate = {2022-07-30},
  abstract = {Graph Neural Networks (GNNs) have emerged as a powerful technique for learning on relational data. Owing to the relatively limited number of message passing steps they perform -- and hence a smaller receptive field -- there has been significant interest in improving their expressivity by incorporating structural aspects of the underlying graph. In this paper, we explore the use of affinity measures as features in graph neural networks, in particular measures arising from random walks, including effective resistance, hitting and commute times. We propose message passing networks based on these features and evaluate their performance on a variety of node and graph property prediction tasks. Our architecture has lower computational complexity, while our features are invariant to the permutations of the underlying graph. The measures we compute allow the network to exploit the connectivity properties of the graph, thereby allowing us to outperform relevant benchmarks for a wide variety of tasks, often with significantly fewer message passing steps. On one of the largest publicly available graph regression datasets, OGB-LSC-PCQM4Mv1, we obtain the best known single-model validation MAE at the time of writing.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/B2I3WTR6/Velingker et al. - 2022 - Affinity-Aware Graph Networks.pdf;/Users/lukakuma/Zotero/storage/8BJGWAXH/2206.html}
}

@article{vempralaChatGPTRoboticsDesign2023,
  title = {{{ChatGPT}} for {{Robotics}}: {{Design Principles}} and {{Model Abilities}}},
  author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
  date = {2023-02-20},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/78PUYGTD/Vemprala et al. - ChatGPT for Robotics Design Principles and Model .pdf}
}

@online{venutoMultiEnvironmentPretrainingEnables2022,
  title = {Multi-{{Environment Pretraining Enables Transfer}} to {{Action Limited Datasets}}},
  author = {Venuto, David and Yang, Sherry and Abbeel, Pieter and Precup, Doina and Mordatch, Igor and Nachum, Ofir},
  date = {2022-11-23},
  number = {arXiv:2211.13337},
  eprint = {arXiv:2211.13337},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.13337},
  urldate = {2022-12-02},
  abstract = {Using massive datasets to train large-scale models has emerged as a dominant approach for broad generalization in natural language and vision applications. In reinforcement learning, however, a key challenge is that available data of sequential decision making is often not annotated with actions - for example, videos of game-play are much more available than sequences of frames paired with their logged game controls. We propose to circumvent this challenge by combining large but sparsely-annotated datasets from a \textbackslash emph\{target\} environment of interest with fully-annotated datasets from various other \textbackslash emph\{source\} environments. Our method, Action Limited PreTraining (ALPT), leverages the generalization capabilities of inverse dynamics modelling (IDM) to label missing action data in the target environment. We show that utilizing even one additional environment dataset of labelled data during IDM pretraining gives rise to substantial improvements in generating action labels for unannotated sequences. We evaluate our method on benchmark game-playing environments and show that we can significantly improve game performance and generalization capability compared to other approaches, using annotated datasets equivalent to only \$12\$ minutes of gameplay. Highlighting the power of IDM, we show that these benefits remain even when target and source environments share no common actions.},
  pubstate = {preprint},
  keywords = {action/physical},
  file = {/Users/lukakuma/Zotero/storage/2HI4PFQV/Venuto et al. - 2022 - Multi-Environment Pretraining Enables Transfer to .pdf;/Users/lukakuma/Zotero/storage/273IRJAC/2211.html}
}

@unpublished{vermaInterpolationConsistencyTraining2020,
  title = {Interpolation {{Consistency Training}} for {{Semi-Supervised Learning}}},
  author = {Verma, Vikas and Kawaguchi, Kenji and Lamb, Alex and Kannala, Juho and Bengio, Yoshua and Lopez-Paz, David},
  date = {2020-12-29},
  eprint = {1903.03825},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1903.03825},
  urldate = {2022-05-12},
  abstract = {We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-theart performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets. Our theoretical analysis shows that ICT corresponds to a certain type of data-adaptive regularization with unlabeled points which reduces overfitting to labeled points under high confidence values.},
  langid = {english},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/VD9FECY4/Verma et al. - 2020 - Interpolation Consistency Training for Semi-Superv.pdf}
}

@article{vinyalsGrandmasterLevelStarCraft2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha\"el and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R\'emi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W\"unsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  date = {2019-11},
  journaltitle = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/s41586-019-1724-z},
  urldate = {2022-06-10},
  abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1\textendash 3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
  issue = {7782},
  langid = {english},
  keywords = {(ext) Sparrow,DeepMind},
  file = {/Users/lukakuma/Zotero/storage/4S3M7TFY/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf;/Users/lukakuma/Zotero/storage/P48G8HDP/s41586-019-1724-z.html}
}

@inproceedings{volumCraftIronSword2022,
  title = {Craft an {{Iron Sword}}: {{Dynamically Generating Interactive Game Characters}} by {{Prompting Large Language Models Tuned}} on {{Code}}},
  shorttitle = {Craft an {{Iron Sword}}},
  author = {Volum, Ryan and Rao, Sudha and Xu, Michael and DesGarennes, Gabriel A. and Brockett, Chris and Durme, Benjamin Van and Deng, Olivia and Malhotra, Akanksha and Dolan, Bill},
  date = {2022-08-02},
  url = {https://openreview.net/forum?id=I9glM3N6iAa},
  urldate = {2022-12-26},
  abstract = {Non-Player Characters (NPCs) significantly enhance the player experience in many games. Historically, players' interactions with NPCs have tended to be highly scripted, to be limited to natural language responses to be selected by the player, and to not involve dynamic change in game state. In this work, we demonstrate that use of a few example conversational prompts can power a conversational agent to generate both natural language and novel code. This approach can permit development of NPCs with which players can have grounded conversations that are free-form and less repetitive. We demonstrate our approach using OpenAI Codex (GPT-3 finetuned on GitHub), with Minecraft game development as our test bed. We show that with a few example prompts, a Codex-based agent can generate novel code, hold multi-turn conversations and answer questions about structured data. We evaluate this application using experienced gamers in a Minecraft realm and provide analysis of failure cases and suggest possible directions for solutions.},
  eventtitle = {The {{Third Wordplay}}: {{When Language Meets Games Workshop}}},
  langid = {english},
  keywords = {action/digital},
  file = {/Users/lukakuma/Zotero/storage/QKWQASJR/Volum et al. - 2022 - Craft an Iron Sword Dynamically Generating Intera.pdf}
}

@online{vonwerraEvaluateEvaluationHub2022,
  title = {Evaluate \& {{Evaluation}} on the {{Hub}}: {{Better Best Practices}} for {{Data}} and {{Model Measurements}}},
  shorttitle = {Evaluate \& {{Evaluation}} on the {{Hub}}},
  author = {family=Werra, given=Leandro, prefix=von, useprefix=true and Tunstall, Lewis and Thakur, Abhishek and Luccioni, Alexandra Sasha and Thrush, Tristan and Piktus, Aleksandra and Marty, Felix and Rajani, Nazneen and Mustar, Victor and Ngo, Helen and Sanseviero, Omar and \v{S}a\v{s}ko, Mario and Villanova, Albert and Lhoest, Quentin and Chaumond, Julien and Mitchell, Margaret and Rush, Alexander M. and Wolf, Thomas and Kiela, Douwe},
  date = {2022-10-06},
  number = {arXiv:2210.01970},
  eprint = {arXiv:2210.01970},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.01970},
  urldate = {2022-11-15},
  abstract = {Evaluation is a key part of machine learning (ML), yet there is a lack of support and tooling to enable its informed and systematic practice. We introduce Evaluate and Evaluation on the Hub --a set of tools to facilitate the evaluation of models and datasets in ML. Evaluate is a library to support best practices for measurements, metrics, and comparisons of data and models. Its goal is to support reproducibility of evaluation, centralize and document the evaluation process, and broaden evaluation to cover more facets of model performance. It includes over 50 efficient canonical implementations for a variety of domains and scenarios, interactive documentation, and the ability to easily share implementations and outcomes. The library is available at https://github.com/huggingface/evaluate. In addition, we introduce Evaluation on the Hub, a platform that enables the large-scale evaluation of over 75,000 models and 11,000 datasets on the Hugging Face Hub, for free, at the click of a button. Evaluation on the Hub is available at https://huggingface.co/autoevaluate.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/N4QUH3ZM/von Werra et al. - 2022 - Evaluate & Evaluation on the Hub Better Best Prac.pdf;/Users/lukakuma/Zotero/storage/ZSSUHIZX/2210.html}
}

@article{vranaAquariumOpensourceLaboratory2021,
  title = {Aquarium: Open-Source Laboratory Software for Design, Execution and Data Management},
  shorttitle = {Aquarium},
  author = {Vrana, Justin and family=Lange, given=Orlando, prefix=de, useprefix=true and Yang, Yaoyu and Newman, Garrett and Saleem, Ayesha and Miller, Abraham and Cordray, Cameron and Halabiya, Samer and Parks, Michelle and Lopez, Eriberto and Goldberg, Sarah and Keller, Benjamin and Strickland, Devin and Klavins, Eric},
  date = {2021-10-01},
  journaltitle = {Synthetic Biology},
  shortjournal = {Synthetic Biology},
  volume = {6},
  number = {1},
  pages = {ysab006},
  issn = {2397-7000},
  url = {https://doi.org/10.1093/synbio/ysab006},
  urldate = {2022-05-25},
  abstract = {Automation has been shown to improve the replicability and scalability of biomedical and bioindustrial research. Although the work performed in many labs is repetitive and can be standardized, few academic labs can afford the time and money required to automate their workflows with robotics. We propose that human-in-the-loop automation can fill this critical gap. To this end, we present Aquarium, an open-source, web-based software application that integrates experimental design, inventory management, protocol execution and data capture. We provide a high-level view of how researchers can install Aquarium and use it in their own labs. We discuss the impacts of the Aquarium on working practices, use in biofoundries and opportunities it affords for collaboration and education in life science laboratory research and manufacture.},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/XMF49FRJ/Vrana et al. - 2021 - Aquarium open-source laboratory software for desi.pdf;/Users/lukakuma/Zotero/storage/IXJUETXN/6124325.html}
}

@unpublished{vrcekLearningUntangleGenome2022,
  title = {Learning to {{Untangle Genome Assembly}} with {{Graph Convolutional Networks}}},
  author = {Vr\v{c}ek, Lovro and Bresson, Xavier and Laurent, Thomas and Schmitz, Martin and \v{S}iki\'c, Mile},
  date = {2022-06-01},
  eprint = {2206.00668},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2206.00668},
  urldate = {2022-06-03},
  abstract = {A quest to determine the complete sequence of a human DNA from telomere to telomere started three decades ago and was finally completed in 2021. This accomplishment was a result of a tremendous effort of numerous experts who engineered various tools and performed laborious manual inspection to achieve the first gapless genome sequence. However, such method can hardly be used as a general approach to assemble different genomes, especially when the assembly speed is critical given the large amount of data. In this work, we explore a different approach to the central part of the genome assembly task that consists of untangling a large assembly graph from which a genomic sequence needs to be reconstructed. Our main motivation is to reduce human-engineered heuristics and use deep learning to develop more generalizable reconstruction techniques. Precisely, we introduce a new learning framework to train a graph convolutional network to resolve assembly graphs by finding a correct path through them. The training is supervised with a dataset generated from the resolved CHM13 human sequence and tested on assembly graphs built using real human PacBio HiFi reads. Experimental results show that a model, trained on simulated graphs generated solely from a single chromosome, is able to remarkably resolve all other chromosomes. Moreover, the model outperforms hand-crafted heuristics from a state-of-the-art \textbackslash textit\{de novo\} assembler on the same graphs. Reconstructed chromosomes with graph networks are more accurate on nucleotide level, report lower number of contigs, higher genome reconstructed fraction and NG50/NGA50 assessment metrics.},
  file = {/Users/lukakuma/Zotero/storage/ZN4XK4Q9/Vrƒçek et al. - 2022 - Learning to Untangle Genome Assembly with Graph Convolutional Networks.pdf}
}

@online{wangAdaMixMixtureofAdaptationsParameterefficient2022,
  title = {{{AdaMix}}: {{Mixture-of-Adaptations}} for {{Parameter-efficient Model Tuning}}},
  shorttitle = {{{AdaMix}}},
  author = {Wang, Yaqing and Agarwal, Sahaj and Mukherjee, Subhabrata and Liu, Xiaodong and Gao, Jing and Awadallah, Ahmed Hassan and Gao, Jianfeng},
  date = {2022-11-01},
  number = {arXiv:2205.12410},
  eprint = {arXiv:2205.12410},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.12410},
  urldate = {2022-12-16},
  abstract = {Standard fine-tuning of large pre-trained language models (PLMs) for downstream tasks requires updating hundreds of millions to billions of parameters, and storing a large copy of the PLM weights for every task resulting in increased cost for storing, sharing and serving the models. To address this, parameter-efficient fine-tuning (PEFT) techniques were introduced where small trainable components are injected in the PLM and updated during fine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of adaptation modules -- given the underlying PEFT method of choice -- introduced in each Transformer layer while keeping most of the PLM weights frozen. For instance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture of low rank decomposition matrices like LoRA to improve downstream task performance over the corresponding PEFT methods for fully supervised and few-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the same computational cost and the number of tunable parameters as the underlying PEFT method. By only tuning 0.1-0.2\% of PLM parameters, we show that AdaMix outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for both NLU and NLG tasks.},
  pubstate = {preprint},
  keywords = {*peft,read},
  file = {/Users/lukakuma/Zotero/storage/IEG9BHPA/Wang et al. - 2022 - AdaMix Mixture-of-Adaptations for Parameter-effic.pdf;/Users/lukakuma/Zotero/storage/GBA6MSC6/2205.html}
}

@online{wangDescribeExplainPlan2023,
  title = {Describe, {{Explain}}, {{Plan}} and {{Select}}: {{Interactive Planning}} with {{Large Language Models Enables Open-World Multi-Task Agents}}},
  shorttitle = {Describe, {{Explain}}, {{Plan}} and {{Select}}},
  author = {Wang, Zihao and Cai, Shaofei and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
  date = {2023-02-03},
  number = {arXiv:2302.01560},
  eprint = {arXiv:2302.01560},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.01560},
  urldate = {2023-02-07},
  abstract = {In this paper, we study the problem of planning in Minecraft, a popular, democratized yet challenging open-ended environment for developing multi-task embodied agents. We've found two primary challenges of empowering such agents with planning: 1) planning in an open-ended world like Minecraft requires precise and multi-step reasoning due to the long-term nature of the tasks, and 2) as vanilla planners do not consider the proximity to the current agent when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient. To this end, we propose "Describe, Explain, Plan and Select" (DEPS), an interactive planning approach based on Large Language Models (LLMs). Our approach helps with better error correction from the feedback during the long-haul planning, while also bringing the sense of proximity via goal Selector, a learnable module that ranks parallel sub-goals based on the estimated steps of completion and improves the original plan accordingly. Our experiments mark the milestone of the first multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly doubles the overall performances. Finally, the ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the \$\textbackslash texttt\{ObtainDiamond\}\$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.},
  pubstate = {preprint},
  keywords = {action/digital},
  file = {/Users/lukakuma/Zotero/storage/L2JS745C/Wang et al. - 2023 - Describe, Explain, Plan and Select Interactive Pl.pdf;/Users/lukakuma/Zotero/storage/SRMCFUYP/2302.html}
}

@unpublished{wangDynamicGraphCNN2019,
  title = {Dynamic {{Graph CNN}} for {{Learning}} on {{Point Clouds}}},
  author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
  date = {2019-06-11},
  eprint = {1801.07829},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1801.07829},
  urldate = {2022-03-24},
  abstract = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/EQ2VNUTB/Wang et al. - 2019 - Dynamic Graph CNN for Learning on Point Clouds.pdf;/Users/lukakuma/Zotero/storage/52HIRLQL/1801.html}
}

@online{wangFinetuningLanguageModels2022,
  title = {Fine-Tuning {{Language Models}} over {{Slow Networks}} Using {{Activation Compression}} with {{Guarantees}}},
  author = {Wang, Jue and Yuan, Binhang and Rimanic, Luka and He, Yongjun and Dao, Tri and Chen, Beidi and Re, Christopher and Zhang, Ce},
  date = {2022-06-10},
  number = {arXiv:2206.01299},
  eprint = {arXiv:2206.01299},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.01299},
  urldate = {2022-12-01},
  abstract = {Communication compression is a crucial technique for modern distributed learning systems to alleviate their communication bottlenecks over slower networks. Despite recent intensive studies of gradient compression for data parallel-style training, compressing the activations for models trained with pipeline parallelism is still an open problem. In this paper, we propose AC-SGD, a novel activation compression algorithm for communication-efficient pipeline parallelism training over slow networks. Different from previous efforts in activation compression, instead of compressing activation values directly, AC-SGD compresses the changes of the activati{$\surd$}ons. This allows us to show, to the best of our knowledge for the first time, that one can still achieve O(1/ T ) convergence rate for non-convex objectives under activation compression, without making assumptions on gradient unbiasedness that do not hold for deep learning models with non-linear activation functions. We then show that AC-SGD can be optimized and implemented efficiently, without additional end-toend runtime overhead. We evaluated AC-SGD to fine-tune language models with up to 1.5 billion parameters, compressing activations to 2-4 bits. AC-SGD provides up to 4.3\texttimes{} end-to-end speed-up in slower networks, without sacrificing model quality. Moreover, we also show that AC-SGD can be combined with state-of-the-art gradient compression algorithms to enable ``end-to-end communication compression'': All communications between machines, including model gradients, forward activations, and backward gradients are compressed into lower precision. This provides up to 4.9\texttimes{} end-to-end speed-up, without sacrificing model quality.},
  langid = {english},
  pubstate = {preprint},
  keywords = {*open source AI},
  file = {/Users/lukakuma/Zotero/storage/NXAQKVSV/Wang et al. - 2022 - Fine-tuning Language Models over Slow Networks usi.pdf}
}

@online{wangFoundationTransformers2022,
  title = {Foundation {{Transformers}}},
  author = {Wang, Hongyu and Ma, Shuming and Huang, Shaohan and Dong, Li and Wang, Wenhui and Peng, Zhiliang and Wu, Yu and Bajaj, Payal and Singhal, Saksham and Benhaim, Alon and Patra, Barun and Liu, Zhun and Chaudhary, Vishrav and Song, Xia and Wei, Furu},
  date = {2022-10-12},
  number = {arXiv:2210.06423},
  eprint = {arXiv:2210.06423},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.06423},
  urldate = {2022-10-14},
  abstract = {A big convergence of model architectures across language, vision, speech, and multimodal is emerging. However, under the same name "Transformers", the above areas use different implementations for better performance, e.g., Post-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We call for the development of Foundation Transformer for true general-purpose modeling, which serves as a go-to architecture for various tasks and modalities with guaranteed training stability. In this work, we introduce a Transformer variant, named Magneto, to fulfill the goal. Specifically, we propose Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up. Extensive experiments demonstrate its superior performance and better stability than the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/Y24VVL56/Wang et al. - 2022 - Foundation Transformers.pdf;/Users/lukakuma/Zotero/storage/8JI536UR/2210.html}
}

@online{wangGITGenerativeImagetotext2022,
  title = {{{GIT}}: {{A Generative Image-to-text Transformer}} for {{Vision}} and {{Language}}},
  shorttitle = {{{GIT}}},
  author = {Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  date = {2022-12-15},
  number = {arXiv:2205.14100},
  eprint = {arXiv:2205.14100},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.14100},
  urldate = {2023-02-12},
  abstract = {In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks. Codes are released at \textbackslash url\{https://github.com/microsoft/GenerativeImage2Text\}.},
  pubstate = {preprint},
  keywords = {7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/636RBQG2/Wang et al. - 2022 - GIT A Generative Image-to-text Transformer for Vi.pdf;/Users/lukakuma/Zotero/storage/TMZP52UF/2205.html}
}

@online{wangImageForeignLanguage2022,
  title = {Image as a {{Foreign Language}}: {{BEiT Pretraining}} for {{All Vision}} and {{Vision-Language Tasks}}},
  shorttitle = {Image as a {{Foreign Language}}},
  author = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and Wei, Furu},
  date = {2022-08-22},
  number = {arXiv:2208.10442},
  eprint = {arXiv:2208.10442},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.10442},
  urldate = {2022-08-23},
  abstract = {A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked "language" modeling on images (Imglish), texts (English), and image-text pairs ("parallel sentences") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).},
  pubstate = {preprint},
  keywords = {7-multimodal,read},
  file = {/Users/lukakuma/Zotero/storage/BJP7NVAM/Wang et al. - 2022 - Image as a Foreign Language BEiT Pretraining for .pdf;/Users/lukakuma/Zotero/storage/CMUHIJCQ/2208.html}
}

@inproceedings{wangKAdapterInfusingKnowledge2021,
  title = {K-{{Adapter}}: {{Infusing Knowledge}} into {{Pre-Trained Models}} with {{Adapters}}},
  shorttitle = {K-{{Adapter}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Wang, Ruize and Tang, Duyu and Duan, Nan and Wei, Zhongyu and Huang, Xuanjing and Ji, Jianshu and Cao, Guihong and Jiang, Daxin and Zhou, Ming},
  date = {2021-08},
  pages = {1405--1418},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  url = {https://aclanthology.org/2021.findings-acl.121},
  urldate = {2023-03-02},
  eventtitle = {Findings 2021},
  file = {/Users/lukakuma/Zotero/storage/PFV7I8SX/Wang et al. - 2021 - K-Adapter Infusing Knowledge into Pre-Trained Mod.pdf}
}

@unpublished{wangLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Open Knowledge Graphs}}},
  author = {Wang, Chenguang and Liu, Xiao and Song, Dawn},
  date = {2020-10-22},
  eprint = {2010.11967},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.11967},
  urldate = {2022-04-13},
  abstract = {This paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available.},
  file = {/Users/lukakuma/Zotero/storage/MBHCEES7/Wang et al. - 2020 - Language Models are Open Knowledge Graphs.pdf;/Users/lukakuma/Zotero/storage/IGLK5RRL/2010.html}
}

@online{wangRationaleAugmentedEnsemblesLanguage2022,
  title = {Rationale-{{Augmented Ensembles}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  date = {2022-07-02},
  number = {arXiv:2207.00747},
  eprint = {arXiv:2207.00747},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.00747},
  urldate = {2022-10-25},
  abstract = {Recent research has shown that rationales, or step-by-step chains of thought, can be used to improve performance in multi-step reasoning tasks. We reconsider rationale-augmented prompting for few-shot in-context learning, where (input -{$>$} output) prompts are expanded to (input, rationale -{$>$} output) prompts. For rationale-augmented prompting we demonstrate how existing approaches, which rely on manual prompt engineering, are subject to sub-optimal rationales that may harm performance. To mitigate this brittleness, we propose a unified framework of rationale-augmented ensembles, where we identify rationale sampling in the output space as the key component to robustly improve performance. This framework is general and can easily be extended to common natural language processing tasks, even those that do not traditionally leverage intermediate steps, such as question answering, word sense disambiguation, and sentiment analysis. We demonstrate that rationale-augmented ensembles achieve more accurate and interpretable results than existing prompting approaches--including standard prompting without rationales and rationale-based chain-of-thought prompting--while simultaneously improving interpretability of model predictions through the associated rationales.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/8F9LT3MM/Wang et al. - 2022 - Rationale-Augmented Ensembles in Language Models.pdf;/Users/lukakuma/Zotero/storage/CMM97JG6/2207.html}
}

@online{wangSelfConsistencyImprovesChain2022a,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  date = {2022-10-04},
  number = {arXiv:2203.11171},
  eprint = {arXiv:2203.11171},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.11171},
  urldate = {2022-11-02},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  langid = {english},
  pubstate = {preprint},
  keywords = {5-reasoning,read},
  file = {/Users/lukakuma/Zotero/storage/DDHRUAUE/Wang et al. - 2022 - Self-Consistency Improves Chain of Thought Reasoni.pdf;/Users/lukakuma/Zotero/storage/IHJBNP2D/2203.html}
}

@online{wangSelfInstructAligningLanguage2022,
  title = {Self-{{Instruct}}: {{Aligning Language Model}} with {{Self Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  date = {2022-12-20},
  number = {arXiv:2212.10560},
  eprint = {arXiv:2212.10560},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.10560},
  urldate = {2022-12-23},
  abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/3ZQ5H9RB/Wang et al. - 2022 - Self-Instruct Aligning Language Model with Self G.pdf;/Users/lukakuma/Zotero/storage/WLNY7B7P/2212.html}
}

@online{wangSuperNaturalInstructionsGeneralizationDeclarative2022,
  title = {Super-{{NaturalInstructions}}: {{Generalization}} via {{Declarative Instructions}} on 1600+ {{NLP Tasks}}},
  shorttitle = {Super-{{NaturalInstructions}}},
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Sampat, Shailaja Keyur and Doshi, Savan and Mishra, Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh and Khashabi, Daniel},
  date = {2022-10-24},
  number = {arXiv:2204.07705},
  eprint = {arXiv:2204.07705},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.07705},
  urldate = {2022-12-12},
  abstract = {How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9\% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.},
  pubstate = {preprint},
  keywords = {finetuning,read},
  file = {/Users/lukakuma/Zotero/storage/RT3G7JRU/Wang et al. - 2022 - Super-NaturalInstructions Generalization via Decl.pdf;/Users/lukakuma/Zotero/storage/G8GKE76F/2204.html}
}

@article{wangSystematicOverviewBlockchain2021,
  title = {A {{Systematic Overview}} of {{Blockchain Research}}},
  author = {Wang, Guizhou and Zhang, Si and Yu, Tao and Ning, Yu},
  date = {2021-06-01},
  journaltitle = {Journal of Systems Science and Information},
  volume = {9},
  number = {3},
  pages = {205--238},
  publisher = {{De Gruyter}},
  issn = {2512-6660},
  url = {https://www.degruyter.com/document/doi/10.21078/JSSI-2021-205-34/html?lang=en},
  urldate = {2022-03-08},
  abstract = {Blockchain has been receiving growing attention from both academia and practices. This paper aims to investigate the research status of blockchain-related studies and to analyze the development and evolution of this latest hot area via bibliometric analysis. We selected and explored 2451 papers published between 2013 and 2019 from the Web of Science Core Collection database. The analysis considers different dimensions, including annual publications and citation trends, author distribution, popular research themes, collaboration of countries (regions) and institutions, top papers, major publication journals (conferences), supportive funding agencies, and emerging research trends. The results show that the number of blockchain literature is still increasing, and the research priorities in blockchain-related research shift during the observation period from bitcoin, cryptocurrency, blockchain, smart contract, internet of thing, to the distributed ledger, and challenge and the inefficiency of blockchain. The findings of this research deliver a holistic picture of blockchain research, which illuminates the future direction of research, and provides implications for both academic research and enterprise practice.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/6HWNYIEY/Wang et al. - 2021 - A Systematic Overview of Blockchain Research.pdf}
}

@unpublished{wangUnifyingArchitecturesTasks2022,
  title = {Unifying {{Architectures}}, {{Tasks}}, and {{Modalities Through}} a {{Simple Sequence-to-Sequence Learning Framework}}},
  author = {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  date = {2022-02-07},
  eprint = {2202.03052},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.03052},
  urldate = {2022-03-31},
  abstract = {In this work, we pursue a unified paradigm for multimodal pretraining to break the scaffolds of complex task/modality-specific customization. We propose OFA, a unified multimodal pretrained model that unifies modalities (i.e., cross-modality, vision, language) and tasks (e.g., image generation, visual grounding, image captioning, image classification, text generation, etc.) to a simple sequence-to-sequence learning framework based on the encoder-decoder architecture. OFA performs pretraining and finetuning with task instructions and introduces no extra task-specific layers for finetuning. Experimental results show that OFA achieves new state-of-the-arts on a series of multimodal tasks, including image captioning (COCO test CIDEr: 149.6), text-to-image generation (COCO test FID: 10.5), VQA (test-std acc.: 80.02), SNLI-VE (test acc.: 90.20), and referring expression comprehension (RefCOCO / RefCOCO+ / RefCOCOg test acc.: 92.93 / 90.10 / 85.20). Through extensive analyses, we demonstrate that OFA reaches comparable performance with uni-modal pretrained models (e.g., BERT, MAE, MoCo v3, SimCLR v2, etc.) in uni-modal tasks, including NLU, NLG, and image classification, and it effectively transfers to unseen tasks and domains. Code shall be released soon at http://github.com/OFA-Sys/OFA},
  version = {1},
  file = {/Users/lukakuma/Zotero/storage/JG6NHGFK/Wang et al. - 2022 - Unifying Architectures, Tasks, and Modalities Thro.pdf;/Users/lukakuma/Zotero/storage/64ISLCGV/2202.html}
}

@online{wangWhatLanguageModel2022a,
  title = {What {{Language Model Architecture}} and {{Pretraining Objective Work Best}} for {{Zero-Shot Generalization}}?},
  author = {Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Scao, Teven Le and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  date = {2022-04-12},
  number = {arXiv:2204.05832},
  eprint = {arXiv:2204.05832},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.05832},
  urldate = {2022-10-30},
  abstract = {Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.},
  pubstate = {preprint},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/LT6HQP4G/Wang et al. - 2022 - What Language Model Architecture and Pretraining O.pdf;/Users/lukakuma/Zotero/storage/L89TXDMJ/2204.html}
}

@online{weiChainThoughtPrompting2022,
  title = {Chain of {{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2022-01-28},
  url = {https://arxiv.org/abs/2201.11903v5},
  urldate = {2022-10-25},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  langid = {english},
  pubstate = {preprint},
  keywords = {5-reasoning,read},
  file = {/Users/lukakuma/Zotero/storage/VQI8HMLS/Wei et al. - 2022 - Chain of Thought Prompting Elicits Reasoning in La.pdf;/Users/lukakuma/Zotero/storage/2944RY7L/2201.html}
}

@online{weidingerEthicalSocialRisks2021,
  title = {Ethical and Social Risks of Harm from {{Language Models}}},
  author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
  date = {2021-12-08},
  number = {arXiv:2112.04359},
  eprint = {arXiv:2112.04359},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.04359},
  urldate = {2022-11-21},
  abstract = {This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/85MBI2KC/Weidinger et al. - 2021 - Ethical and social risks of harm from Language Mod.pdf;/Users/lukakuma/Zotero/storage/A5E9CYD5/2112.html}
}

@online{weiELITEEncodingVisual2023,
  title = {{{ELITE}}: {{Encoding Visual Concepts}} into {{Textual Embeddings}} for {{Customized Text-to-Image Generation}}},
  shorttitle = {{{ELITE}}},
  author = {Wei, Yuxiang and Zhang, Yabo and Ji, Zhilong and Bai, Jinfeng and Zhang, Lei and Zuo, Wangmeng},
  date = {2023-02-27},
  number = {arXiv:2302.13848},
  eprint = {arXiv:2302.13848},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.13848},
  urldate = {2023-02-28},
  abstract = {Despite unprecedented ability in imaginary creation, large text-to-image models are further expected to express customized concepts. Existing works generally learn such concepts in an optimization-based manner, yet bringing excessive computation or memory burden. In this paper, we instead propose a learning-based encoder for fast and accurate concept customization, which consists of global and local mapping networks. In specific, the global mapping network separately projects the hierarchical features of a given image into multiple ``new'' words in the textual word embedding space, i.e., one primary word for well-editable concept and other auxiliary words to exclude irrelevant disturbances (e.g., background). In the meantime, a local mapping network injects the encoded patch features into cross attention layers to provide omitted details, without sacrificing the editability of primary concepts. We compare our method with prior optimization-based approaches on a variety of user-defined concepts, and demonstrate that our method enables more high-fidelity inversion and robust editability with a significantly faster encoding process. Our code will be publicly available at https://github.com/csyxwei/ELITE.},
  pubstate = {preprint},
  keywords = {*diffusion,2-adapter},
  file = {/Users/lukakuma/Zotero/storage/XK3BHUUV/Wei et al. - 2023 - ELITE Encoding Visual Concepts into Textual Embed.pdf;/Users/lukakuma/Zotero/storage/95W7XMT7/2302.html}
}

@online{weiEmergentAbilitiesLarge2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  date = {2022-06-15},
  number = {arXiv:2206.07682},
  eprint = {arXiv:2206.07682},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2022-06-18},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  pubstate = {preprint},
  keywords = {emergence,read},
  file = {/Users/lukakuma/Zotero/storage/W46Y7MN7/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf;/Users/lukakuma/Zotero/storage/SESCSJXE/2206.html}
}

@unpublished{weiFinetunedLanguageModels2022,
  title = {Finetuned {{Language Models Are Zero-Shot Learners}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  date = {2022-02-08},
  eprint = {2109.01652},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2109.01652},
  urldate = {2022-05-09},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  keywords = {(ext) Flamingo,(ext) Gato,finetuning,read},
  file = {/Users/lukakuma/Zotero/storage/2ECYP98H/Wei et al. - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf;/Users/lukakuma/Zotero/storage/4DYQJUMS/2109.html}
}

@video{weights&biasesWojciechZarembaWhat2021,
  title = {Wojciech {{Zaremba}} \textemdash{} {{What Could Make AI Conscious}}?},
  editor = {{Weights \& Biases}},
  date = {2021-06-03},
  url = {https://www.youtube.com/watch?v=429QC4Yl-mA},
  urldate = {2022-08-18},
  editortype = {director}
}

@online{weiInverseScalingCan2022,
  title = {Inverse Scaling Can Become {{U-shaped}}},
  author = {Wei, Jason and Tay, Yi and Le, Quoc V.},
  date = {2022-11-14},
  number = {arXiv:2211.02011},
  eprint = {arXiv:2211.02011},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.02011},
  urldate = {2022-12-24},
  abstract = {Although scaling language models improves performance on a range of tasks, there are apparently some scenarios where scaling hurts performance. For instance, the Inverse Scaling Prize Round 1 (McKensie et al., 2022) identified four "inverse scaling" tasks, for which performance gets worse for larger models. These tasks were evaluated on models of up to 280B parameters, trained up to 500 zettaFLOPs of compute. This paper takes a closer look at these four tasks. We evaluate models of up to 540B parameters, trained on five times more compute than those evaluated in the Inverse Scaling Prize. With this increased range of model sizes and training compute, two out of the four tasks exhibit what we call "U-shaped scaling" -- performance decreases up to a certain model size, and then increases again up to the largest model evaluated. One hypothesis is that U-shaped scaling occurs when a task comprises a "true task" and a "distractor task". Medium-size models can do the distractor task, which hurts performance, while only large-enough models can ignore the distractor task and do the true task. The existence of U-shaped scaling implies that inverse scaling may not hold for larger models. Second, we evaluate the inverse scaling tasks using chain-of-thought (CoT) prompting, in addition to basic prompting without CoT. With CoT prompting, all four tasks show either U-shaped scaling or positive scaling, achieving perfect solve rates on two tasks and several sub-tasks. This suggests that the term "inverse scaling task" is under-specified -- a given task may be inverse scaling for one prompt but positive or U-shaped scaling for a different prompt.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/X6EJZM86/Wei et al. - 2022 - Inverse scaling can become U-shaped.pdf;/Users/lukakuma/Zotero/storage/SJAEPELB/2211.html}
}

@online{weinbachMVADERModelDiffusion2022,
  title = {M-{{VADER}}: {{A Model}} for {{Diffusion}} with {{Multimodal Context}}},
  shorttitle = {M-{{VADER}}},
  author = {Weinbach, Samuel and Bellagente, Marco and Eichenberg, Constantin and Dai, Andrew and Baldock, Robert and Nanda, Souradeep and Deiseroth, Bj\"orn and Oostermeijer, Koen and Teufel, Hannah and Cruz-Salinas, Andres Felipe},
  date = {2022-12-07},
  number = {arXiv:2212.02936},
  eprint = {arXiv:2212.02936},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.02936},
  urldate = {2022-12-09},
  abstract = {We introduce M-VADER: a diffusion model (DM) for image generation where the output can be specified using arbitrary combinations of images and text. We show how M-VADER enables the generation of images specified using combinations of image and text, and combinations of multiple images. Previously, a number of successful DM image generation algorithms have been introduced that make it possible to specify the output image using a text prompt. Inspired by the success of those models, and led by the notion that language was already developed to describe the elements of visual contexts that humans find most important, we introduce an embedding model closely related to a vision-language model. Specifically, we introduce the embedding model S-MAGMA: a 13 billion parameter multimodal decoder combining components from an autoregressive vision-language model MAGMA and biases finetuned for semantic search.},
  pubstate = {preprint},
  keywords = {*diffusion,7-multimodal,image generation},
  file = {/Users/lukakuma/Zotero/storage/ZVZURZR3/Weinbach et al. - 2022 - M-VADER A Model for Diffusion with Multimodal Con.pdf;/Users/lukakuma/Zotero/storage/CLM36SZC/2212.html}
}

@online{welblChallengesDetoxifyingLanguage2021,
  title = {Challenges in {{Detoxifying Language Models}}},
  author = {Welbl, Johannes and Glaese, Amelia and Uesato, Jonathan and Dathathri, Sumanth and Mellor, John and Hendricks, Lisa Anne and Anderson, Kirsty and Kohli, Pushmeet and Coppin, Ben and Huang, Po-Sen},
  date = {2021-09-15},
  number = {arXiv:2109.07445},
  eprint = {arXiv:2109.07445},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.07445},
  urldate = {2022-11-02},
  abstract = {Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the REALTOXICITYPROMPTS dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions\textemdash highlighting further the nuances involved in careful evaluation of LM toxicity.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/K5RTVHUV/Welbl et al. - 2021 - Challenges in Detoxifying Language Models.pdf}
}

@online{wengAttentionAttention2018,
  title = {Attention? {{Attention}}!},
  shorttitle = {Attention?},
  author = {Weng, Lilian},
  date = {2018-06-24T00:00:00+00:00},
  url = {https://lilianweng.github.io/posts/2018-06-24-attention/},
  urldate = {2022-08-22},
  abstract = {[Updated on 2018-10-28: Add Pointer Network and the link to my implementation of Transformer.] [Updated on 2018-11-06: Add a link to the implementation of Transformer model.] [Updated on 2018-11-18: Add Neural Turing Machines.] [Updated on 2019-07-18: Correct the mistake on using the term ``self-attention'' when introducing the show-attention-tell paper; moved it to Self-Attention section.] [Updated on 2020-04-07: A follow-up post on improved Transformer models is here.] Attention is, to some extent, motivated by how we pay visual attention to different regions of an image or correlate words in one sentence.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/KBXL43FU/2018-06-24-attention.html}
}

@online{wengEnvPoolHighlyParallel2022,
  title = {{{EnvPool}}: {{A Highly Parallel Reinforcement Learning Environment Execution Engine}}},
  shorttitle = {{{EnvPool}}},
  author = {Weng, Jiayi and Lin, Min and Huang, Shengyi and Liu, Bo and Makoviichuk, Denys and Makoviychuk, Viktor and Liu, Zichen and Song, Yufan and Luo, Ting and Jiang, Yukun and Xu, Zhongwen and Yan, Shuicheng},
  date = {2022-06-21},
  number = {arXiv:2206.10558},
  eprint = {arXiv:2206.10558},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.10558},
  urldate = {2022-06-22},
  abstract = {There has been significant progress in developing reinforcement learning (RL) training systems. Past works such as IMPALA, Apex, Seed RL, Sample Factory, and others aim to improve the system's overall throughput. In this paper, we try to address a common bottleneck in the RL training system, i.e., parallel environment execution, which is often the slowest part of the whole system but receives little attention. With a curated design for paralleling RL environments, we have improved the RL environment simulation speed across different hardware setups, ranging from a laptop, and a modest workstation, to a high-end machine like NVIDIA DGX-A100. On a high-end machine, EnvPool achieves 1 million frames per second for the environment execution on Atari environments and 3 million frames per second on MuJoCo environments. When running on a laptop, the speed of EnvPool is 2.8 times of the Python subprocess. Moreover, great compatibility with existing RL training libraries has been demonstrated in the open-sourced community, including CleanRL, rl\_games, DeepMind Acme, etc. Finally, EnvPool allows researchers to iterate their ideas at a much faster pace and has the great potential to become the de facto RL environment execution engine. Example runs show that it takes only 5 minutes to train Atari Pong and MuJoCo Ant, both on a laptop. EnvPool has already been open-sourced at https://github.com/sail-sg/envpool.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/Y6DQFD3C/Weng et al. - 2022 - EnvPool A Highly Parallel Reinforcement Learning .pdf;/Users/lukakuma/Zotero/storage/3GM3KM9E/2206.html}
}

@online{wengGeneralizedLanguageModels2019,
  title = {Generalized {{Language Models}}},
  author = {Weng, Lilian},
  date = {2019-01-31T00:00:00+00:00},
  url = {https://lilianweng.github.io/posts/2019-01-31-lm/},
  urldate = {2022-08-08},
  abstract = {[Updated on 2019-02-14: add ULMFiT and GPT-2.] [Updated on 2020-02-29: add ALBERT.] [Updated on 2020-10-25: add RoBERTa.] [Updated on 2020-12-13: add T5.] [Updated on 2020-12-30: add GPT-3.] [Updated on 2021-11-13: add XLNet, BART and ELECTRA; Also updated the Summary section.] Fig. 0. I guess they are Elmo \& Bert? (Image source: here) We have seen amazing progress in NLP in 2018. Large-scale pre-trained language modes like OpenAI GPT and BERT have achieved great performance on a variety of language tasks using generic model architectures.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/LBDKFRJ7/2019-01-31-lm.html}
}

@online{wengGeneralizedVisualLanguage2022,
  title = {Generalized {{Visual Language Models}}},
  author = {Weng, Lilian},
  date = {2022-06-09T15:10:30-07:00},
  url = {https://lilianweng.github.io/posts/2022-06-09-vlm/},
  urldate = {2022-08-08},
  abstract = {Processing images to generate text, such as image captioning and visual question-answering, has been studied for years. Traditionally such systems rely on an object detection network as a vision encoder to capture visual features and then produce text via a text decoder. Given a large amount of existing literature, in this post, I would like to only focus on one approach for solving vision language tasks, which is to extend pre-trained generalized language models to be capable of consuming visual signals.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/578KKAS5/2022-06-09-vlm.html}
}

@online{wengTransformerFamily2020,
  title = {The {{Transformer Family}}},
  author = {Weng, Lilian},
  date = {2020-04-07T00:00:00+00:00},
  url = {https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/},
  urldate = {2022-08-22},
  abstract = {It has been almost two years since my last post on attention. Recent progress on new and enhanced versions of Transformer motivates me to write another post on this specific topic, focusing on how the vanilla Transformer can be improved for longer-term attention span, less memory and computation consumption, RL task solving and more. Notations    Symbol Meaning     \$d\$ The model size / hidden state dimension / positional encoding size.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/TEK6RNXS/2020-04-07-the-transformer-family.html}
}

@online{wenHardPromptsMade2023,
  title = {Hard {{Prompts Made Easy}}: {{Gradient-Based Discrete Optimization}} for {{Prompt Tuning}} and {{Discovery}}},
  shorttitle = {Hard {{Prompts Made Easy}}},
  author = {Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  date = {2023-02-07},
  number = {arXiv:2302.03668},
  eprint = {arXiv:2302.03668},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.03668},
  urldate = {2023-02-08},
  abstract = {The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical "hard" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also "soft" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification.},
  pubstate = {preprint},
  keywords = {*peft,3-prefix tuning},
  file = {/Users/lukakuma/Zotero/storage/FGEW7VLZ/Wen et al. - 2023 - Hard Prompts Made Easy Gradient-Based Discrete Op.pdf;/Users/lukakuma/Zotero/storage/Y9XXAMRY/2302.html}
}

@online{wenzekCCNetExtractingHigh2019,
  title = {{{CCNet}}: {{Extracting High Quality Monolingual Datasets}} from {{Web Crawl Data}}},
  shorttitle = {{{CCNet}}},
  author = {Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm\'an, Francisco and Joulin, Armand and Grave, Edouard},
  date = {2019-11-14},
  number = {arXiv:1911.00359},
  eprint = {arXiv:1911.00359},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.00359},
  urldate = {2022-11-22},
  abstract = {Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.},
  pubstate = {preprint},
  keywords = {pretraining},
  file = {/Users/lukakuma/Zotero/storage/KENP2ERM/Wenzek et al. - 2019 - CCNet Extracting High Quality Monolingual Dataset.pdf;/Users/lukakuma/Zotero/storage/ETFY3UU2/1911.html}
}

@online{westSymbolicKnowledgeDistillation2022,
  title = {Symbolic {{Knowledge Distillation}}: From {{General Language Models}} to {{Commonsense Models}}},
  shorttitle = {Symbolic {{Knowledge Distillation}}},
  author = {West, Peter and Bhagavatula, Chandra and Hessel, Jack and Hwang, Jena D. and Jiang, Liwei and Bras, Ronan Le and Lu, Ximing and Welleck, Sean and Choi, Yejin},
  date = {2022-11-28},
  number = {arXiv:2110.07178},
  eprint = {arXiv:2110.07178},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.07178},
  urldate = {2022-12-14},
  abstract = {The common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the neural model. We also distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models.},
  pubstate = {preprint},
  keywords = {read,student teacher},
  file = {/Users/lukakuma/Zotero/storage/548Z2JAL/West et al. - 2022 - Symbolic Knowledge Distillation from General Lang.pdf;/Users/lukakuma/Zotero/storage/EF3I53JJ/2110.html}
}

@report{weylDecentralizedSocietyFinding2022,
  type = {SSRN Scholarly Paper},
  title = {Decentralized {{Society}}: {{Finding Web3}}'s {{Soul}}},
  shorttitle = {Decentralized {{Society}}},
  author = {Weyl, E. Glen and Ohlhaver, Puja and Buterin, Vitalik},
  date = {2022-05-10},
  number = {4105763},
  institution = {{Social Science Research Network}},
  location = {{Rochester, NY}},
  url = {https://papers.ssrn.com/abstract=4105763},
  urldate = {2022-05-12},
  abstract = {Web3 today centers around expressing transferable, financialized assets, rather than encoding social relationships of trust. Yet many core economic activities\textemdash such as uncollateralized lending and building personal brands\textemdash are built on persistent, non-transferable relationships. In this paper, we illustrate how non-transferable ``soulbound'' tokens (SBTs) representing the commitments, credentials, and affiliations of ``Souls'' can encode the trust networks of the real economy to establish provenance and reputation. More importantly, SBTs enable other applications of increasing ambition, such as community wallet recovery, sybil-resistant governance, mechanisms for decentralization, and novel markets with decomposable, shared rights. We call this richer, pluralistic ecosystem ``Decentralized Society'' (DeSoc)\textemdash a co-determined sociality, where Souls and communities come together bottom-up, as emergent properties of each other to co-create plural network goods and intelligences, at a range of scales. Key to this sociality is decomposable property rights and enhanced governance mechanisms\textemdash such as quadratic funding discounted by correlation scores\textemdash that reward trust and cooperation while protecting networks from capture, extraction, and domination. With such augmented sociality, web3 can eschew today's hyper-financialization in favor of a more transformative, pluralist future of increasing returns across social distance.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/XMRL4IJJ/Weyl et al. - 2022 - Decentralized Society Finding Web3's Soul.pdf;/Users/lukakuma/Zotero/storage/WHNLLWWY/papers.html}
}

@inproceedings{whittingtonRelatingTransformersModels2021,
  title = {Relating Transformers to Models and Neural Representations of the Hippocampal Formation},
  author = {Whittington, James C. R. and Warren, Joseph and Behrens, Tim E. J.},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=B8DVo9B1YE0},
  urldate = {2022-04-24},
  abstract = {Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/JVEEHWJU/Whittington et al. - 2021 - Relating transformers to models and neural represe.pdf;/Users/lukakuma/Zotero/storage/6YC93QTT/forum.html}
}

@online{WhyAmPluralist2022,
  title = {Why {{I Am}} a {{Pluralist}}},
  date = {2022-02-10},
  url = {https://www.radicalxchange.org/media/blog/why-i-am-a-pluralist/},
  urldate = {2022-03-08},
  abstract = {We are a community of activists, artists, entrepreneurs, and scholars committed to using mechanism design to inspire radical social change.},
  langid = {english},
  organization = {{RadicalxChange}},
  file = {/Users/lukakuma/Zotero/storage/CMTPM8WC/why-i-am-a-pluralist.html}
}

@unpublished{wildeClassicalQuantumShannon2017,
  title = {From {{Classical}} to {{Quantum Shannon Theory}}},
  author = {Wilde, Mark M.},
  date = {2017},
  eprint = {1106.1445},
  eprinttype = {arxiv},
  eprintclass = {quant-ph},
  url = {http://arxiv.org/abs/1106.1445},
  urldate = {2022-03-22},
  abstract = {The aim of this book is to develop "from the ground up" many of the major, exciting, pre- and post-millenium developments in the general area of study known as quantum Shannon theory. As such, we spend a significant amount of time on quantum mechanics for quantum information theory (Part II), we give a careful study of the important unit protocols of teleportation, super-dense coding, and entanglement distribution (Part III), and we develop many of the tools necessary for understanding information transmission or compression (Part IV). Parts V and VI are the culmination of this book, where all of the tools developed come into play for understanding many of the important results in quantum Shannon theory.},
  keywords = {information theory},
  file = {/Users/lukakuma/Zotero/storage/68ADNMAN/Wilde - 2017 - From Classical to Quantum Shannon Theory.pdf;/Users/lukakuma/Zotero/storage/BSE2W9AG/1106.html}
}

@article{wirnsbergerNormalizingFlowsAtomic2022,
  title = {Normalizing Flows for Atomic Solids},
  author = {Wirnsberger, Peter and Papamakarios, George and Ibarz, Borja and Racani\`ere, S\'ebastien and Ballard, Andrew J. and Pritzel, Alexander and Blundell, Charles},
  date = {2022-05},
  journaltitle = {Machine Learning: Science and Technology},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  volume = {3},
  number = {2},
  pages = {025009},
  publisher = {{IOP Publishing}},
  issn = {2632-2153},
  url = {https://doi.org/10.1088/2632-2153/ac6b16},
  urldate = {2022-05-17},
  abstract = {We present a machine-learning approach, based on normalizing flows, for modelling atomic solids. Our model transforms an analytically tractable base distribution into the target solid without requiring ground-truth samples for training. We report Helmholtz free energy estimates for cubic and hexagonal ice modelled as monatomic water as well as for a truncated and shifted Lennard-Jones system, and find them to be in excellent agreement with literature values and with estimates from established baseline methods. We further investigate structural properties and show that the model samples are nearly indistinguishable from the ones obtained with molecular dynamics. Our results thus demonstrate that normalizing flows can provide high-quality samples and free energy estimates without the need for multi-staging.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/H5HHQE3E/Wirnsberger et al. - 2022 - Normalizing flows for atomic solids.pdf}
}

@online{woodLaunchParachains2021,
  title = {The {{Launch}} of {{Parachains}}},
  author = {Wood, Gavin},
  date = {2021-05-17T18:01:00},
  url = {https://medium.com/polkadot-network/the-launch-of-parachains-78188fcf024f},
  urldate = {2021-05-18},
  abstract = {The staged launch of Polkadot is about to reach another chapter. As we approach the launch of Parachain functionality and the auctions and\ldots},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/lukakuma/Zotero/storage/BMZJSSHG/the-launch-of-parachains-78188fcf024f.html}
}

@online{workshopBLOOM176BParameterOpenAccess2022a,
  title = {{{BLOOM}}: {{A 176B-Parameter Open-Access Multilingual Language Model}}},
  shorttitle = {{{BLOOM}}},
  author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili\'c, Suzana and Hesslow, Daniel and Castagn\'e, Roman and Luccioni, Alexandra Sasha and Yvon, Fran\c{c}ois and Gall\'e, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Beno\^it and Muennighoff, Niklas and family=Moral, given=Albert Villanova, prefix=del, useprefix=true and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Lauren\c{c}on, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and family=Strien, given=Daniel, prefix=van, useprefix=true and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo Gonz\'alez and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, G\'erard and Kruszewski, Germ\'an and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and family=Rosa, given=Javier, prefix=de la, useprefix=true and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, J\"org and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and {allal}, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Mu\~noz, Manuel Romero and Masoud, Maraim and Grandury, Mar\'ia and \v{S}a\v{s}ko, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and family=Gibert, given=Ona, prefix=de, useprefix=true and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and L\'opez, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Ta\c{s}ar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and family=Platen, given=Patrick, prefix=von, useprefix=true and Cornette, Pierre and Lavall\'ee, Pierre Fran\c{c}ois and Lacroix, R\'emi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, St\'ephane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and N\'ev\'eol, Aur\'elie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and family=Wal, given=Oskar, prefix=van der, useprefix=true and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zden\v{e}k and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Mu\~noz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Cl\'ementine and Peri\~n\'an, Daniel Le\'on and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and family=Bykhovetz, given=Madeleine Hahn, prefix=de, useprefix=true and Takeuchi, Maiko and P\`amies, Marc and Castillo, Maria A. and Nezhurina, Marianna and S\"anger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Th\'eo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
  date = {2022-12-10},
  number = {arXiv:2211.05100},
  eprint = {arXiv:2211.05100},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.05100},
  urldate = {2023-01-10},
  abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/HACSNA3R/Workshop et al. - 2022 - BLOOM A 176B-Parameter Open-Access Multilingual L.pdf;/Users/lukakuma/Zotero/storage/ZWRT8DFK/2211.html}
}

@online{wortsmanLofiDistributedFinetuning2022,
  title = {Lo-Fi: Distributed Fine-Tuning without Communication},
  shorttitle = {Lo-Fi},
  author = {Wortsman, Mitchell and Gururangan, Suchin and Li, Shen and Farhadi, Ali and Schmidt, Ludwig and Rabbat, Michael and Morcos, Ari S.},
  date = {2022-11-12},
  number = {arXiv:2210.11948},
  eprint = {arXiv:2210.11948},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.11948},
  urldate = {2022-11-30},
  abstract = {When fine-tuning large neural networks, it is common to use multiple nodes and to communicate gradients at each optimization step. By contrast, we investigate completely local fine-tuning, which we refer to as lo-fi. During lo-fi, each node is fine-tuned independently without any communication. Then, the weights are averaged across nodes at the conclusion of fine-tuning. When fine-tuning DeiT-base and DeiT-large on ImageNet, this procedure matches accuracy in-distribution and improves accuracy under distribution shift compared to the baseline, which observes the same amount of data but communicates gradients at each step. We also observe that lo-fi matches the baseline's performance when fine-tuning OPT language models (up to 1.3B parameters) on Common Crawl. By removing the communication requirement, lo-fi reduces resource barriers for fine-tuning large models and enables fine-tuning in settings with prohibitive communication cost.},
  pubstate = {preprint},
  keywords = {model fusion},
  file = {/Users/lukakuma/Zotero/storage/H94GEP84/Wortsman et al. - 2022 - lo-fi distributed fine-tuning without communicati.pdf;/Users/lukakuma/Zotero/storage/XE986L2C/2210.html}
}

@online{wortsmanModelSoupsAveraging2022,
  title = {Model Soups: Averaging Weights of Multiple Fine-Tuned Models Improves Accuracy without Increasing Inference Time},
  shorttitle = {Model Soups},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  date = {2022-07-01},
  number = {arXiv:2203.05482},
  eprint = {arXiv:2203.05482},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.05482},
  urldate = {2022-12-16},
  abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results "model soups." When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94\% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.},
  pubstate = {preprint},
  keywords = {model fusion,read},
  file = {/Users/lukakuma/Zotero/storage/6M6XHKD2/Wortsman et al. - 2022 - Model soups averaging weights of multiple fine-tu.pdf;/Users/lukakuma/Zotero/storage/UAKKKZB8/2203.html}
}

@inproceedings{wuAIChainsTransparent2022,
  title = {{{AI Chains}}: {{Transparent}} and {{Controllable Human-AI Interaction}} by {{Chaining Large Language Model Prompts}}},
  shorttitle = {{{AI Chains}}},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
  date = {2022-04-29},
  series = {{{CHI}} '22},
  pages = {1--22},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/3491102.3517582},
  urldate = {2022-12-21},
  abstract = {Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by ``unit-testing'' sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.},
  isbn = {978-1-4503-9157-3},
  keywords = {calibration},
  file = {/Users/lukakuma/Zotero/storage/9YD2L6HK/Wu et al. - 2022 - AI Chains Transparent and Controllable Human-AI I.pdf}
}

@online{wuCONQRRConversationalQuery2022,
  title = {{{CONQRR}}: {{Conversational Query Rewriting}} for {{Retrieval}} with {{Reinforcement Learning}}},
  shorttitle = {{{CONQRR}}},
  author = {Wu, Zeqiu and Luan, Yi and Rashkin, Hannah and Reitter, David and Hajishirzi, Hannaneh and Ostendorf, Mari and Tomar, Gaurav Singh},
  date = {2022-10-28},
  number = {arXiv:2112.08558},
  eprint = {arXiv:2112.08558},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.08558},
  urldate = {2023-02-20},
  abstract = {Compared to standard retrieval tasks, passage retrieval for conversational question answering (CQA) poses new challenges in understanding the current user question, as each question needs to be interpreted within the dialogue context. Moreover, it can be expensive to re-train well-established retrievers such as search engines that are originally developed for non-conversational queries. To facilitate their use, we develop a query rewriting model CONQRR that rewrites a conversational question in the context into a standalone question. It is trained with a novel reward function to directly optimize towards retrieval using reinforcement learning and can be adapted to any off-the-shelf retriever. CONQRR achieves state-of-the-art results on a recent open-domain CQA dataset containing conversations from three different sources, and is effective for two different off-the-shelf retrievers. Our extensive analysis also shows the robustness of CONQRR to out-of-domain dialogues as well as to zero query rewriting supervision.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/8RELUZVN/Wu et al. - 2022 - CONQRR Conversational Query Rewriting for Retriev.pdf;/Users/lukakuma/Zotero/storage/RAGM7DAP/2112.html}
}

@online{wuNUWAInfinityAutoregressiveAutoregressive2022,
  title = {{{NUWA-Infinity}}: {{Autoregressive}} over {{Autoregressive Generation}} for {{Infinite Visual Synthesis}}},
  shorttitle = {{{NUWA-Infinity}}},
  author = {Wu, Chenfei and Liang, Jian and Hu, Xiaowei and Gan, Zhe and Wang, Jianfeng and Wang, Lijuan and Liu, Zicheng and Fang, Yuejian and Duan, Nan},
  date = {2022-08-12},
  number = {arXiv:2207.09814},
  eprint = {arXiv:2207.09814},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.09814},
  urldate = {2022-12-27},
  abstract = {In this paper, we present NUWA-Infinity, a generative model for infinite visual synthesis, which is defined as the task of generating arbitrarily-sized high-resolution images or long-duration videos. An autoregressive over autoregressive generation mechanism is proposed to deal with this variable-size generation task, where a global patch-level autoregressive model considers the dependencies between patches, and a local token-level autoregressive model considers dependencies between visual tokens within each patch. A Nearby Context Pool (NCP) is introduced to cache-related patches already generated as the context for the current patch being generated, which can significantly save computation costs without sacrificing patch-level dependency modeling. An Arbitrary Direction Controller (ADC) is used to decide suitable generation orders for different visual synthesis tasks and learn order-aware positional embeddings. Compared to DALL-E, Imagen and Parti, NUWA-Infinity can generate high-resolution images with arbitrary sizes and support long-duration video generation additionally. Compared to NUWA, which also covers images and videos, NUWA-Infinity has superior visual synthesis capabilities in terms of resolution and variable-size generation. The GitHub link is https://github.com/microsoft/NUWA. The homepage link is https://nuwa-infinity.microsoft.com.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/YMKW7C3D/Wu et al. - 2022 - NUWA-Infinity Autoregressive over Autoregressive .pdf;/Users/lukakuma/Zotero/storage/Q655AGL2/2207.html}
}

@online{wuPromptChainerChainingLarge2022,
  title = {{{PromptChainer}}: {{Chaining Large Language Model Prompts}} through {{Visual Programming}}},
  shorttitle = {{{PromptChainer}}},
  author = {Wu, Tongshuang and Jiang, Ellen and Donsbach, Aaron and Gray, Jeff and Molina, Alejandra and Terry, Michael and Cai, Carrie J.},
  date = {2022-03-12},
  number = {arXiv:2203.06566},
  eprint = {arXiv:2203.06566},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.06566},
  urldate = {2023-02-20},
  abstract = {While LLMs can effectively help prototype single ML functionalities, many real-world applications involve complex tasks that cannot be easily handled via a single run of an LLM. Recent work has found that chaining multiple LLM runs together (with the output of one step being the input to the next) can help users accomplish these more complex tasks, and in a way that is perceived to be more transparent and controllable. However, it remains unknown what users need when authoring their own LLM chains -- a key step for lowering the barriers for non-AI-experts to prototype AI-infused applications. In this work, we explore the LLM chain authoring process. We conclude from pilot studies find that chaining requires careful scaffolding for transforming intermediate node outputs, as well as debugging the chain at multiple granularities; to help with these needs, we designed PromptChainer, an interactive interface for visually programming chains. Through case studies with four people, we show that PromptChainer supports building prototypes for a range of applications, and conclude with open questions on scaling chains to complex tasks, and supporting low-fi chain prototyping.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/I2IKGM8A/Wu et al. - 2022 - PromptChainer Chaining Large Language Model Promp.pdf;/Users/lukakuma/Zotero/storage/YMDG99MS/2203.html}
}

@online{wuRecursivelySummarizingBooks2021a,
  title = {Recursively {{Summarizing Books}} with {{Human Feedback}}},
  author = {Wu, Jeff and Ouyang, Long and Ziegler, Daniel M. and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
  date = {2021-09-27},
  number = {arXiv:2109.10862},
  eprint = {arXiv:2109.10862},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.10862},
  urldate = {2023-02-20},
  abstract = {A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases (\$\textbackslash sim5\textbackslash\%\$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow,alignment,RLHF,summarization},
  file = {/Users/lukakuma/Zotero/storage/24EEB24C/Wu et al. - 2021 - Recursively Summarizing Books with Human Feedback.pdf;/Users/lukakuma/Zotero/storage/AFDYJI5Y/2109.html}
}

@online{wuScalableTrustregionMethod2017,
  title = {Scalable Trust-Region Method for Deep Reinforcement Learning Using {{Kronecker-factored}} Approximation},
  author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
  date = {2017-08-18},
  number = {arXiv:1708.05144},
  eprint = {arXiv:1708.05144},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1708.05144},
  urldate = {2022-07-23},
  abstract = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/7HENXBA4/Wu et al. - 2017 - Scalable trust-region method for deep reinforcemen.pdf;/Users/lukakuma/Zotero/storage/56NGRFX5/1708.html}
}

@online{wynnDiffusioNeRFRegularizingNeural2023,
  title = {{{DiffusioNeRF}}: {{Regularizing Neural Radiance Fields}} with {{Denoising Diffusion Models}}},
  shorttitle = {{{DiffusioNeRF}}},
  author = {Wynn, Jamie and Turmukhambetov, Daniyar},
  date = {2023-02-23},
  number = {arXiv:2302.12231},
  eprint = {arXiv:2302.12231},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.12231},
  urldate = {2023-02-25},
  abstract = {Under good conditions, Neural Radiance Fields (NeRFs) have shown impressive results on novel view synthesis tasks. NeRFs learn a scene's color and density fields by minimizing the photometric discrepancy between training views and differentiable renders of the scene. Once trained from a sufficient set of views, NeRFs can generate novel views from arbitrary camera positions. However, the scene geometry and color fields are severely under-constrained, which can lead to artifacts, especially when trained with few input views. To alleviate this problem we learn a prior over scene geometry and color, using a denoising diffusion model (DDM). Our DDM is trained on RGBD patches of the synthetic Hypersim dataset and can be used to predict the gradient of the logarithm of a joint probability distribution of color and depth patches. We show that, during NeRF training, these gradients of logarithms of RGBD patch priors serve to regularize geometry and color for a scene. During NeRF training, random RGBD patches are rendered and the estimated gradients of the log-likelihood are backpropagated to the color and density fields. Evaluations on LLFF, the most relevant dataset, show that our learned prior achieves improved quality in the reconstructed geometry and improved generalization to novel views. Evaluations on DTU show improved reconstruction quality among NeRF methods.},
  pubstate = {preprint},
  keywords = {*diffusion},
  file = {/Users/lukakuma/Zotero/storage/AQUEEAZR/Wynn and Turmukhambetov - 2023 - DiffusioNeRF Regularizing Neural Radiance Fields .pdf;/Users/lukakuma/Zotero/storage/WP7U5XJY/2302.html}
}

@online{xcorrMyStackResearch2021,
  title = {My Stack for Research {{ML}} Projects},
  author = {{XCORR}},
  date = {2021-06-09T17:04:38+00:00},
  url = {https://xcorr.net/2021/06/09/my-stack-for-research-ml-projects/},
  urldate = {2022-05-31},
  abstract = {For the past few months, I've been working on a machine learning research project, which I just submitted to NeurIPS [update: it was accepted as a spotlight! Preprint here]. The scale is, all\ldots},
  langid = {english},
  organization = {{xcorr: comp neuro}},
  file = {/Users/lukakuma/Zotero/storage/ETB4C5H9/my-stack-for-research-ml-projects.html}
}

@unpublished{xiangSAPIENSimulAtedPartbased2020,
  title = {{{SAPIEN}}: {{A SimulAted Part-based Interactive ENvironment}}},
  shorttitle = {{{SAPIEN}}},
  author = {Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu and Wang, He and Yi, Li and Chang, Angel X. and Guibas, Leonidas J. and Su, Hao},
  date = {2020-03-18},
  eprint = {2003.08515},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.08515},
  urldate = {2022-04-28},
  abstract = {Building home assistant robots has long been a pursuit for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufficient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simplification and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set for articulated objects. Our SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We evaluate state-of-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that our SAPIEN can open a lot of research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment.},
  file = {/Users/lukakuma/Zotero/storage/QIVU2YEJ/Xiang et al. - 2020 - SAPIEN A SimulAted Part-based Interactive ENviron.pdf;/Users/lukakuma/Zotero/storage/RIK468TI/2003.html}
}

@online{xiaoOffsiteTuningTransferLearning2023,
  title = {Offsite-{{Tuning}}: {{Transfer Learning}} without {{Full Model}}},
  shorttitle = {Offsite-{{Tuning}}},
  author = {Xiao, Guangxuan and Lin, Ji and Han, Song},
  date = {2023-02-09},
  url = {https://arxiv.org/abs/2302.04870v1},
  urldate = {2023-02-11},
  abstract = {Transfer learning is important for foundation models to adapt to downstream tasks. However, many foundation models are proprietary, so users must share their data with model owners to fine-tune the models, which is costly and raise privacy concerns. Moreover, fine-tuning large foundation models is computation-intensive and impractical for most downstream users. In this paper, we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning framework that can adapt billion-parameter foundation models to downstream data without access to the full model. In offsite-tuning, the model owner sends a light-weight adapter and a lossy compressed emulator to the data owner, who then fine-tunes the adapter on the downstream data with the emulator's assistance. The fine-tuned adapter is then returned to the model owner, who plugs it into the full model to create an adapted foundation model. Offsite-tuning preserves both parties' privacy and is computationally more efficient than the existing fine-tuning methods that require access to the full model weights. We demonstrate the effectiveness of offsite-tuning on various large language and vision foundation models. Offsite-tuning can achieve comparable accuracy as full model fine-tuning while being privacy-preserving and efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is available at https://github.com/mit-han-lab/offsite-tuning.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/GYTV6XEW/Xiao et al. - 2023 - Offsite-Tuning Transfer Learning without Full Mod.pdf}
}

@online{xiaoRoboticSkillAcquisition2022,
  title = {Robotic {{Skill Acquisition}} via {{Instruction Augmentation}} with {{Vision-Language Models}}},
  author = {Xiao, Ted and Chan, Harris and Sermanet, Pierre and Wahid, Ayzaan and Brohan, Anthony and Hausman, Karol and Levine, Sergey and Tompson, Jonathan},
  date = {2022-11-22},
  number = {arXiv:2211.11736},
  eprint = {arXiv:2211.11736},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.11736},
  urldate = {2023-02-20},
  abstract = {In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration data and then train language-conditioned policies on the augmented datasets. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where 96.5\% of the 80,000 demonstrations do not contain crowd-sourced language annotations. DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.},
  pubstate = {preprint},
  keywords = {action/physical},
  file = {/Users/lukakuma/Zotero/storage/N8IIEKJ2/Xiao et al. - 2022 - Robotic Skill Acquisition via Instruction Augmenta.pdf;/Users/lukakuma/Zotero/storage/9G986PKU/2211.html}
}

@online{xuAcceleratedPolicyLearning2022,
  title = {Accelerated {{Policy Learning}} with {{Parallel Differentiable Simulation}}},
  author = {Xu, Jie and Makoviychuk, Viktor and Narang, Yashraj and Ramos, Fabio and Matusik, Wojciech and Garg, Animesh and Macklin, Miles},
  date = {2022-04-14},
  number = {arXiv:2204.07137},
  eprint = {arXiv:2204.07137},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.07137},
  urldate = {2022-07-29},
  abstract = {Deep reinforcement learning can generate complex control policies, but requires large amounts of training data to work effectively. Recent work has attempted to address this issue by leveraging differentiable simulators. However, inherent problems such as local minima and exploding/vanishing numerical gradients prevent these methods from being generally applied to control tasks with complex contact-rich dynamics, such as humanoid locomotion in classical RL benchmarks. In this work we present a high-performance differentiable simulator and a new policy learning algorithm (SHAC) that can effectively leverage simulation gradients, even in the presence of non-smoothness. Our learning algorithm alleviates problems with local minima through a smooth critic function, avoids vanishing/exploding gradients through a truncated learning window, and allows many physical environments to be run in parallel. We evaluate our method on classical RL control tasks, and show substantial improvements in sample efficiency and wall-clock time over state-of-the-art RL and differentiable simulation-based algorithms. In addition, we demonstrate the scalability of our method by applying it to the challenging high-dimensional problem of muscle-actuated locomotion with a large action space, achieving a greater than 17x reduction in training time over the best-performing established RL algorithm.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/KNSG47N6/Xu et al. - 2022 - Accelerated Policy Learning with Parallel Differen.pdf;/Users/lukakuma/Zotero/storage/ACBTJBN7/2204.html}
}

@online{xuAutomaticCrossReplicaSharding2020,
  title = {Automatic {{Cross-Replica Sharding}} of {{Weight Update}} in {{Data-Parallel Training}}},
  author = {Xu, Yuanzhong and Lee, HyoukJoong and Chen, Dehao and Choi, Hongjun and Hechtman, Blake and Wang, Shibo},
  date = {2020-04-28},
  number = {arXiv:2004.13336},
  eprint = {arXiv:2004.13336},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.13336},
  urldate = {2022-10-18},
  abstract = {In data-parallel synchronous training of deep neural networks, different devices (replicas) run the same program with different partitions of the training batch, but weight update computation is repeated on all replicas, because the weights do not have a batch dimension to partition. This can be a bottleneck for performance and scalability in typical language models with large weights, and models with small per-replica batch size which is typical in large-scale training. This paper presents an approach to automatically shard the weight update computation across replicas with efficient communication primitives and data formatting, using static analysis and transformations on the training computation graph. We show this technique achieves substantial speedups on typical image and language models on Cloud TPUs, requiring no change to model code. This technique helps close the gap between traditionally expensive (ADAM) and cheap (SGD) optimizers, as they will only take a small part of training step time and have similar peak memory usage. It helped us to achieve state-of-the-art training performance in Google's MLPerf 0.6 submission.},
  pubstate = {preprint},
  keywords = {distributed training},
  file = {/Users/lukakuma/Zotero/storage/4LWL6S7I/Xu et al. - 2020 - Automatic Cross-Replica Sharding of Weight Update .pdf;/Users/lukakuma/Zotero/storage/FJTM7JCD/2004.html}
}

@online{xuDetoxifyingLanguageModels2021,
  title = {Detoxifying {{Language Models Risks Marginalizing Minority Voices}}},
  author = {Xu, Albert and Pathak, Eshaan and Wallace, Eric and Gururangan, Suchin and Sap, Maarten and Klein, Dan},
  date = {2021-04-13},
  number = {arXiv:2104.06390},
  eprint = {arXiv:2104.06390},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.06390},
  urldate = {2022-11-02},
  abstract = {Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/UC755C5D/Xu et al. - 2021 - Detoxifying Language Models Risks Marginalizing Mi.pdf}
}

@online{xueGoWiderInstead2021,
  title = {Go {{Wider Instead}} of {{Deeper}}},
  author = {Xue, Fuzhao and Shi, Ziji and Wei, Futao and Lou, Yuxuan and Liu, Yong and You, Yang},
  date = {2021-09-07},
  number = {arXiv:2107.11817},
  eprint = {arXiv:2107.11817},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.11817},
  urldate = {2022-10-08},
  abstract = {More transformer blocks with residual connections have recently achieved impressive results on various tasks. To achieve better performance with fewer trainable parameters, recent methods are proposed to go shallower by parameter sharing or model compressing along with the depth. However, weak modeling capacity limits their performance. Contrastively, going wider by inducing more trainable matrixes and parameters would produce a huge model requiring advanced parallelism to train and inference. In this paper, we propose a parameter-efficient framework, going wider instead of deeper. Specially, following existing works, we adapt parameter sharing to compress along depth. But, such deployment would limit the performance. To maximize modeling capacity, we scale along model width by replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across transformer blocks, instead of sharing normalization layers, we propose to use individual layernorms to transform various semantic representations in a more parameter-efficient way. To evaluate our plug-and-run framework, we design WideNet and conduct comprehensive experiments on popular computer vision and natural language processing benchmarks. On ImageNet-1K, our best model outperforms Vision Transformer (ViT) by \$1.5\textbackslash\%\$ with \$0.72 \textbackslash times\$ trainable parameters. Using \$0.46 \textbackslash times\$ and \$0.13 \textbackslash times\$ parameters, our WideNet can still surpass ViT and ViT-MoE by \$0.8\textbackslash\%\$ and \$2.1\textbackslash\%\$, respectively. On four natural language processing datasets, WideNet outperforms ALBERT by \$1.8\textbackslash\%\$ on average and surpass BERT using factorized embedding parameterization by \$0.8\textbackslash\%\$ with fewer parameters.},
  pubstate = {preprint},
  keywords = {sparsity},
  file = {/Users/lukakuma/Zotero/storage/IXSPT6VU/Xue et al. - 2021 - Go Wider Instead of Deeper.pdf;/Users/lukakuma/Zotero/storage/QZDDRWG7/2107.html}
}

@inproceedings{xueMT5MassivelyMultilingual2021,
  title = {{{mT5}}: {{A Massively Multilingual Pre-trained Text-to-Text Transformer}}},
  shorttitle = {{{mT5}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  date = {2021},
  pages = {483--498},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  url = {https://aclanthology.org/2021.naacl-main.41},
  urldate = {2022-11-01},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/3ZVTEJ9Q/Xue et al. - 2021 - mT5 A Massively Multilingual Pre-trained Text-to-.pdf}
}

@inproceedings{xuGoldfishMemoryLongTerm2022,
  title = {Beyond {{Goldfish Memory}}: {{Long-Term Open-Domain Conversation}}},
  shorttitle = {Beyond {{Goldfish Memory}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Xu, Jing and Szlam, Arthur and Weston, Jason},
  date = {2022-05},
  pages = {5180--5197},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  url = {https://aclanthology.org/2022.acl-long.356},
  urldate = {2022-12-05},
  abstract = {Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about each other's interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art.},
  eventtitle = {{{ACL}} 2022},
  keywords = {memory},
  file = {/Users/lukakuma/Zotero/storage/A94FKFCR/Xu et al. - 2022 - Beyond Goldfish Memory Long-Term Open-Domain Conv.pdf}
}

@online{xuGSPMDGeneralScalable2021,
  title = {{{GSPMD}}: {{General}} and {{Scalable Parallelization}} for {{ML Computation Graphs}}},
  shorttitle = {{{GSPMD}}},
  author = {Xu, Yuanzhong and Lee, HyoukJoong and Chen, Dehao and Hechtman, Blake and Huang, Yanping and Joshi, Rahul and Krikun, Maxim and Lepikhin, Dmitry and Ly, Andy and Maggioni, Marcello and Pang, Ruoming and Shazeer, Noam and Wang, Shibo and Wang, Tao and Wu, Yonghui and Chen, Zhifeng},
  date = {2021-12-23},
  number = {arXiv:2105.04663},
  eprint = {arXiv:2105.04663},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.04663},
  urldate = {2022-07-28},
  abstract = {We present GSPMD, an automatic, compiler-based parallelization system for common machine learning computations. It allows users to write programs in the same way as for a single device, then give hints through a few annotations on how to distribute tensors, based on which GSPMD will parallelize the computation. Its representation of partitioning is simple yet general, allowing it to express different or mixed paradigms of parallelism on a wide variety of models. GSPMD infers the partitioning for every operator based on limited user annotations, making it convenient to scale existing single-device programs. It solves several technical challenges for production usage, allowing GSPMD to achieve 50\% to 62\% compute utilization on up to 2048 Cloud TPUv3 cores for models with up to one trillion parameters.},
  pubstate = {preprint},
  keywords = {distributed training,read},
  file = {/Users/lukakuma/Zotero/storage/EVASXW6C/Xu et al. - 2021 - GSPMD General and Scalable Parallelization for ML.pdf;/Users/lukakuma/Zotero/storage/3Y2XNACM/2105.html}
}

@online{xuHowNeuralNetworks2021,
  title = {How {{Neural Networks Extrapolate}}: {{From Feedforward}} to {{Graph Neural Networks}}},
  shorttitle = {How {{Neural Networks Extrapolate}}},
  author = {Xu, Keyulu and Zhang, Mozhi and Li, Jingling and Du, Simon S. and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  date = {2021-03-02},
  number = {arXiv:2009.11848},
  eprint = {arXiv:2009.11848},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.11848},
  urldate = {2022-12-20},
  abstract = {We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) -- structured networks with MLP modules -- have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently "diverse". Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/JPBJPLA3/Xu et al. - 2021 - How Neural Networks Extrapolate From Feedforward .pdf;/Users/lukakuma/Zotero/storage/3HDNP3B4/2009.html}
}

@unpublished{xuHowPowerfulAre2019,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  date = {2019-02-22},
  eprint = {1810.00826},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.00826},
  urldate = {2022-03-23},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/XC4YXCPH/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf;/Users/lukakuma/Zotero/storage/HM3BWM22/1810.html}
}

@online{xuLearningNewSkills2022,
  title = {Learning {{New Skills}} after {{Deployment}}: {{Improving}} Open-Domain Internet-Driven Dialogue with Human Feedback},
  shorttitle = {Learning {{New Skills}} after {{Deployment}}},
  author = {Xu, Jing and Ung, Megan and Komeili, Mojtaba and Arora, Kushal and Boureau, Y.-Lan and Weston, Jason},
  date = {2022-08-16},
  number = {arXiv:2208.03270},
  eprint = {arXiv:2208.03270},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.03270},
  urldate = {2022-11-28},
  abstract = {Frozen models trained to mimic static datasets can never improve their performance. Models that can employ internet-retrieval for up-to-date information and obtain feedback from humans during deployment provide the promise of both adapting to new information, and improving their performance. In this work we study how to improve internet-driven conversational skills in such a learning framework. We collect deployment data, which we make publicly available, of human interactions, and collect various types of human feedback -- including binary quality measurements, free-form text feedback, and fine-grained reasons for failure. We then study various algorithms for improving from such feedback, including standard supervised learning, rejection sampling, model-guiding and reward-based learning, in order to make recommendations on which type of feedback and algorithms work best. We find the recently introduced Director model (Arora et al., '22) shows significant improvements over other existing approaches.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/TWDFDC52/Xu et al. - 2022 - Learning New Skills after Deployment Improving op.pdf;/Users/lukakuma/Zotero/storage/EDM3VC7Y/2208.html}
}

@online{xuMultimodalLearningTransformers2022,
  title = {Multimodal {{Learning}} with {{Transformers}}: {{A Survey}}},
  shorttitle = {Multimodal {{Learning}} with {{Transformers}}},
  author = {Xu, Peng and Zhu, Xiatian and Clifton, David A.},
  date = {2022-06-13},
  number = {arXiv:2206.06488},
  eprint = {arXiv:2206.06488},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.06488},
  urldate = {2022-08-08},
  abstract = {Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.},
  pubstate = {preprint},
  keywords = {7-multimodal,read},
  file = {/Users/lukakuma/Zotero/storage/HQQ5M6YU/Xu et al. - 2022 - Multimodal Learning with Transformers A Survey.pdf;/Users/lukakuma/Zotero/storage/FSSX8TS4/2206.html}
}

@online{xuPairConnectComputeEfficientMLP2021,
  title = {{{PairConnect}}: {{A Compute-Efficient MLP Alternative}} to {{Attention}}},
  shorttitle = {{{PairConnect}}},
  author = {Xu, Zhaozhuo and Yan, Minghao and Zhang, Junyan and Shrivastava, Anshumali},
  date = {2021-06-15},
  number = {arXiv:2106.08235},
  eprint = {arXiv:2106.08235},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.08235},
  urldate = {2022-06-09},
  abstract = {Transformer models have demonstrated superior performance in natural language processing. The dot product self-attention in Transformer allows us to model interactions between words. However, this modeling comes with significant computational overhead. In this work, we revisit the memory-compute trade-off associated with Transformer, particularly multi-head attention, and show a memory-heavy but significantly more compute-efficient alternative to Transformer. Our proposal, denoted as PairConnect, a multilayer perceptron (MLP), models the pairwise interaction between words by explicit pairwise word embeddings. As a result, PairConnect substitutes self dot product with a simple embedding lookup. We show mathematically that despite being an MLP, our compute-efficient PairConnect is strictly more expressive than Transformer. Our experiment on language modeling tasks suggests that PairConnect could achieve comparable results with Transformer while reducing the computational cost associated with inference significantly.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/GAM9WFVV/Xu et al. - 2021 - PairConnect A Compute-Efficient MLP Alternative t.pdf}
}

@online{xuWhatCanNeural2020,
  title = {What {{Can Neural Networks Reason About}}?},
  author = {Xu, Keyulu and Li, Jingling and Zhang, Mozhi and Du, Simon S. and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  date = {2020-02-15},
  number = {arXiv:1905.13211},
  eprint = {arXiv:1905.13211},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.13211},
  urldate = {2022-12-20},
  abstract = {Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/DM7R3GQK/Xu et al. - 2020 - What Can Neural Networks Reason About.pdf;/Users/lukakuma/Zotero/storage/TRPURHX4/1905.html}
}

@unpublished{xuZeroPromptScalingPromptBased2022,
  title = {{{ZeroPrompt}}: {{Scaling Prompt-Based Pretraining}} to 1,000 {{Tasks Improves Zero-Shot Generalization}}},
  shorttitle = {{{ZeroPrompt}}},
  author = {Xu, Hanwei and Chen, Yujun and Du, Yulun and Shao, Nan and Wang, Yanggang and Li, Haiyu and Yang, Zhilin},
  date = {2022-01-18},
  eprint = {2201.06910},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.06910},
  urldate = {2022-05-09},
  abstract = {We propose a multitask pretraining approach ZeroPrompt for zero-shot generalization, focusing on task scaling and zero-shot prompting. While previous models are trained on only a few dozen tasks, we scale to 1,000 tasks for the first time using real-world data. This leads to a crucial discovery that task scaling can be an efficient alternative to model scaling; i.e., the model size has little impact on performance with an extremely large number of tasks. Our results show that task scaling can substantially improve training efficiency by 30 times in FLOPs. Moreover, we present a prompting method that incorporates a genetic algorithm to automatically search for the best prompt for unseen tasks, along with a few other improvements. Empirically, ZeroPrompt substantially improves both the efficiency and the performance of zero-shot learning across a variety of academic and production datasets.},
  keywords = {(ext) Flamingo},
  file = {/Users/lukakuma/Zotero/storage/4GCGBV2N/Xu et al. - 2022 - ZeroPrompt Scaling Prompt-Based Pretraining to 1,.pdf;/Users/lukakuma/Zotero/storage/4F3IJQBN/2201.html}
}

@online{yangChineseCLIPContrastive2022,
  title = {Chinese {{CLIP}}: {{Contrastive Vision-Language Pretraining}} in {{Chinese}}},
  shorttitle = {Chinese {{CLIP}}},
  author = {Yang, An and Pan, Junshu and Lin, Junyang and Men, Rui and Zhang, Yichang and Zhou, Jingren and Zhou, Chang},
  date = {2022-11-03},
  number = {arXiv:2211.01335},
  eprint = {arXiv:2211.01335},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.01335},
  urldate = {2022-11-14},
  abstract = {The tremendous success of CLIP (Radford et al., 2021) has promoted the research and application of contrastive learning for vision-language pretraining. In this work, we construct a large-scale dataset of image-text pairs in Chinese, where most data are retrieved from publicly available datasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5 Chinese CLIP models of multiple sizes, spanning from 77 to 958 million parameters. Furthermore, we propose a two-stage pretraining method, where the model is first trained with the image encoder frozen and then trained with all parameters being optimized, to achieve enhanced model performance. Our comprehensive experiments demonstrate that Chinese CLIP can achieve the state-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups of zero-shot learning and finetuning, and it is able to achieve competitive performance in zero-shot image classification based on the evaluation on the ELEVATER benchmark (Li et al., 2022). We have released our codes, models, and demos in https://github.com/OFA-Sys/Chinese-CLIP},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/CWXQBXYJ/Yang et al. - 2022 - Chinese CLIP Contrastive Vision-Language Pretrain.pdf;/Users/lukakuma/Zotero/storage/XCXKTZI4/2211.html}
}

@unpublished{yangCollectiveIntelligenceInfrastructure2022,
  title = {Collective {{Intelligence}} as {{Infrastructure}} for {{Reducing Broad Global Catastrophic Risks}}},
  author = {Yang, Vicky Chuqiao and Sandberg, Anders},
  date = {2022-05-06},
  number = {arXiv:2205.03300},
  eprint = {2205.03300},
  eprinttype = {arxiv},
  eprintclass = {nlin, physics:physics},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.03300},
  urldate = {2022-05-31},
  abstract = {Academic and philanthropic communities have grown increasingly concerned with global catastrophic risks (GCRs), including artificial intelligence safety, pandemics, biosecurity, and nuclear war. Outcomes of many risk situations hinge on the performance of human groups, such as whether democratic governments and scientific communities can work effectively. We propose to think about these issues as Collective Intelligence (CI) problems -- of how to process distributed information effectively. CI is a transdisciplinary perspective, whose application involves humans and animal groups, markets, robotic swarms, collections of neurons, and other distributed systems. In this article, we argue that improving CI can improve general resilience against a wide variety of risks. Given the priority of GCR mitigation, CI research can benefit from developing concrete, practical applications to global risks. GCR researchers can benefit from engaging more with behavioral sciences. Behavioral researchers can benefit from recognizing an opportunity to impact critical social issues by engaging with these transdisciplinary efforts.},
  file = {/Users/lukakuma/Zotero/storage/JF6NLMSX/Yang and Sandberg - 2022 - Collective Intelligence as Infrastructure for Redu.pdf;/Users/lukakuma/Zotero/storage/WBVPETJH/2205.html}
}

@article{yangDiscoveryComplexOxides2021,
  title = {Discovery of Complex Oxides via Automated Experiments and Data Science},
  author = {Yang, Lusann and Haber, Joel A. and Armstrong, Zan and Yang, Samuel J. and Kan, Kevin and Zhou, Lan and Richter, Matthias H. and Roat, Christopher and Wagner, Nicholas and Coram, Marc and Berndl, Marc and Riley, Patrick and Gregoire, John M.},
  date = {2021-09-14},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {37},
  pages = {e2106042118},
  publisher = {{Proceedings of the National Academy of Sciences}},
  url = {https://www.pnas.org/doi/10.1073/pnas.2106042118},
  urldate = {2022-03-11},
  file = {/Users/lukakuma/Zotero/storage/UUFZR9YH/Yang et al. - 2021 - Discovery of complex oxides via automated experime.pdf}
}

@online{yangDOCImprovingLong2022,
  title = {{{DOC}}: {{Improving Long Story Coherence With Detailed Outline Control}}},
  shorttitle = {{{DOC}}},
  author = {Yang, Kevin and Klein, Dan and Peng, Nanyun and Tian, Yuandong},
  date = {2022-12-20},
  number = {arXiv:2212.10077},
  eprint = {arXiv:2212.10077},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.10077},
  urldate = {2023-02-20},
  abstract = {We propose the Detailed Outline Control (DOC) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. DOC consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more detailed, hierarchically structured outline, shifting creative burden from the main drafting procedure to the planning stage. The detailed controller ensures the more detailed outline is still respected during generation by controlling story passages to align with outline details. In human evaluations of automatically generated stories, DOC substantially outperforms a strong Re3 baseline (Yang et al., 2022) on plot coherence (22.5\% absolute gain), outline relevance (28.2\%), and interestingness (20.7\%). Humans also judged DOC to be much more controllable in an interactive generation setting.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/GUVW6XNA/Yang et al. - 2022 - DOC Improving Long Story Coherence With Detailed .pdf;/Users/lukakuma/Zotero/storage/8HXF4VJ3/2212.html}
}

@online{yangEmpiricalStudyGPT32021,
  title = {An {{Empirical Study}} of {{GPT-3}} for {{Few-Shot Knowledge-Based VQA}}},
  author = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
  date = {2021-09-10},
  number = {arXiv:2109.05014},
  eprint = {arXiv:2109.05014},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.05014},
  urldate = {2022-08-10},
  abstract = {Knowledge-based visual question answering (VQA) involves answering questions that require external knowledge not present in the image. Existing methods first retrieve knowledge from external resources, then reason over the selected knowledge, the input image, and question for answer prediction. However, this two-step approach could lead to mismatches that potentially limit the VQA performance. For example, the retrieved knowledge might be noisy and irrelevant to the question, and the re-embedded knowledge features during reasoning might deviate from their original meanings in the knowledge base (KB). To address this challenge, we propose PICa, a simple yet effective method that Prompts GPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by GPT-3's power in knowledge retrieval and question answering, instead of using structured KBs as in previous work, we treat GPT-3 as an implicit and unstructured KB that can jointly acquire and process relevant knowledge. Specifically, we first convert the image into captions (or tags) that GPT-3 can understand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just providing a few in-context VQA examples. We further boost performance by carefully investigating: (i) what text formats best describe the image content, and (ii) how in-context examples can be better selected and used. PICa unlocks the first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa surpasses the supervised state of the art by an absolute +8.6 points on the OK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent few-shot performance.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/WYA93PY3/Yang et al. - 2021 - An Empirical Study of GPT-3 for Few-Shot Knowledge.pdf;/Users/lukakuma/Zotero/storage/2BUSFK9T/2109.html}
}

@online{yangHotpotQADatasetDiverse2018,
  title = {{{HotpotQA}}: {{A Dataset}} for {{Diverse}}, {{Explainable Multi-hop Question Answering}}},
  shorttitle = {{{HotpotQA}}},
  author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
  date = {2018-09-25},
  number = {arXiv:1809.09600},
  eprint = {arXiv:1809.09600},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1809.09600},
  urldate = {2023-01-06},
  abstract = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/LJW8CJP2/Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi.pdf;/Users/lukakuma/Zotero/storage/PAG2N7V2/1809.html}
}

@online{yangRe3GeneratingLonger2022,
  title = {Re3: {{Generating Longer Stories With Recursive Reprompting}} and {{Revision}}},
  shorttitle = {Re3},
  author = {Yang, Kevin and Tian, Yuandong and Peng, Nanyun and Klein, Dan},
  date = {2022-10-13},
  number = {arXiv:2210.06774},
  eprint = {arXiv:2210.06774},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.06774},
  urldate = {2023-02-20},
  abstract = {We consider the problem of automatically generating longer stories of over two thousand words. Compared to prior work on shorter stories, long-range plot coherence and relevance are more central challenges here. We propose the Recursive Reprompting and Revision framework (Re3) to address these challenges by (a) prompting a general-purpose language model to construct a structured overarching plan, and (b) generating story passages by repeatedly injecting contextual information from both the plan and current story state into a language model prompt. We then revise by (c) reranking different continuations for plot coherence and premise relevance, and finally (d) editing the best continuation for factual consistency. Compared to similar-length stories generated directly from the same base model, human evaluators judged substantially more of Re3's stories as having a coherent overarching plot (by 14\% absolute increase), and relevant to the given initial premise (by 20\%).},
  pubstate = {preprint},
  version = {1},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/5XWFXU53/Yang et al. - 2022 - Re3 Generating Longer Stories With Recursive Repr.pdf;/Users/lukakuma/Zotero/storage/T9KKGGZI/2210.html}
}

@online{yangRepresentationMattersOffline2021,
  title = {Representation {{Matters}}: {{Offline Pretraining}} for {{Sequential Decision Making}}},
  shorttitle = {Representation {{Matters}}},
  author = {Yang, Mengjiao and Nachum, Ofir},
  date = {2021-02-10},
  number = {arXiv:2102.05815},
  eprint = {arXiv:2102.05815},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.05815},
  urldate = {2022-07-23},
  abstract = {The recent success of supervised learning methods on ever larger offline datasets has spurred interest in the reinforcement learning (RL) field to investigate whether the same paradigms can be translated to RL algorithms. This research area, known as offline RL, has largely focused on offline policy optimization, aiming to find a return-maximizing policy exclusively from offline data. In this paper, we consider a slightly different approach to incorporating offline data into sequential decision-making. We aim to answer the question, what unsupervised objectives applied to offline datasets are able to learn state representations which elevate performance on downstream tasks, whether those downstream tasks be online RL, imitation learning from expert demonstrations, or even offline policy optimization based on the same offline dataset? Through a variety of experiments utilizing standard offline RL datasets, we find that the use of pretraining with unsupervised learning objectives can dramatically improve the performance of policy learning algorithms that otherwise yield mediocre performance on their own. Extensive ablations further provide insights into what components of these unsupervised objectives -- e.g., reward prediction, continuous or discrete representations, pretraining or finetuning -- are most important and in which settings.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/NKAXZXPY/Yang and Nachum - 2021 - Representation Matters Offline Pretraining for Se.pdf;/Users/lukakuma/Zotero/storage/3EM43NWI/2102.html}
}

@online{yangSeqZeroFewshotCompositional2022,
  title = {{{SeqZero}}: {{Few-shot Compositional Semantic Parsing}} with {{Sequential Prompts}} and {{Zero-shot Models}}},
  shorttitle = {{{SeqZero}}},
  author = {Yang, Jingfeng and Jiang, Haoming and Yin, Qingyu and Zhang, Danqing and Yin, Bing and Yang, Diyi},
  date = {2022-05-15},
  number = {arXiv:2205.07381},
  eprint = {arXiv:2205.07381},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.07381},
  urldate = {2023-02-20},
  abstract = {Recent research showed promising results on combining pretrained language models (LMs) with canonical utterance for few-shot semantic parsing. The canonical utterance is often lengthy and complex due to the compositional structure of formal languages. Learning to generate such canonical utterance requires significant amount of data to reach high performance. Fine-tuning with only few-shot samples, the LMs can easily forget pretrained knowledge, overfit spurious biases, and suffer from compositionally out-of-distribution generalization errors. To tackle these issues, we propose a novel few-shot semantic parsing method -- SeqZero. SeqZero decomposes the problem into a sequence of sub-problems, which correspond to the sub-clauses of the formal language. Based on the decomposition, the LMs only need to generate short answers using prompts for predicting sub-clauses. Thus, SeqZero avoids generating a long canonical utterance at once. Moreover, SeqZero employs not only a few-shot model but also a zero-shot model to alleviate the overfitting. In particular, SeqZero brings out the merits from both models via ensemble equipped with our proposed constrained rescaling. SeqZero achieves SOTA performance of BART-based models on GeoQuery and EcommerceQuery, which are two few-shot datasets with compositional data split.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/BGMCSPVP/Yang et al. - 2022 - SeqZero Few-shot Compositional Semantic Parsing w.pdf;/Users/lukakuma/Zotero/storage/QRN2IHHZ/2205.html}
}

@online{yaoReActSynergizingReasoning2022,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  date = {2022-10-05},
  number = {arXiv:2210.03629},
  eprint = {arXiv:2210.03629},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.03629},
  urldate = {2022-11-17},
  abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
  pubstate = {preprint},
  keywords = {*general tools,5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/IK9ZBAKL/Yao et al. - 2022 - ReAct Synergizing Reasoning and Acting in Languag.pdf;/Users/lukakuma/Zotero/storage/7SYVYMS4/2210.html}
}

@online{yaoWebShopScalableRealWorld2023,
  title = {{{WebShop}}: {{Towards Scalable Real-World Web Interaction}} with {{Grounded Language Agents}}},
  shorttitle = {{{WebShop}}},
  author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  date = {2023-02-07},
  number = {arXiv:2207.01206},
  eprint = {arXiv:2207.01206},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2207.01206},
  urldate = {2023-02-20},
  abstract = {Existing benchmarks for grounding language in interactive environments either lack real-world linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals. To bridge this gap, we develop WebShop -- a simulated e-commerce website environment with \$1.18\$ million real-world products and \$12,087\$ crowd-sourced text instructions. Given a text instruction specifying a product requirement, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase an item. WebShop provides several challenges for language grounding including understanding compositional instructions, query (re-)formulation, comprehending and acting on noisy text in webpages, and performing strategic exploration. We collect over \$1,600\$ human demonstrations for the task, and train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models. Our best model achieves a task success rate of \$29\textbackslash\%\$, which outperforms rule-based heuristics (\$9.6\textbackslash\%\$) but is far lower than human expert performance (\$59\textbackslash\%\$). We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities. Finally, we show that agents trained on WebShop exhibit non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of WebShop in developing practical web-based agents that can operate in the wild.},
  pubstate = {preprint},
  keywords = {*general tools},
  file = {/Users/lukakuma/Zotero/storage/S8HFIF9A/Yao et al. - 2023 - WebShop Towards Scalable Real-World Web Interacti.pdf;/Users/lukakuma/Zotero/storage/5RJ8T88U/2207.html}
}

@online{yasunagaRetrievalAugmentedMultimodalLanguage2022,
  title = {Retrieval-{{Augmented Multimodal Language Modeling}}},
  author = {Yasunaga, Michihiro and Aghajanyan, Armen and Shi, Weijia and James, Rich and Leskovec, Jure and Liang, Percy and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  date = {2022-11-22},
  number = {arXiv:2211.12561},
  eprint = {arXiv:2211.12561},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.12561},
  urldate = {2022-11-24},
  abstract = {Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant knowledge fetched by a retriever from external memory (e.g., multimodal documents on the web). Specifically, we implement a retriever using the pretrained CLIP model and a generator using the CM3 Transformer architecture, and train this model using the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate mixtures of text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute for training ({$<$}30\% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities such as knowledge-intensive image generation and multimodal in-context learning.},
  pubstate = {preprint},
  keywords = {4-document retrieval,7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/E58B3PIK/Yasunaga et al. - 2022 - Retrieval-Augmented Multimodal Language Modeling.pdf;/Users/lukakuma/Zotero/storage/VTDDJERT/Yasunaga et al. - 2022 - Retrieval-Augmented Multimodal Language Modeling.pdf;/Users/lukakuma/Zotero/storage/F4VBK7CH/2211.html}
}

@unpublished{yeMasteringAtariGames2021,
  title = {Mastering {{Atari Games}} with {{Limited Data}}},
  author = {Ye, Weirui and Liu, Shaohuai and Kurutach, Thanard and Abbeel, Pieter and Gao, Yang},
  date = {2021-12-11},
  number = {arXiv:2111.00210},
  eprint = {2111.00210},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2111.00210},
  urldate = {2022-06-07},
  abstract = {Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3\% mean human performance and 109.0\% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.},
  keywords = {DeepMind},
  file = {/Users/lukakuma/Zotero/storage/ZNMTN8BM/Ye et al. - 2021 - Mastering Atari Games with Limited Data.pdf;/Users/lukakuma/Zotero/storage/77QLKYKV/2111.html}
}

@article{yeohSynBiopythonOpensourceSoftware2021,
  title = {{{SynBiopython}}: An Open-Source Software Library for {{Synthetic Biology}}},
  shorttitle = {{{SynBiopython}}},
  author = {Yeoh, Jing Wui and Swainston, Neil and Vegh, Peter and Zulkower, Valentin and Carbonell, Pablo and Holowko, Maciej B and Peddinti, Gopal and Poh, Chueh Loo},
  date = {2021-10-01},
  journaltitle = {Synthetic Biology},
  shortjournal = {Synthetic Biology},
  volume = {6},
  number = {1},
  pages = {ysab001},
  issn = {2397-7000},
  url = {https://doi.org/10.1093/synbio/ysab001},
  urldate = {2022-05-25},
  abstract = {Advances in hardware automation in synthetic biology laboratories are not yet fully matched by those of their software counterparts. Such automated laboratories, now commonly called biofoundries, require software solutions that would help with many specialized tasks such as batch DNA design, sample and data tracking, and data analysis, among others. Typically, many of the challenges facing biofoundries are shared, yet there is frequent wheel-reinvention where many labs develop similar software solutions in parallel. In this article, we present the first attempt at creating a standardized, open-source Python package. A number of tools will be integrated and developed that we envisage will become the obvious starting point for software development projects within biofoundries globally. Specifically, we describe the current state of available software, present usage scenarios and case studies for common problems, and finally describe plans for future development. SynBiopython is publicly available at the following address: http://synbiopython.org.},
  keywords = {bioeconomy},
  file = {/Users/lukakuma/Zotero/storage/M5NKDXDP/Yeoh et al. - 2021 - SynBiopython an open-source software library for .pdf;/Users/lukakuma/Zotero/storage/JWFJ3TUD/6146421.html}
}

@online{yeProGenProgressiveZeroshot2022,
  title = {{{ProGen}}: {{Progressive Zero-shot Dataset Generation}} via {{In-context Feedback}}},
  shorttitle = {{{ProGen}}},
  author = {Ye, Jiacheng and Gao, Jiahui and Feng, Jiangtao and Wu, Zhiyong and Yu, Tao and Kong, Lingpeng},
  date = {2022-10-21},
  number = {arXiv:2210.12329},
  eprint = {arXiv:2210.12329},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.12329},
  urldate = {2022-12-21},
  abstract = {Recently, dataset-generation-based zero-shot learning has shown promising results by training a task-specific model with a dataset synthesized from large pre-trained language models (PLMs). The final task-specific model often achieves compatible or even better performance than PLMs under the zero-shot setting, with orders of magnitude fewer parameters. However, synthetic datasets have their drawbacks. They have long been suffering from low-quality issues (e.g., low informativeness and redundancy). This explains why the massive synthetic data does not lead to better performance -- a scenario we would expect in the human-labeled data. To improve the quality of dataset synthesis, we propose a progressive zero-shot dataset generation framework, ProGen, which leverages the feedback from the task-specific model to guide the generation of new training data via in-context examples. Extensive experiments on five text classification datasets demonstrate the effectiveness of the proposed approach. We also show ProGen achieves on-par or superior performance with only 1\textbackslash\% synthetic dataset size compared to baseline methods without in-context feedback.},
  pubstate = {preprint},
  keywords = {read,student teacher},
  file = {/Users/lukakuma/Zotero/storage/Q936BV3Q/Ye et al. - 2022 - ProGen Progressive Zero-shot Dataset Generation v.pdf;/Users/lukakuma/Zotero/storage/R7ZWA74J/2210.html}
}

@unpublished{yingHierarchicalGraphRepresentation2019,
  title = {Hierarchical {{Graph Representation Learning}} with {{Differentiable Pooling}}},
  author = {Ying, Rex and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
  date = {2019-02-20},
  number = {arXiv:1806.08804},
  eprint = {1806.08804},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1806.08804},
  urldate = {2022-06-13},
  abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
  file = {/Users/lukakuma/Zotero/storage/H5LIZ4EB/Ying et al. - 2019 - Hierarchical Graph Representation Learning with Di.pdf;/Users/lukakuma/Zotero/storage/VRJ9N65E/1806.html}
}

@inproceedings{yongAdaptingBigScienceMultilingual2022,
  title = {Adapting {{BigScience Multilingual Model}} to {{Unseen Languages}}},
  author = {Yong, Zheng Xin and Nikoulina, Vassilina},
  date = {2022-04-10},
  url = {https://openreview.net/forum?id=rL7mI3GSIbq},
  urldate = {2022-10-30},
  abstract = {We benchmark different strategies of adding new languages (German and Korean) into the BigScience's pretrained multilingual language model with 1.3 billion parameters that currently supports 13 languages. We investigate the factors that affect the language adaptability of the model and the trade-offs between computational costs and expected performance.},
  eventtitle = {Challenges \{\textbackslash\&\vphantom\}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/NEX7CKJZ/Yong and Nikoulina - 2022 - Adapting BigScience Multilingual Model to Unseen L.pdf;/Users/lukakuma/Zotero/storage/JBD2NX5B/forum.html}
}

@online{yooScalableTrainingLanguage2022,
  title = {Scalable {{Training}} of {{Language Models}} Using {{JAX}} Pjit and {{TPUv4}}},
  author = {Yoo, Joanna and Perlin, Kuba and Kamalakara, Siddhartha Rao and Ara\'ujo, Jo\~ao G. M.},
  date = {2022-04-13},
  number = {arXiv:2204.06514},
  eprint = {arXiv:2204.06514},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.06514},
  urldate = {2022-07-27},
  abstract = {Modern large language models require distributed training strategies due to their size. The challenges of efficiently and robustly training them are met with rapid developments on both software and hardware frontiers. In this technical report, we explore challenges and design decisions associated with developing a scalable training framework, and present a quantitative analysis of efficiency improvements coming from adopting new software and hardware solutions.},
  pubstate = {preprint},
  keywords = {distributed training,read},
  file = {/Users/lukakuma/Zotero/storage/9AYJLIL5/Yoo et al. - 2022 - Scalable Training of Language Models using JAX pji.pdf;/Users/lukakuma/Zotero/storage/UWYWYJLT/2204.html}
}

@unpublished{youDesignSpaceGraph2021,
  title = {Design {{Space}} for {{Graph Neural Networks}}},
  author = {You, Jiaxuan and Ying, Rex and Leskovec, Jure},
  date = {2021-07-23},
  eprint = {2011.08843},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.08843},
  urldate = {2022-03-23},
  abstract = {The rapid evolution of Graph Neural Networks (GNNs) has led to a growing number of new architectures as well as novel applications. However, current research focuses on proposing and evaluating specific architectural designs of GNNs, as opposed to studying the more general design space of GNNs that consists of a Cartesian product of different design dimensions, such as the number of layers or the type of the aggregation function. Additionally, GNN designs are often specialized to a single task, yet few efforts have been made to understand how to quickly find the best GNN design for a novel task or a novel dataset. Here we define and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks. Our approach features three key innovations: (1) A general GNN design space; (2) a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture; (3) an efficient and effective design space evaluation method which allows insights to be distilled from a huge number of model-task combinations. Our key results include: (1) A comprehensive set of guidelines for designing well-performing GNNs; (2) while best GNN designs for different tasks vary significantly, the GNN task space allows for transferring the best designs across different tasks; (3) models discovered using our design space achieve state-of-the-art performance. Overall, our work offers a principled and scalable approach to transition from studying individual GNN designs for specific tasks, to systematically studying the GNN design space and the task space. Finally, we release GraphGym, a powerful platform for exploring different GNN designs and tasks. GraphGym features modularized GNN implementation, standardized GNN evaluation, and reproducible and scalable experiment management.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/38ANHMAH/You et al. - 2021 - Design Space for Graph Neural Networks.pdf;/Users/lukakuma/Zotero/storage/EPFLIW47/2011.html}
}

@online{yuALERTAdaptingLanguage2022,
  title = {{{ALERT}}: {{Adapting Language Models}} to {{Reasoning Tasks}}},
  shorttitle = {{{ALERT}}},
  author = {Yu, Ping and Wang, Tianlu and Golovneva, Olga and Alkhamissy, Badr and Ghosh, Gargi and Diab, Mona and Celikyilmaz, Asli},
  date = {2022-12-16},
  number = {arXiv:2212.08286},
  eprint = {arXiv:2212.08286},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.08286},
  urldate = {2023-02-20},
  abstract = {Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during finetuning stage compared to pretraining state. We also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/YTFDQB5S/Yu et al. - 2022 - ALERT Adapting Language Models to Reasoning Tasks.pdf;/Users/lukakuma/Zotero/storage/K4DDWDGC/2212.html}
}

@unpublished{yuanDecentralizedTrainingFoundation2022,
  title = {Decentralized {{Training}} of {{Foundation Models}} in {{Heterogeneous Environments}}},
  author = {Yuan, Binhang and He, Yongjun and Davis, Jared Quincy and Zhang, Tianyi and Dao, Tri and Chen, Beidi and Liang, Percy and Re, Christopher and Zhang, Ce},
  date = {2022-06-02},
  number = {arXiv:2206.01288},
  eprint = {2206.01288},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.01288},
  urldate = {2022-06-09},
  abstract = {Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational "tasklets" in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8X faster than prior state-of-the-art training systems (Megatron).},
  keywords = {*open source AI,read},
  file = {/Users/lukakuma/Zotero/storage/IZZBAXX8/Yuan et al. - 2022 - Decentralized Training of Foundation Models in Het.pdf;/Users/lukakuma/Zotero/storage/SXUCINTA/2206.html}
}

@unpublished{yuanRoadmapBigModel2022,
  title = {A {{Roadmap}} for {{Big Model}}},
  author = {Yuan, Sha and Zhao, Hanyu and Zhao, Shuai and Leng, Jiahong and Liang, Yangxiao and Wang, Xiaozhi and Yu, Jifan and Lv, Xin and Shao, Zhou and He, Jiaao and Lin, Yankai and Han, Xu and Liu, Zhenghao and Ding, Ning and Rao, Yongming and Gao, Yizhao and Zhang, Liang and Ding, Ming and Fang, Cong and Wang, Yisen and Long, Mingsheng and Zhang, Jing and Dong, Yinpeng and Pang, Tianyu and Cui, Peng and Huang, Lingxiao and Liang, Zheng and Shen, Huawei and Zhang, Hui and Zhang, Quanshi and Dong, Qingxiu and Tan, Zhixing and Wang, Mingxuan and Wang, Shuo and Zhou, Long and Li, Haoran and Bao, Junwei and Pan, Yingwei and Zhang, Weinan and Yu, Zhou and Yan, Rui and Shi, Chence and Xu, Minghao and Zhang, Zuobai and Wang, Guoqiang and Pan, Xiang and Li, Mengjie and Chu, Xiaoyu and Yao, Zijun and Zhu, Fangwei and Cao, Shulin and Xue, Weicheng and Ma, Zixuan and Zhang, Zhengyan and Hu, Shengding and Qin, Yujia and Xiao, Chaojun and Zeng, Zheni and Cui, Ganqu and Chen, Weize and Zhao, Weilin and Yao, Yuan and Li, Peng and Zheng, Wenzhao and Zhao, Wenliang and Wang, Ziyi and Zhang, Borui and Fei, Nanyi and Hu, Anwen and Ling, Zenan and Li, Haoyang and Cao, Boxi and Han, Xianpei and Zhan, Weidong and Chang, Baobao and Sun, Hao and Deng, Jiawen and Li, Juanzi and Hou, Lei and Cao, Xigang and Zhai, Jidong and Liu, Zhiyuan and Sun, Maosong and Lu, Jiwen and Lu, Zhiwu and Jin, Qin and Song, Ruihua and Wen, Ji-Rong and Lin, Zhouchen and Wang, Liwei and Su, Hang and Zhu, Jun and Sui, Zhifang and Zhang, Jiajun and Liu, Yang and He, Xiaodong and Huang, Minlie and Tang, Jian and Tang, Jie},
  date = {2022-03-26},
  eprint = {2203.14101},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.14101},
  urldate = {2022-03-30},
  abstract = {With the rapid development of deep learning, training Big Models (BMs) for multiple downstream tasks becomes a popular paradigm. Researchers have achieved various outcomes in the construction of BMs and the BM application in many fields. At present, there is a lack of research work that sorts out the overall progress of BMs and guides the follow-up research. In this paper, we cover not only the BM technologies themselves but also the prerequisites for BM training and applications with BMs, dividing the BM review into four parts: Resource, Models, Key Technologies and Application. We introduce 16 specific BM-related topics in those four parts, they are Data, Knowledge, Computing System, Parallel Training System, Language Model, Vision Model, Multi-modal Model, Theory\&Interpretability, Commonsense Reasoning, Reliability\&Security, Governance, Evaluation, Machine Translation, Text Generation, Dialogue and Protein Research. In each topic, we summarize clearly the current studies and propose some future research directions. At the end of this paper, we conclude the further development of BMs in a more general view.},
  file = {/Users/lukakuma/Zotero/storage/WASUFGDT/Yuan et al. - 2022 - A Roadmap for Big Model.pdf;/Users/lukakuma/Zotero/storage/YWI8UAL9/2203.html}
}

@online{yuceStructuredDictionaryPerspective2022,
  title = {A {{Structured Dictionary Perspective}} on {{Implicit Neural Representations}}},
  author = {Y\"uce, Gizem and Ortiz-Jim\'enez, Guillermo and Besbinar, Beril and Frossard, Pascal},
  date = {2022-03-25},
  number = {arXiv:2112.01917},
  eprint = {arXiv:2112.01917},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.01917},
  urldate = {2022-08-29},
  abstract = {Implicit neural representations (INRs) have recently emerged as a promising alternative to classical discretized representations of signals. Nevertheless, despite their practical success, we still do not understand how INRs represent signals. We propose a novel unified perspective to theoretically analyse INRs. Leveraging results from harmonic analysis and deep learning theory, we show that most INR families are analogous to structured signal dictionaries whose atoms are integer harmonics of the set of initial mapping frequencies. This structure allows INRs to express signals with an exponentially increasing frequency support using a number of parameters that only grows linearly with depth. We also explore the inductive bias of INRs exploiting recent results about the empirical neural tangent kernel (NTK). Specifically, we show that the eigenfunctions of the NTK can be seen as dictionary atoms whose inner product with the target signal determines the final performance of their reconstruction. In this regard, we reveal that meta-learning has a reshaping effect on the NTK analogous to dictionary learning, building dictionary atoms as a combination of the examples seen during meta-training. Our results permit to design and tune novel INR architectures, but can also be of interest for the wider deep learning theory community.},
  pubstate = {preprint},
  keywords = {functa},
  file = {/Users/lukakuma/Zotero/storage/U5T9BJ4Z/Y√ºce et al. - 2022 - A Structured Dictionary Perspective on Implicit Ne.pdf;/Users/lukakuma/Zotero/storage/B8J5XRMH/2112.html}
}

@unpublished{yuCoCaContrastiveCaptioners2022,
  title = {{{CoCa}}: {{Contrastive Captioners}} Are {{Image-Text Foundation Models}}},
  shorttitle = {{{CoCa}}},
  author = {Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  date = {2022-05-04},
  eprint = {2205.01917},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.01917},
  urldate = {2022-05-05},
  abstract = {Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3\% zero-shot top-1 accuracy, 90.6\% with a frozen encoder and learned classification head, and new state-of-the-art 91.0\% top-1 accuracy on ImageNet with a finetuned encoder.},
  keywords = {(ext) Flamingo,7-multimodal,read},
  file = {/Users/lukakuma/Zotero/storage/4XJ6HQ2C/Yu et al. - 2022 - CoCa Contrastive Captioners are Image-Text Founda.pdf;/Users/lukakuma/Zotero/storage/S9K5ZZ9B/2205.html}
}

@inproceedings{yuHowLeverageUnlabeled2022,
  title = {How to {{Leverage Unlabeled Data}} in {{Offline Reinforcement Learning}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Yu, Tianhe and Kumar, Aviral and Chebotar, Yevgen and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  date = {2022-06-28},
  pages = {25611--25635},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/yu22c.html},
  urldate = {2022-08-02},
  abstract = {Offline reinforcement learning (RL) can learn control policies from static datasets but, like standard RL methods, it requires reward annotations for every transition. In many cases, labeling large datasets with rewards may be costly, especially if those rewards must be provided by human labelers, while collecting diverse unlabeled data might be comparatively inexpensive. How can we best leverage such unlabeled data in offline RL? One natural solution is to learn a reward function from the labeled data and use it to label the unlabeled data. In this paper, we find that, perhaps surprisingly, a much simpler method that simply applies zero rewards to unlabeled data leads to effective data sharing both in theory and in practice, without learning any reward model at all. While this approach might seem strange (and incorrect) at first, we provide extensive theoretical and empirical analysis that illustrates how it trades off reward bias, sample complexity and distributional shift, often leading to good results. We characterize conditions under which this simple strategy is effective, and further show that extending it with a simple reweighting approach can further alleviate the bias introduced by using incorrect reward labels. Our empirical evaluation confirms these findings in simulated robotic locomotion, navigation, and manipulation settings.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/ZJXZ45UB/Yu et al. - 2022 - How to Leverage Unlabeled Data in Offline Reinforc.pdf}
}

@online{yuLegalPromptingTeaching2022,
  title = {Legal {{Prompting}}: {{Teaching}} a {{Language Model}} to {{Think Like}} a {{Lawyer}}},
  shorttitle = {Legal {{Prompting}}},
  author = {Yu, Fangyi and Quartey, Lee and Schilder, Frank},
  date = {2022-12-08},
  number = {arXiv:2212.01326},
  eprint = {arXiv:2212.01326},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2212.01326},
  urldate = {2023-02-02},
  abstract = {Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought (CoT) prompts can improve arithmetic or common sense tasks significantly. We explore how such approaches fare with legal reasoning tasks and take the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning approaches. Our findings show that while CoT prompting and fine-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from specific legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/STA2DVGJ/Yu et al. - 2022 - Legal Prompting Teaching a Language Model to Thin.pdf;/Users/lukakuma/Zotero/storage/435PGFHU/2212.html}
}

@unpublished{yuMetaFormerActuallyWhat2021,
  title = {{{MetaFormer}} Is {{Actually What You Need}} for {{Vision}}},
  author = {Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  date = {2021-11-29},
  eprint = {2111.11418},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.11418},
  urldate = {2022-05-17},
  abstract = {Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only the most basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1\% top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3\%/1.1\% accuracy with 35\%/52\% fewer parameters and 48\%/60\% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of "MetaFormer", a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design. Code is available at https://github.com/sail-sg/poolformer},
  file = {/Users/lukakuma/Zotero/storage/WXFPHPY4/Yu et al. - 2021 - MetaFormer is Actually What You Need for Vision.pdf}
}

@online{yuScalingAutoregressiveModels2022,
  title = {Scaling {{Autoregressive Models}} for {{Content-Rich Text-to-Image Generation}}},
  author = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},
  date = {2022-06-21},
  number = {arXiv:2206.10789},
  eprint = {arXiv:2206.10789},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.10789},
  urldate = {2022-08-09},
  abstract = {We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/TZHLCFEL/Yu et al. - 2022 - Scaling Autoregressive Models for Content-Rich Tex.pdf;/Users/lukakuma/Zotero/storage/Z76WBRQT/Yu et al. - 2022 - Scaling Autoregressive Models for Content-Rich Tex.pdf;/Users/lukakuma/Zotero/storage/58XQYNZD/2206.html}
}

@article{zaadnoordijkLessonsInfantLearning2022,
  title = {Lessons from Infant Learning for Unsupervised Machine Learning},
  author = {Zaadnoordijk, Lorijn and Besold, Tarek R. and Cusack, Rhodri},
  date = {2022-06},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {4},
  number = {6},
  pages = {510--520},
  issn = {2522-5839},
  url = {https://www.nature.com/articles/s42256-022-00488-2},
  urldate = {2022-07-21},
  langid = {english},
  keywords = {read},
  file = {/Users/lukakuma/Zotero/storage/C7QR7SHJ/Zaadnoordijk et al. - 2022 - Lessons from infant learning for unsupervised mach.pdf}
}

@online{zamaniConversationalInformationSeeking2022,
  title = {Conversational {{Information Seeking}}},
  author = {Zamani, Hamed and Trippas, Johanne R. and Dalton, Jeff and Radlinski, Filip},
  date = {2022-01-21},
  number = {arXiv:2201.08808},
  eprint = {arXiv:2201.08808},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.08808},
  urldate = {2022-12-27},
  abstract = {Conversational information seeking (CIS) is concerned with a sequence of interactions between one or more users and an information system. Interactions in CIS are primarily based on natural language dialogue, while they may include other types of interactions, such as click, touch, and body gestures. This monograph provides a thorough overview of CIS definitions, applications, interactions, interfaces, design, implementation, and evaluation. This monograph views CIS applications as including conversational search, conversational question answering, and conversational recommendation. Our aim is to provide an overview of past research related to CIS, introduce the current state-of-the-art in CIS, highlight the challenges still being faced in the community. and suggest future directions.},
  pubstate = {preprint},
  keywords = {(ext) Sparrow},
  file = {/Users/lukakuma/Zotero/storage/EZSU3TBJ/Zamani et al. - 2022 - Conversational Information Seeking.pdf;/Users/lukakuma/Zotero/storage/XTWUNTJ2/2201.html}
}

@unpublished{zbontarBarlowTwinsSelfSupervised2021,
  title = {Barlow {{Twins}}: {{Self-Supervised Learning}} via {{Redundancy Reduction}}},
  shorttitle = {Barlow {{Twins}}},
  author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St\'ephane},
  date = {2021-06-14},
  eprint = {2103.03230},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2103.03230},
  urldate = {2022-04-25},
  abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
  file = {/Users/lukakuma/Zotero/storage/VL47PHKF/Zbontar et al. - 2021 - Barlow Twins Self-Supervised Learning via Redunda.pdf;/Users/lukakuma/Zotero/storage/XKNW8XNF/2103.html}
}

@online{zelikmanSTaRBootstrappingReasoning2022a,
  title = {{{STaR}}: {{Bootstrapping Reasoning With Reasoning}}},
  shorttitle = {{{STaR}}},
  author = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah D.},
  date = {2022-05-20},
  number = {arXiv:2203.14465},
  eprint = {arXiv:2203.14465},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.14465},
  urldate = {2022-11-17},
  abstract = {Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30\$\textbackslash times\$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.},
  pubstate = {preprint},
  keywords = {5-reasoning,self-play},
  file = {/Users/lukakuma/Zotero/storage/6XLWLH5G/Zelikman et al. - 2022 - STaR Bootstrapping Reasoning With Reasoning.pdf;/Users/lukakuma/Zotero/storage/BX3YAZD9/2203.html}
}

@unpublished{zellersPIGLeTLanguageGrounding2022,
  title = {{{PIGLeT}}: {{Language Grounding Through Neuro-Symbolic Interaction}} in a {{3D World}}},
  shorttitle = {{{PIGLeT}}},
  author = {Zellers, Rowan and Holtzman, Ari and Peters, Matthew and Mottaghi, Roozbeh and Kembhavi, Aniruddha and Farhadi, Ali and Choi, Yejin},
  date = {2022-01-30},
  eprint = {2106.00188},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.00188},
  urldate = {2022-03-08},
  abstract = {We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. We factorize PIGLeT into a physical dynamics model, and a separate language model. Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don't. We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. PIGLeT can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language. Experimental results show that our model effectively learns world dynamics, along with how to communicate them. It is able to correctly forecast "what happens next" given an English sentence over 80\% of the time, outperforming a 100x larger, text-to-text approach by over 10\%. Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives. We present comprehensive analysis showing room for future work.},
  file = {/Users/lukakuma/Zotero/storage/B36U4TAS/Zellers et al. - 2022 - PIGLeT Language Grounding Through Neuro-Symbolic .pdf;/Users/lukakuma/Zotero/storage/R5DGWMT9/2106.html}
}

@online{zengGLM130BOpenBilingual2022,
  title = {{{GLM-130B}}: {{An Open Bilingual Pre-trained Model}}},
  shorttitle = {{{GLM-130B}}},
  author = {Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and Tam, Weng Lam and Ma, Zixuan and Xue, Yufei and Zhai, Jidong and Chen, Wenguang and Zhang, Peng and Dong, Yuxiao and Tang, Jie},
  date = {2022-10-05},
  number = {arXiv:2210.02414},
  eprint = {arXiv:2210.02414},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.02414},
  urldate = {2022-10-11},
  abstract = {We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4\$\textbackslash times\$RTX 3090 (24G) or 8\$\textbackslash times\$RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B .},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/RDCU6NBD/Zeng et al. - 2022 - GLM-130B An Open Bilingual Pre-trained Model.pdf;/Users/lukakuma/Zotero/storage/9GTYCFMA/2210.html}
}

@online{zengSocraticModelsComposing2022,
  title = {Socratic {{Models}}: {{Composing Zero-Shot Multimodal Reasoning}} with {{Language}}},
  shorttitle = {Socratic {{Models}}},
  author = {Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
  date = {2022-05-27},
  number = {arXiv:2204.00598},
  eprint = {arXiv:2204.00598},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.00598},
  urldate = {2023-02-20},
  abstract = {Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.},
  pubstate = {preprint},
  keywords = {action/digital},
  file = {/Users/lukakuma/Zotero/storage/K397DXX4/Zeng et al. - 2022 - Socratic Models Composing Zero-Shot Multimodal Re.pdf;/Users/lukakuma/Zotero/storage/CBMTLPMY/2204.html}
}

@online{ZeROInferenceDemocratizingMassive2022,
  title = {{{ZeRO-Inference}}: {{Democratizing}} Massive Model Inference},
  shorttitle = {{{ZeRO-Inference}}},
  date = {2022-09-09T17:09:00-07:00},
  url = {https://www.deepspeed.ai/2022/09/09/zero-inference.html},
  urldate = {2022-09-16},
  abstract = {DeepSpeed is a deep learning optimization library that makes distributed training easy, efficient, and effective.},
  langid = {english},
  organization = {{DeepSpeed}},
  file = {/Users/lukakuma/Zotero/storage/MLG9FJMY/zero-inference.html}
}

@online{zhaiLiTZeroShotTransfer2022,
  title = {{{LiT}}: {{Zero-Shot Transfer}} with {{Locked-image}} Text {{Tuning}}},
  shorttitle = {{{LiT}}},
  author = {Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  date = {2022-06-22},
  number = {arXiv:2111.07991},
  eprint = {arXiv:2111.07991},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.07991},
  urldate = {2023-02-07},
  abstract = {This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning "Locked-image Tuning" (LiT), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2\% zero-shot transfer accuracy on the ImageNet test set, and 82.5\% on the challenging out-of-distribution ObjectNet test set.},
  pubstate = {preprint},
  keywords = {2-adapter,7-multimodal},
  file = {/Users/lukakuma/Zotero/storage/ZCL5XI28/Zhai et al. - 2022 - LiT Zero-Shot Transfer with Locked-image text Tun.pdf;/Users/lukakuma/Zotero/storage/46W7ME24/2111.html}
}

@online{zhangAddingConditionalControl2023,
  title = {Adding {{Conditional Control}} to {{Text-to-Image Diffusion Models}}},
  author = {Zhang, Lvmin and Agrawala, Maneesh},
  date = {2023-02-10},
  number = {arXiv:2302.05543},
  eprint = {arXiv:2302.05543},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.05543},
  urldate = {2023-02-20},
  abstract = {We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small ({$<$} 50k). Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices. Alternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data. We report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc. This may enrich the methods to control large diffusion models and further facilitate related applications.},
  pubstate = {preprint},
  keywords = {*diffusion,2-adapter,image generation},
  file = {/Users/lukakuma/Zotero/storage/9HIQ3ZDS/Zhang and Agrawala - 2023 - Adding Conditional Control to Text-to-Image Diffus.pdf;/Users/lukakuma/Zotero/storage/SHQI5FUV/2302.html}
}

@online{zhangAutomaticChainThought2022,
  title = {Automatic {{Chain}} of {{Thought Prompting}} in {{Large Language Models}}},
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  date = {2022-10-07},
  number = {arXiv:2210.03493},
  eprint = {arXiv:2210.03493},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.03493},
  urldate = {2022-10-25},
  abstract = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
  pubstate = {preprint},
  keywords = {5-reasoning},
  file = {/Users/lukakuma/Zotero/storage/UDQIQE9U/Zhang et al. - 2022 - Automatic Chain of Thought Prompting in Large Lang.pdf;/Users/lukakuma/Zotero/storage/HS9QVNNI/2210.html}
}

@unpublished{zhangCotrainingTransformerVideos2021,
  title = {Co-Training {{Transformer}} with {{Videos}} and {{Images Improves Action Recognition}}},
  author = {Zhang, Bowen and Yu, Jiahui and Fifty, Christopher and Han, Wei and Dai, Andrew M. and Pang, Ruoming and Sha, Fei},
  date = {2021-12-14},
  eprint = {2112.07175},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.07175},
  urldate = {2022-04-29},
  abstract = {In learning action recognition, models are typically pre-trained on object recognition with images, such as ImageNet, and later fine-tuned on target action recognition with videos. This approach has achieved good empirical performance especially with recent transformer-based video architectures. While recently many works aim to design more advanced transformer architectures for action recognition, less effort has been made on how to train video transformers. In this work, we explore several training paradigms and present two findings. First, video transformers benefit from joint training on diverse video datasets and label spaces (e.g., Kinetics is appearance-focused while SomethingSomething is motion-focused). Second, by further co-training with images (as single-frame videos), the video transformers learn even better video representations. We term this approach as Co-training Videos and Images for Action Recognition (CoVeR). In particular, when pretrained on ImageNet-21K based on the TimeSFormer architecture, CoVeR improves Kinetics-400 Top-1 Accuracy by 2.4\%, Kinetics-600 by 2.3\%, and SomethingSomething-v2 by 2.3\%. When pretrained on larger-scale image datasets following previous state-of-the-art, CoVeR achieves best results on Kinetics-400 (87.2\%), Kinetics-600 (87.9\%), Kinetics-700 (79.8\%), SomethingSomething-v2 (70.9\%), and Moments-in-Time (46.1\%), with a simple spatio-temporal video transformer.},
  file = {/Users/lukakuma/Zotero/storage/BQV8DV3P/Zhang et al. - 2021 - Co-training Transformer with Videos and Images Imp.pdf;/Users/lukakuma/Zotero/storage/X8A6IFS7/2112.html}
}

@unpublished{zhangCrossModalContrastiveLearning2021,
  title = {Cross-{{Modal Contrastive Learning}} for {{Text-to-Image Generation}}},
  author = {Zhang, Han and Koh, Jing Yu and Baldridge, Jason and Lee, Honglak and Yang, Yinfei},
  date = {2021-06-09},
  eprint = {2101.04702},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2101.04702},
  urldate = {2022-03-11},
  abstract = {The output of text-to-image synthesis systems should be coherent, clear, photo-realistic scenes with high semantic fidelity to their conditioned text descriptions. Our Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN) addresses this challenge by maximizing the mutual information between image and text. It does this via multiple contrastive losses which capture inter-modality and intra-modality correspondences. XMC-GAN uses an attentional self-modulation generator, which enforces strong text-image correspondence, and a contrastive discriminator, which acts as a critic as well as a feature encoder for contrastive learning. The quality of XMC-GAN's output is a major step up from previous models, as we show on three challenging datasets. On MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33, but--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1 for image-text alignment, compared to three other recent models. XMC-GAN also generalizes to the challenging Localized Narratives dataset (which has longer, more detailed descriptions), improving state-of-the-art FID from 48.70 to 14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images data, establishing a strong benchmark FID score of 26.91.},
  file = {/Users/lukakuma/Zotero/storage/QWHLR3SG/Zhang et al. - 2021 - Cross-Modal Contrastive Learning for Text-to-Image.pdf;/Users/lukakuma/Zotero/storage/TR3RSP4S/2101.html}
}

@unpublished{zhangGenerativeFlowNetworks2022,
  title = {Generative {{Flow Networks}} for {{Discrete Probabilistic Modeling}}},
  author = {Zhang, Dinghuai and Malkin, Nikolay and Liu, Zhen and Volokhova, Alexandra and Courville, Aaron and Bengio, Yoshua},
  date = {2022-02-02},
  eprint = {2202.01361},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2202.01361},
  urldate = {2022-05-10},
  abstract = {We present energy-based generative flow networks (EB-GFN), a novel probabilistic modeling algorithm for high-dimensional discrete data. Building upon the theory of generative flow networks (GFlowNets), we model the generation process by a stochastic data construction policy and thus amortize expensive MCMC exploration into a fixed number of actions sampled from a GFlowNet. We show how GFlowNets can approximately perform large-block Gibbs sampling to mix between modes. We propose a framework to jointly train a GFlowNet with an energy function, so that the GFlowNet learns to sample from the energy distribution, while the energy learns with an approximate MLE objective with negative samples from the GFlowNet. We demonstrate EB-GFN's effectiveness on various probabilistic modeling tasks.},
  keywords = {GFlowNet},
  file = {/Users/lukakuma/Zotero/storage/ULNYZUPB/Zhang et al. - 2022 - Generative Flow Networks for Discrete Probabilisti.pdf;/Users/lukakuma/Zotero/storage/LWAFRNTF/2202.html}
}

@unpublished{zhangGreaseLMGraphREASoning2022,
  title = {{{GreaseLM}}: {{Graph REASoning Enhanced Language Models}} for {{Question Answering}}},
  shorttitle = {{{GreaseLM}}},
  author = {Zhang, Xikun and Bosselut, Antoine and Yasunaga, Michihiro and Ren, Hongyu and Liang, Percy and Manning, Christopher D. and Leskovec, Jure},
  date = {2022-01-21},
  eprint = {2201.08860},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.08860},
  urldate = {2022-04-25},
  abstract = {Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.},
  keywords = {QA},
  file = {/Users/lukakuma/Zotero/storage/BP76MT68/Zhang et al. - 2022 - GreaseLM Graph REASoning Enhanced Language Models.pdf;/Users/lukakuma/Zotero/storage/N26GITZ2/2201.html}
}

@online{zhangMiCSNearlinearScaling2022,
  title = {{{MiCS}}: {{Near-linear Scaling}} for {{Training Gigantic Model}} on {{Public Cloud}}},
  shorttitle = {{{MiCS}}},
  author = {Zhang, Zhen and Zheng, Shuai and Wang, Yida and Chiu, Justin and Karypis, George and Chilimbi, Trishul and Li, Mu and Jin, Xin},
  date = {2022-05-24},
  number = {arXiv:2205.00119},
  eprint = {arXiv:2205.00119},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.00119},
  urldate = {2022-10-11},
  abstract = {Existing general purpose frameworks for gigantic model training, i.e., models with billions to trillions of parameters, cannot scale efficiently on public cloud environments due to large communication overheads. In this paper, we propose MiCS, which Minimizes the Communication Scale to bring down communication overhead. Specifically, by decreasing the number of participants in a communication collective, MiCS can utilize existing heterogeneous network bandwidth on the cloud, reduce network traffic over slower links, and amortize expensive global gradient synchronization overheads. Our evaluation on AWS shows that the system throughput of MiCS is up to 2.89\$\textbackslash times\$ that of the state-of-the-art large model training systems. MiCS achieves near-linear scaling efficiency, which is up to 1.27\$\textbackslash times\$ that of DeepSpeed. MiCS allows us to train a proprietary model with 100 billion parameters on 512 GPUs with 99.4\% weak-scaling efficiency, and it is able to saturate over 54.5\% theoretical computation power of each GPU on a public cloud with less GPU memory and more restricted networks than DGX-A100 clusters.},
  pubstate = {preprint},
  keywords = {distributed training,read},
  file = {/Users/lukakuma/Zotero/storage/PR9E4RVQ/Zhang et al. - 2022 - MiCS Near-linear Scaling for Training Gigantic Mo.pdf;/Users/lukakuma/Zotero/storage/DWUJM6IK/2205.html}
}

@unpublished{zhangMorphMLPSelfAttentionFree2021,
  title = {{{MorphMLP}}: {{A Self-Attention Free}}, {{MLP-Like Backbone}} for {{Image}} and {{Video}}},
  shorttitle = {{{MorphMLP}}},
  author = {Zhang, David Junhao and Li, Kunchang and Chen, Yunpeng and Wang, Yali and Chandra, Shashwat and Qiao, Yu and Liu, Luoqi and Shou, Mike Zheng},
  date = {2021-11-24},
  number = {arXiv:2111.12527},
  eprint = {2111.12527},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2111.12527},
  urldate = {2022-06-09},
  abstract = {Self-attention has become an integral component of the recent network architectures, e.g., Transformer, that dominate major image and video benchmarks. This is because self-attention can flexibly model long-range information. For the same reason, researchers make attempts recently to revive Multiple Layer Perceptron (MLP) and propose a few MLP-Like architectures, showing great potential. However, the current MLP-Like architectures are not good at capturing local details and lack progressive understanding of core details in the images and/or videos. To overcome this issue, we propose a novel MorphMLP architecture that focuses on capturing local details at the low-level layers, while gradually changing to focus on long-term modeling at the high-level layers. Specifically, we design a Fully-Connected-Like layer, dubbed as MorphFC, of two morphable filters that gradually grow its receptive field along the height and width dimension. More interestingly, we propose to flexibly adapt our MorphFC layer in the video domain. To our best knowledge, we are the first to create a MLP-Like backbone for learning video representation. Finally, we conduct extensive experiments on image classification, semantic segmentation and video classification. Our MorphMLP, such a self-attention free backbone, can be as powerful as and even outperform self-attention based models.},
  file = {/Users/lukakuma/Zotero/storage/ZKL3I954/Zhang et al. - 2021 - MorphMLP A Self-Attention Free, MLP-Like Backbone.pdf;/Users/lukakuma/Zotero/storage/KSBSM9MM/2111.html}
}

@online{zhangMultimodalChainofThoughtReasoning2023,
  title = {Multimodal {{Chain-of-Thought Reasoning}} in {{Language Models}}},
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  date = {2023-02-02},
  number = {arXiv:2302.00923},
  eprint = {arXiv:2302.00923},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.00923},
  urldate = {2023-02-03},
  abstract = {Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies are mostly isolated in the language modality with LLMs, where LLMs are hard to deploy. To elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. To mitigate the effect of such mistakes, we propose Multimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages. By incorporating the vision features in both stages, the model is able to generate effective rationales that contribute to answer inference. With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16\% (75.17\%-{$>$}91.68\%) on the ScienceQA benchmark and even surpasses human performance. Code is publicly available at https://github.com/amazon-science/mm-cot.},
  pubstate = {preprint},
  keywords = {7-multimodal,read},
  file = {/Users/lukakuma/Zotero/storage/CJ3XYXRX/Zhang et al. - 2023 - Multimodal Chain-of-Thought Reasoning in Language .pdf;/Users/lukakuma/Zotero/storage/XIYF78YW/2302.html}
}

@unpublished{zhangOPTOpenPretrained2022,
  title = {{{OPT}}: {{Open Pre-trained Transformer Language Models}}},
  shorttitle = {{{OPT}}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  date = {2022-05-02},
  eprint = {2205.01068},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.01068},
  urldate = {2022-05-03},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  file = {/Users/lukakuma/Zotero/storage/MU7BEELE/Zhang et al. - 2022 - OPT Open Pre-trained Transformer Language Models.pdf;/Users/lukakuma/Zotero/storage/4F9TPMIS/2205.html}
}

@online{zhangWisdomHindsightMakes2023,
  title = {The {{Wisdom}} of {{Hindsight Makes Language Models Better Instruction Followers}}},
  author = {Zhang, Tianjun and Liu, Fangchen and Wong, Justin and Abbeel, Pieter and Gonzalez, Joseph E.},
  date = {2023-02-10},
  number = {arXiv:2302.05206},
  eprint = {arXiv:2302.05206},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.05206},
  urldate = {2023-02-28},
  abstract = {Reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. The so-called algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying Reinforcement Learning (RL) algorithm is complex and requires an additional training pipeline for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.},
  pubstate = {preprint},
  keywords = {alignment},
  file = {/Users/lukakuma/Zotero/storage/EB5F575C/Zhang et al. - 2023 - The Wisdom of Hindsight Makes Language Models Bett.pdf;/Users/lukakuma/Zotero/storage/DJX53C7X/2302.html}
}

@unpublished{zhaoCalculatingQuestionSimilarity2022,
  title = {Calculating {{Question Similarity}} Is {{Enough}}: {{A New Method}} for {{KBQA Tasks}}},
  shorttitle = {Calculating {{Question Similarity}} Is {{Enough}}},
  author = {Zhao, Hanyu and Yuan, Sha and Leng, Jiahong and Pan, Xiang and Wang, Guoqiang and Wu, Ledell and Tang, Jie},
  date = {2022-02-09},
  eprint = {2111.07658},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.07658},
  urldate = {2022-04-13},
  abstract = {Knowledge Base Question Answering (KBQA) aims to answer natural language questions with the help of an external knowledge base. The core idea is to find the link between the internal knowledge behind questions and known triples of the knowledge base. Traditional KBQA task pipelines contain several steps, including entity recognition, entity linking, answering selection, etc. In this kind of pipeline methods, errors in any procedure will inevitably propagate to the final prediction. To address this challenge, this paper proposes a Corpus Generation - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for the KBQA task. The major novelty lies in the design of the new method, wherein our approach, the knowledge enhanced T5 (kT5) model aims to generate natural language QA pairs based on Knowledge Graph triples and directly solve the QA by retrieving the synthetic dataset. The new method can extract more information about the entities from PLM to improve accuracy and simplify the processes. We test our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that our method improves the performance of KBQA and the out straight-forward method is competitive with the state-of-the-art.},
  file = {/Users/lukakuma/Zotero/storage/MB5DT9LL/Zhao et al. - 2022 - Calculating Question Similarity is Enough A New M.pdf;/Users/lukakuma/Zotero/storage/I95XQTCX/2111.html}
}

@online{zhaoCalibrateUseImproving2021,
  title = {Calibrate {{Before Use}}: {{Improving Few-Shot Performance}} of {{Language Models}}},
  shorttitle = {Calibrate {{Before Use}}},
  author = {Zhao, Tony Z. and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  date = {2021-06-10},
  number = {arXiv:2102.09690},
  eprint = {arXiv:2102.09690},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.09690},
  urldate = {2022-10-27},
  abstract = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0\% absolute) and reduces variance across different choices of the prompt.},
  pubstate = {preprint},
  keywords = {calibration},
  file = {/Users/lukakuma/Zotero/storage/Y6CUV7CZ/Zhao et al. - 2021 - Calibrate Before Use Improving Few-Shot Performan.pdf;/Users/lukakuma/Zotero/storage/TS6CMGR9/2102.html}
}

@inproceedings{zhaoComparingDistributionsMeasuring2021,
  title = {Comparing {{Distributions}} by {{Measuring Differences}} That {{Affect Decision Making}}},
  author = {Zhao, Shengjia and Sinha, Abhishek and He, Yutong and Perreault, Aidan and Song, Jiaming and Ermon, Stefano},
  date = {2021-09-29},
  url = {https://openreview.net/forum?id=KB5onONJIAU},
  urldate = {2022-04-28},
  abstract = {Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/XG7BVJBR/Zhao et al. - 2021 - Comparing Distributions by Measuring Differences t.pdf}
}

@unpublished{zhengAIEconomistImproving2020,
  title = {The {{AI Economist}}: {{Improving Equality}} and {{Productivity}} with {{AI-Driven Tax Policies}}},
  shorttitle = {The {{AI Economist}}},
  author = {Zheng, Stephan and Trott, Alexander and Srinivasa, Sunil and Naik, Nikhil and Gruesbeck, Melvin and Parkes, David C. and Socher, Richard},
  date = {2020-04-28},
  eprint = {2004.13332},
  eprinttype = {arxiv},
  eprintclass = {cs, econ, q-fin, stat},
  url = {http://arxiv.org/abs/2004.13332},
  urldate = {2022-01-08},
  abstract = {Tackling real-world socio-economic challenges requires designing and testing economic policies. However, this is hard in practice, due to a lack of appropriate (micro-level) economic data and limited opportunity to experiment. In this work, we train social planners that discover tax policies in dynamic economies that can effectively trade-off economic equality and productivity. We propose a two-level deep reinforcement learning approach to learn dynamic tax policies, based on economic simulations in which both agents and a government learn and adapt. Our data-driven approach does not make use of economic modeling assumptions, and learns from observational data alone. We make four main contributions. First, we present an economic simulation environment that features competitive pressures and market dynamics. We validate the simulation by showing that baseline tax systems perform in a way that is consistent with economic theory, including in regard to learned agent behaviors and specializations. Second, we show that AI-driven tax policies improve the trade-off between equality and productivity by 16\% over baseline policies, including the prominent Saez tax framework. Third, we showcase several emergent features: AI-driven tax policies are qualitatively different from baselines, setting a higher top tax rate and higher net subsidies for low incomes. Moreover, AI-driven tax policies perform strongly in the face of emergent tax-gaming strategies learned by AI agents. Lastly, AI-driven tax policies are also effective when used in experiments with human participants. In experiments conducted on MTurk, an AI tax policy provides an equality-productivity trade-off that is similar to that provided by the Saez framework along with higher inverse-income weighted social welfare.},
  keywords = {policty design},
  file = {/Users/lukakuma/Zotero/storage/PPXBDYZ3/Zheng et al. - 2020 - The AI Economist Improving Equality and Productiv.pdf;/Users/lukakuma/Zotero/storage/V5JTI7FK/2004.html}
}

@unpublished{zhengAIEconomistOptimal2021,
  title = {The {{AI Economist}}: {{Optimal Economic Policy Design}} via {{Two-level Deep Reinforcement Learning}}},
  shorttitle = {The {{AI Economist}}},
  author = {Zheng, Stephan and Trott, Alexander and Srinivasa, Sunil and Parkes, David C. and Socher, Richard},
  date = {2021-08-05},
  eprint = {2108.02755},
  eprinttype = {arxiv},
  eprintclass = {cs, econ, q-fin},
  url = {http://arxiv.org/abs/2108.02755},
  urldate = {2022-01-08},
  abstract = {AI and reinforcement learning (RL) have improved many areas, but are not yet widely adopted in economic policy design, mechanism design, or economics at large. At the same time, current economic methodology is limited by a lack of counterfactual data, simplistic behavioral models, and limited opportunities to experiment with policies and evaluate behavioral responses. Here we show that machine-learning-based economic simulation is a powerful policy and mechanism design framework to overcome these limitations. The AI Economist is a two-level, deep RL framework that trains both agents and a social planner who co-adapt, providing a tractable solution to the highly unstable and novel two-level RL challenge. From a simple specification of an economy, we learn rational agent behaviors that adapt to learned planner policies and vice versa. We demonstrate the efficacy of the AI Economist on the problem of optimal taxation. In simple one-step economies, the AI Economist recovers the optimal tax policy of economic theory. In complex, dynamic economies, the AI Economist substantially improves both utilitarian social welfare and the trade-off between equality and productivity over baselines. It does so despite emergent tax-gaming strategies, while accounting for agent interactions and behavioral change more accurately than economic theory. These results demonstrate for the first time that two-level, deep RL can be used for understanding and as a complement to theory for economic design, unlocking a new computational learning-based approach to understanding economic policy.},
  keywords = {policty design},
  file = {/Users/lukakuma/Zotero/storage/87K46T5P/Zheng et al. - 2021 - The AI Economist Optimal Economic Policy Design v.pdf;/Users/lukakuma/Zotero/storage/7IFCN8LE/2108.html}
}

@article{zhengAIEconomistTaxation2022,
  title = {The {{AI Economist}}: {{Taxation}} Policy Design via Two-Level Deep Multiagent Reinforcement Learning},
  shorttitle = {The {{AI Economist}}},
  author = {Zheng, Stephan and Trott, Alexander and Srinivasa, Sunil and Parkes, David C. and Socher, Richard},
  date = {2022-05-04},
  journaltitle = {Science Advances},
  volume = {8},
  number = {18},
  pages = {eabk2607},
  publisher = {{American Association for the Advancement of Science}},
  url = {https://www.science.org/doi/10.1126/sciadv.abk2607},
  urldate = {2022-06-09},
  keywords = {policty design},
  file = {/Users/lukakuma/Zotero/storage/32SN2UUJ/Zheng et al. - 2022 - The AI Economist Taxation policy design via two-l.pdf}
}

@online{zhengAlpaAutomatingInter2022,
  title = {Alpa: {{Automating Inter-}} and {{Intra-Operator Parallelism}} for {{Distributed Deep Learning}}},
  shorttitle = {Alpa},
  author = {Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P. and Gonzalez, Joseph E. and Stoica, Ion},
  date = {2022-06-28},
  number = {arXiv:2201.12023},
  eprint = {arXiv:2201.12023},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.12023},
  urldate = {2022-10-05},
  abstract = {Alpa automates model-parallel training of large deep learning (DL) models by generating execution plans that unify data, operator, and pipeline parallelism. Existing model-parallel training systems either require users to manually create a parallelization plan or automatically generate one from a limited space of model parallelism configurations. They do not suffice to scale out complex DL models on distributed compute devices. Alpa distributes the training of large DL models by viewing parallelisms as two hierarchical levels: inter-operator and intra-operator parallelisms. Based on it, Alpa constructs a new hierarchical space for massive model-parallel execution plans. Alpa designs a number of compilation passes to automatically derive efficient parallel execution plans at each parallelism level. Alpa implements an efficient runtime to orchestrate the two-level parallel execution on distributed compute devices. Our evaluation shows Alpa generates parallelization plans that match or outperform hand-tuned model-parallel training systems even on models they are designed for. Unlike specialized systems, Alpa also generalizes to models with heterogeneous architectures and models without manually-designed plans. Alpa's source code is publicly available at https://github.com/alpa-projects/alpa},
  pubstate = {preprint},
  keywords = {distributed training,read},
  file = {/Users/lukakuma/Zotero/storage/L429M2J9/Zheng et al. - 2022 - Alpa Automating Inter- and Intra-Operator Paralle.pdf;/Users/lukakuma/Zotero/storage/F7SC55ZR/2201.html}
}

@online{zhengMiniF2FCrosssystemBenchmark2022,
  title = {{{MiniF2F}}: A Cross-System Benchmark for Formal {{Olympiad-level}} Mathematics},
  shorttitle = {{{MiniF2F}}},
  author = {Zheng, Kunhao and Han, Jesse Michael and Polu, Stanislas},
  date = {2022-02-28},
  number = {arXiv:2109.00110},
  eprint = {arXiv:2109.00110},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.00110},
  urldate = {2022-12-22},
  abstract = {We present miniF2F, a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The miniF2F benchmark currently targets Metamath, Lean, Isabelle (partially) and HOL Light (partially) and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses. We report baseline results using GPT-f, a neural theorem prover based on GPT-3 and provide an analysis of its performance. We intend for miniF2F to be a community-driven effort and hope that our benchmark will help spur advances in neural theorem proving.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/28T64CQU/Zheng et al. - 2022 - MiniF2F a cross-system benchmark for formal Olymp.pdf;/Users/lukakuma/Zotero/storage/WWWJL42T/2109.html}
}

@unpublished{zhengOnlineDecisionTransformer2022,
  title = {Online {{Decision Transformer}}},
  author = {Zheng, Qinqing and Zhang, Amy and Grover, Aditya},
  date = {2022-02-11},
  number = {arXiv:2202.05607},
  eprint = {2202.05607},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.05607},
  urldate = {2022-05-13},
  abstract = {Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via taskspecific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.},
  keywords = {(ext) Gato},
  file = {/Users/lukakuma/Zotero/storage/VC6N6TTZ/Zheng et al. - 2022 - Online Decision Transformer.pdf;/Users/lukakuma/Zotero/storage/G8BWIFBW/2202.html}
}

@online{zhongFactualProbingMASK2021,
  title = {Factual {{Probing Is}} [{{MASK}}]: {{Learning}} vs. {{Learning}} to {{Recall}}},
  shorttitle = {Factual {{Probing Is}} [{{MASK}}]},
  author = {Zhong, Zexuan and Friedman, Dan and Chen, Danqi},
  date = {2021-12-14},
  number = {arXiv:2104.05240},
  eprint = {arXiv:2104.05240},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.05240},
  urldate = {2022-11-02},
  abstract = {Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pre-trained language model by expressing them as cloze-style prompts and interpret the model's prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OptiPrompt, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4\% of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle "learning" from "learning to recall", providing a more detailed picture of what different prompts can reveal about pre-trained language models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {finetuning},
  file = {/Users/lukakuma/Zotero/storage/KYZNHF2Q/Zhong et al. - 2021 - Factual Probing Is [MASK] Learning vs. Learning t.pdf}
}

@online{zhongGoalDrivenDiscovery2023,
  title = {Goal {{Driven Discovery}} of {{Distributional Differences}} via {{Language Descriptions}}},
  author = {Zhong, Ruiqi and Zhang, Peter and Li, Steve and Ahn, Jinwoo and Klein, Dan and Steinhardt, Jacob},
  date = {2023-02-27},
  number = {arXiv:2302.14233},
  eprint = {arXiv:2302.14233},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.14233},
  urldate = {2023-03-03},
  abstract = {Mining large corpora can generate useful discoveries but is time-consuming for humans. We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. The task input is a problem comprising a research goal "\$\textbackslash textit\{comparing the side effects of drug A and drug B\}\$" and a corpus pair (two large collections of patients' self-reported reactions after taking each drug). The output is a language description (discovery) of how these corpora differ (patients taking drug A "\$\textbackslash textit\{mention feelings of paranoia\}\$" more often). We build a D5 system, and to quantitatively measure its performance, we 1) contribute a meta-dataset, OpenD5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health, and 2) propose a set of unified evaluation metrics: validity, relevance, novelty, and significance. With the dataset and the unified metrics, we confirm that language models can use the goals to propose more relevant, novel, and significant candidate discoveries. Finally, our system produces discoveries previously unknown to the authors on a wide range of applications in OpenD5, including temporal and demographic differences in discussion topics, political stances and stereotypes in speech, insights in commercial reviews, and error patterns in NLP models.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/E8A4TPVS/Zhong et al. - 2023 - Goal Driven Discovery of Distributional Difference.pdf;/Users/lukakuma/Zotero/storage/M25HUND6/2302.html}
}

@unpublished{zhongHierarchicalMessagePassingGraph2022,
  title = {Hierarchical {{Message-Passing Graph Neural Networks}}},
  author = {Zhong, Zhiqiang and Li, Cheng-Te and Pang, Jun},
  date = {2022-03-18},
  eprint = {2009.03717},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2009.03717},
  urldate = {2022-03-28},
  abstract = {Graph Neural Networks (GNNs) have become a prominent approach to machine learning with graphs and have been increasingly applied in a multitude of domains. Nevertheless, since most existing GNN models are based on flat message-passing mechanisms, two limitations need to be tackled: (i) they are costly in encoding long-range information spanning the graph structure; (ii) they are failing to encode features in the high-order neighbourhood in the graphs as they only perform information aggregation across the observed edges in the original graph. To deal with these two issues, we propose a novel Hierarchical Message-passing Graph Neural Networks framework. The key idea is generating a hierarchical structure that re-organises all nodes in a flat graph into multi-level super graphs, along with innovative intra- and inter-level propagation manners. The derived hierarchy creates shortcuts connecting far-away nodes so that informative long-range interactions can be efficiently accessed via message passing and incorporates meso- and macro-level semantics into the learned node representations. We present the first model to implement this framework, termed Hierarchical Community-aware Graph Neural Network (HC-GNN), with the assistance of a hierarchical community detection algorithm. The theoretical analysis illustrates HC-GNN's remarkable capacity in capturing long-range information without introducing heavy additional computation complexity. Empirical experiments conducted on 9 datasets under transductive, inductive, and few-shot settings exhibit that HC-GNN can outperform state-of-the-art GNN models in network analysis tasks, including node classification, link prediction, and community detection. Moreover, the model analysis further demonstrates HC-GNN's robustness facing graph sparsity and the flexibility in incorporating different GNN encoders.},
  keywords = {GNN},
  file = {/Users/lukakuma/Zotero/storage/66WZRA2D/Zhong et al. - 2022 - Hierarchical Message-Passing Graph Neural Networks.pdf;/Users/lukakuma/Zotero/storage/YMR8JRXY/2009.html}
}

@online{zhongTrainingLanguageModels2022,
  title = {Training {{Language Models}} with {{Memory Augmentation}}},
  author = {Zhong, Zexuan and Lei, Tao and Chen, Danqi},
  date = {2022-10-25},
  number = {arXiv:2205.12674},
  eprint = {arXiv:2205.12674},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.12674},
  urldate = {2022-11-02},
  abstract = {Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories--local, long-term, and external memory--at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.},
  langid = {english},
  pubstate = {preprint},
  keywords = {memory},
  file = {/Users/lukakuma/Zotero/storage/TGQTVDG3/Zhong et al. - 2022 - Training Language Models with Memory Augmentation.pdf}
}

@online{zhouAutoPEFTAutomaticConfiguration2023,
  title = {{{AutoPEFT}}: {{Automatic Configuration Search}} for {{Parameter-Efficient Fine-Tuning}}},
  shorttitle = {{{AutoPEFT}}},
  author = {Zhou, Han and Wan, Xingchen and Vuli\'c, Ivan and Korhonen, Anna},
  date = {2023-01-28},
  number = {arXiv:2301.12132},
  eprint = {arXiv:2301.12132},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2301.12132},
  urldate = {2023-02-16},
  abstract = {Large pretrained language models have been widely used in downstream NLP tasks via task-specific fine-tuning. Recently, an array of Parameter-Efficient Fine-Tuning (PEFT) methods have also achieved strong task performance while updating a much smaller number of parameters compared to full model tuning. However, it is non-trivial to make informed per-task design choices (i.e., to create PEFT configurations) concerning the selection of PEFT architectures and modules, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually set PEFT configurations might be suboptimal for many tasks from the perspective of the performance-to-efficiency trade-off. To address the core question of the PEFT configuration selection that aims to control and maximise the balance between performance and parameter efficiency, we first define a rich configuration search space spanning multiple representative PEFT modules along with finer-grained configuration decisions over the modules (e.g., parameter budget, insertion layer). We then propose AutoPEFT, a novel framework to traverse this configuration space: it automatically configures multiple PEFT modules via high-dimensional Bayesian optimisation. We show the resource scalability and task transferability of AutoPEFT-found configurations, outperforming existing PEFT methods on average on the standard GLUE benchmark while conducting the configuration search on a single task. The per-task AutoPEFT-based configuration search even outperforms full-model fine-tuning.},
  pubstate = {preprint},
  keywords = {*peft},
  file = {/Users/lukakuma/Zotero/storage/DJ3ZAK7Q/Zhou et al. - 2023 - AutoPEFT Automatic Configuration Search for Param.pdf;/Users/lukakuma/Zotero/storage/MA24PRXW/2301.html}
}

@online{zhouLeasttoMostPromptingEnables2022,
  title = {Least-to-{{Most Prompting Enables Complex Reasoning}} in {{Large Language Models}}},
  author = {Zhou, Denny and Sch\"arli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  date = {2022-10-06},
  number = {arXiv:2205.10625},
  eprint = {arXiv:2205.10625},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.10625},
  urldate = {2022-10-25},
  abstract = {Although chain-of-thought prompting has shown impressive results on many natural language reasoning tasks, it often performs poorly on tasks which need to solve problems harder than the demonstration examples. To tackle such easy-to-hard generalization issues, we propose a novel prompting strategy, least-to-most prompting. It reduces a complex problem into a list of subproblems, and then sequentially solve these subproblems, whereby solving a given subproblem is facilitated by the answers to previously solved subproblems. Experiments on symbolic manipulation, compositional generalization and math reasoning show that least-to-most prompting can generalize to the examples that are harder than those seen in the prompt, and outperform chain-of-thought prompting by a large margin. A notable result is that the GPT-3 code-davinci-002 model with least-to-most-prompting solves the SCAN benchmark regardless of splits (such as length split) with an accuracy of 99.7\% using 14 examples versus an accuracy of 16.2\% by chain-of-thought prompting, and neural-symbolic models in the literature specialized for solving SCAN are trained with the full training set of more than 15,000 examples.},
  pubstate = {preprint},
  keywords = {5-reasoning,read},
  file = {/Users/lukakuma/Zotero/storage/LJ6488EW/Zhou et al. - 2022 - Least-to-Most Prompting Enables Complex Reasoning .pdf;/Users/lukakuma/Zotero/storage/6U3GSLS8/2205.html}
}

@online{zhouNavigatingGreyArea2023,
  title = {Navigating the {{Grey Area}}: {{Expressions}} of {{Overconfidence}} and {{Uncertainty}} in {{Language Models}}},
  shorttitle = {Navigating the {{Grey Area}}},
  author = {Zhou, Kaitlyn and Jurafsky, Dan and Hashimoto, Tatsunori},
  date = {2023-02-26},
  number = {arXiv:2302.13439},
  eprint = {arXiv:2302.13439},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.13439},
  urldate = {2023-03-04},
  abstract = {Despite increasingly fluent, relevant, and coherent language generation, major gaps remain between how humans and machines use language. We argue that a key dimension that is missing from our understanding of language models (LMs) is the model's ability to interpret and generate expressions of uncertainty. Whether it be the weatherperson announcing a chance of rain or a doctor giving a diagnosis, information is often not black-and-white and expressions of uncertainty provide nuance to support human-decision making. The increasing deployment of LMs in the wild motivates us to investigate whether LMs are capable of interpreting expressions of uncertainty and how LMs' behaviors change when learning to emit their own expressions of uncertainty. When injecting expressions of uncertainty into prompts (e.g., "I think the answer is..."), we discover that GPT3's generations vary upwards of 80\% in accuracy based on the expression used. We analyze the linguistic characteristics of these expressions and find a drop in accuracy when naturalistic expressions of certainty are present. We find similar effects when teaching models to emit their own expressions of uncertainty, where model calibration suffers when teaching models to emit certainty rather than uncertainty. Together, these results highlight the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty.},
  pubstate = {preprint},
  file = {/Users/lukakuma/Zotero/storage/LKPBZNFR/Zhou et al. - 2023 - Navigating the Grey Area Expressions of Overconfi.pdf;/Users/lukakuma/Zotero/storage/4AU8JVTZ/2302.html}
}

@online{zhouTeachingAlgorithmicReasoning2022,
  title = {Teaching {{Algorithmic Reasoning}} via {{In-context Learning}}},
  author = {Zhou, Hattie and Nova, Azade and Larochelle, Hugo and Courville, Aaron and Neyshabur, Behnam and Sedghi, Hanie},
  date = {2022-11-15},
  number = {arXiv:2211.09066},
  eprint = {arXiv:2211.09066},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2211.09066},
  urldate = {2022-11-18},
  abstract = {Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.},
  pubstate = {preprint},
  keywords = {formal reasoning},
  file = {/Users/lukakuma/Zotero/storage/HNQWA3GX/Zhou et al. - 2022 - Teaching Algorithmic Reasoning via In-context Lear.pdf;/Users/lukakuma/Zotero/storage/2WJZTERQ/2211.html}
}

@online{zieglerFineTuningLanguageModels2020,
  title = {Fine-{{Tuning Language Models}} from {{Human Preferences}}},
  author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  date = {2020-01-08},
  number = {arXiv:1909.08593},
  eprint = {arXiv:1909.08593},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.08593},
  urldate = {2022-07-23},
  abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
  pubstate = {preprint},
  keywords = {alignment,OpenAI},
  file = {/Users/lukakuma/Zotero/storage/HM3YTLPB/Ziegler et al. - 2020 - Fine-Tuning Language Models from Human Preferences.pdf;/Users/lukakuma/Zotero/storage/G2AUXFG9/1909.html}
}

@unpublished{zitnickIntroductionElectrocatalystDesign2020,
  title = {An {{Introduction}} to {{Electrocatalyst Design}} Using {{Machine Learning}} for {{Renewable Energy Storage}}},
  author = {Zitnick, C. Lawrence and Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Lavril, Thibaut and Palizhati, Aini and Riviere, Morgane and Shuaibi, Muhammed and Sriram, Anuroop and Tran, Kevin and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Ulissi, Zachary},
  date = {2020-10-14},
  eprint = {2010.09435},
  eprinttype = {arxiv},
  eprintclass = {cond-mat},
  url = {http://arxiv.org/abs/2010.09435},
  urldate = {2022-04-29},
  abstract = {Scalable and cost-effective solutions to renewable energy storage are essential to addressing the world's rising energy needs while reducing climate change. As we increase our reliance on renewable energy sources such as wind and solar, which produce intermittent power, storage is needed to transfer power from times of peak generation to peak demand. This may require the storage of power for hours, days, or months. One solution that offers the potential of scaling to nation-sized grids is the conversion of renewable energy to other fuels, such as hydrogen or methane. To be widely adopted, this process requires cost-effective solutions to running electrochemical reactions. An open challenge is finding low-cost electrocatalysts to drive these reactions at high rates. Through the use of quantum mechanical simulations (density functional theory), new catalyst structures can be tested and evaluated. Unfortunately, the high computational cost of these simulations limits the number of structures that may be tested. The use of machine learning may provide a method to efficiently approximate these calculations, leading to new approaches in finding effective electrocatalysts. In this paper, we provide an introduction to the challenges in finding suitable electrocatalysts, how machine learning may be applied to the problem, and the use of the Open Catalyst Project OC20 dataset for model training.},
  file = {/Users/lukakuma/Zotero/storage/6IEL2R3L/Zitnick et al. - 2020 - An Introduction to Electrocatalyst Design using Ma.pdf;/Users/lukakuma/Zotero/storage/DWEJSYFX/2010.html}
}

@online{zophSTMoEDesigningStable2022,
  title = {{{ST-MoE}}: {{Designing Stable}} and {{Transferable Sparse Expert Models}}},
  shorttitle = {{{ST-MoE}}},
  author = {Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  date = {2022-04-29},
  number = {arXiv:2202.08906},
  eprint = {arXiv:2202.08906},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.08906},
  urldate = {2022-09-26},
  abstract = {Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).},
  langid = {english},
  pubstate = {preprint},
  keywords = {read,sparsity},
  file = {/Users/lukakuma/Zotero/storage/TEKZZ4TA/Zoph et al. - 2022 - ST-MoE Designing Stable and Transferable Sparse E.pdf}
}

@inproceedings{zyskindDecentralizingPrivacyUsing2015,
  title = {Decentralizing {{Privacy}}: {{Using Blockchain}} to {{Protect Personal Data}}},
  shorttitle = {Decentralizing {{Privacy}}},
  booktitle = {2015 {{IEEE Security}} and {{Privacy Workshops}}},
  author = {Zyskind, Guy and Nathan, Oz and Pentland, Alex 'Sandy'},
  date = {2015-05},
  pages = {180--184},
  publisher = {{IEEE}},
  location = {{San Jose, CA}},
  url = {https://ieeexplore.ieee.org/document/7163223/},
  urldate = {2022-03-09},
  abstract = {The recent increase in reported incidents of surveillance and security breaches compromising users' privacy call into question the current model, in which third-parties collect and control massive amounts of personal data. Bitcoin has demonstrated in the financial space that trusted, auditable computing is possible using a decentralized network of peers accompanied by a public ledger. In this paper, we describe a decentralized personal data management system that ensures users own and control their data. We implement a protocol that turns a blockchain into an automated access-control manager that does not require trust in a third party. Unlike Bitcoin, transactions in our system are not strictly financial \textendash{} they are used to carry instructions, such as storing, querying and sharing data. Finally, we discuss possible future extensions to blockchains that could harness them into a well-rounded solution for trusted computing problems in society.},
  eventtitle = {2015 {{IEEE Security}} and {{Privacy Workshops}} ({{SPW}})},
  isbn = {978-1-4799-9933-0},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/S2GBDRQM/Zyskind et al. - 2015 - Decentralizing Privacy Using Blockchain to Protec.pdf}
}

@unpublished{zyskindEnigmaDecentralizedComputation2015,
  title = {Enigma: {{Decentralized Computation Platform}} with {{Guaranteed Privacy}}},
  shorttitle = {Enigma},
  author = {Zyskind, Guy and Nathan, Oz and Pentland, Alex},
  date = {2015-06-10},
  eprint = {1506.03471},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.03471},
  urldate = {2022-03-09},
  abstract = {A peer-to-peer network, enabling different parties to jointly store and run computations on data while keeping the data completely private. Enigma's computational model is based on a highly optimized version of secure multi-party computation, guaranteed by a verifiable secret-sharing scheme. For storage, we use a modified distributed hashtable for holding secret-shared data. An external blockchain is utilized as the controller of the network, manages access control, identities and serves as a tamper-proof log of events. Security deposits and fees incentivize operation, correctness and fairness of the system. Similar to Bitcoin, Enigma removes the need for a trusted third party, enabling autonomous control of personal data. For the first time, users are able to share their data with cryptographic guarantees regarding their privacy.},
  langid = {english},
  file = {/Users/lukakuma/Zotero/storage/AF83U4JF/Zyskind et al. - 2015 - Enigma Decentralized Computation Platform with Gu.pdf}
}
