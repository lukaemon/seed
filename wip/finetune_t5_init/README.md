## Context
[mm-cot]() involves finetuning T5, but I haven't done it even once before. Since finetuning task specific model is the selling point of huggingface, build some muscle memory couldn't be wrong. 

I know the purpose of gpt-3 is replacing task specific 3b model with few-shot on single LLM, but look at Whisper, Stable Diffusion, Codex, Neeva and sentence-transformers, I want to find an effective symbiosis between specialized small models and LLM. 

## Done

## Learned

## Trigger