"""difference to chargpt
- mix precision training
- lr scheduler
- compile model
- ddp
- wandb logging
- checkpointing and resume
- gradient accumulation
- gradient clip
- dropout=0 for pretraining, 0.1 for finetuning
"""
