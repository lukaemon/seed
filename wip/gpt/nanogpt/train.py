"""difference to chargpt
- loss fn
- mix precision training
- lr scheduler
- compile model
- ddp
- wandb logging
- checkpointing and resume
- gradient accumulation
- gradient clip
- dropout=0 for pretraining, 0.1 for finetuning
- weight decay
- estimate model flop utilization
"""
