"""difference to chargpt
- gelu
- bias free layernorm
- bias free linear
- batched multihead attention
- use flash attention
- careful with dropout position
- vocab size pad to closest 64: 50304
- weight init
- weight decay
- estimate model flop utilization
"""
