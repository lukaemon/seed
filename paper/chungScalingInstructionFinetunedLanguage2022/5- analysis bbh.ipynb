{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('output/bbh/accuracy/flan-t5-large_cot_False_k_1.csv'),\n",
       " PosixPath('output/bbh/accuracy/flan-t5-base_cot_False_k_1.csv'),\n",
       " PosixPath('output/bbh/accuracy/flan-t5-xxl_cot_False_k_1.csv'),\n",
       " PosixPath('output/bbh/accuracy/flan-t5-large_cot_True_k_1.csv'),\n",
       " PosixPath('output/bbh/accuracy/flan-t5-small_cot_True_k_1.csv'),\n",
       " PosixPath('output/bbh/accuracy/flan-t5-xl_cot_False_k_1.csv'),\n",
       " PosixPath('output/bbh/accuracy/flan-t5-base_cot_True_k_1.csv'),\n",
       " PosixPath('output/bbh/accuracy/flan-t5-xxl_cot_True_k_1.csv'),\n",
       " PosixPath('output/bbh/accuracy/flan-t5-small_cot_False_k_1.csv'),\n",
       " PosixPath('output/bbh/accuracy/flan-t5-xl_cot_True_k_1.csv')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_dir = Path(\"output/bbh/accuracy\")\n",
    "\n",
    "files = list(acc_dir.glob(\"*.csv\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>task_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flan-t5-large</td>\n",
       "      <td>boolean_expressions</td>\n",
       "      <td>0.54000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flan-t5-large</td>\n",
       "      <td>causal_judgement</td>\n",
       "      <td>0.57754</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flan-t5-large</td>\n",
       "      <td>date_understanding</td>\n",
       "      <td>0.24400</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flan-t5-large</td>\n",
       "      <td>disambiguation_qa</td>\n",
       "      <td>0.65200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flan-t5-large</td>\n",
       "      <td>dyck_languages</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model_name            task_name  accuracy    cot\n",
       "0  flan-t5-large  boolean_expressions   0.54000  False\n",
       "1  flan-t5-large     causal_judgement   0.57754  False\n",
       "2  flan-t5-large   date_understanding   0.24400  False\n",
       "3  flan-t5-large    disambiguation_qa   0.65200  False\n",
       "4  flan-t5-large       dyck_languages   0.00000  False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(f) for f in files]).drop(columns=['k'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cot</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flan-t5-base</th>\n",
       "      <td>0.316745</td>\n",
       "      <td>0.253406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flan-t5-large</th>\n",
       "      <td>0.373640</td>\n",
       "      <td>0.311213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flan-t5-small</th>\n",
       "      <td>0.297036</td>\n",
       "      <td>0.111279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flan-t5-xl</th>\n",
       "      <td>0.413981</td>\n",
       "      <td>0.349619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flan-t5-xxl</th>\n",
       "      <td>0.458086</td>\n",
       "      <td>0.412731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy          \n",
       "cot               False      True\n",
       "model_name                       \n",
       "flan-t5-base   0.316745  0.253406\n",
       "flan-t5-large  0.373640  0.311213\n",
       "flan-t5-small  0.297036  0.111279\n",
       "flan-t5-xl     0.413981  0.349619\n",
       "flan-t5-xxl    0.458086  0.412731"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"model_name\", \"cot\"]).mean().unstack()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./output/flan_paper_result.png)\n",
    "\n",
    "Mostly match paper results. One exception, Small CoT is 8% lower than paper. Why? Hunch is my brittle, primitive regex answer extraction is not as good as paper's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cot</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>boolean_expressions</th>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>causal_judgement</th>\n",
       "      <td>0.609626</td>\n",
       "      <td>0.556150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_understanding</th>\n",
       "      <td>0.552000</td>\n",
       "      <td>0.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disambiguation_qa</th>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dyck_languages</th>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formal_fallacies</th>\n",
       "      <td>0.544000</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geometric_shapes</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hyperbaton</th>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logical_deduction_five_objects</th>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logical_deduction_seven_objects</th>\n",
       "      <td>0.612000</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logical_deduction_three_objects</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie_recommendation</th>\n",
       "      <td>0.612000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multistep_arithmetic_two</th>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>navigate</th>\n",
       "      <td>0.596000</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object_counting</th>\n",
       "      <td>0.436000</td>\n",
       "      <td>0.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penguins_in_a_table</th>\n",
       "      <td>0.431507</td>\n",
       "      <td>0.431507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasoning_about_colored_objects</th>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruin_names</th>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salient_translation_error_detection</th>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snarks</th>\n",
       "      <td>0.747191</td>\n",
       "      <td>0.528090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports_understanding</th>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temporal_sequences</th>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tracking_shuffled_objects_five_objects</th>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tracking_shuffled_objects_seven_objects</th>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tracking_shuffled_objects_three_objects</th>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>web_of_lies</th>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_sorting</th>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         accuracy          \n",
       "cot                                         False      True\n",
       "task_name                                                  \n",
       "boolean_expressions                      0.580000  0.632000\n",
       "causal_judgement                         0.609626  0.556150\n",
       "date_understanding                       0.552000  0.608000\n",
       "disambiguation_qa                        0.676000  0.632000\n",
       "dyck_languages                           0.012000  0.008000\n",
       "formal_fallacies                         0.544000  0.560000\n",
       "geometric_shapes                         0.200000  0.268000\n",
       "hyperbaton                               0.740000  0.624000\n",
       "logical_deduction_five_objects           0.536000  0.484000\n",
       "logical_deduction_seven_objects          0.612000  0.520000\n",
       "logical_deduction_three_objects          0.720000  0.636000\n",
       "movie_recommendation                     0.612000  0.400000\n",
       "multistep_arithmetic_two                 0.016000  0.004000\n",
       "navigate                                 0.596000  0.580000\n",
       "object_counting                          0.436000  0.388000\n",
       "penguins_in_a_table                      0.431507  0.431507\n",
       "reasoning_about_colored_objects          0.592000  0.500000\n",
       "ruin_names                               0.504000  0.340000\n",
       "salient_translation_error_detection      0.416000  0.328000\n",
       "snarks                                   0.747191  0.528090\n",
       "sports_understanding                     0.692000  0.632000\n",
       "temporal_sequences                       0.344000  0.284000\n",
       "tracking_shuffled_objects_five_objects   0.180000  0.212000\n",
       "tracking_shuffled_objects_seven_objects  0.164000  0.136000\n",
       "tracking_shuffled_objects_three_objects  0.268000  0.312000\n",
       "web_of_lies                              0.520000  0.524000\n",
       "word_sorting                             0.068000  0.016000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(f) for f in acc_dir.glob('*xxl*.csv')]).drop(columns=['k'])\n",
    "df.groupby(['task_name', 'cot']).mean().unstack().sort_values(by=('task_name'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- Flan improves performance a lot, over t5. \n",
    "- Flan didn't give t5 CoT capability. Across all t5 series, CoT performs worse than direct prompt. \n",
    "- CoT performs better over direct prompt only with 540b models.\n",
    "\n",
    "Since CoT is no show in t5, no point to do failure case analysis. Nothing to analyze for direction prompt. And you would only find the same mistakes as in CoT papaer for CoT prompted failure cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
