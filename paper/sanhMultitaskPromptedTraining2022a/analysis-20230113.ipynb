{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "- Top priority is debug. Find out all inference bugs that cause low to none performance.\n",
    "- Then find the low performant group and learn what's wrong.\n",
    "\n",
    "### Learned\n",
    "- Add few todos to obsidian driver notes. \n",
    "    - How to eval in vector space?\n",
    "    - How to build `dataloader` for evaluation?\n",
    "    - How to deal with `multiple choice` eval? \n",
    "- Full eval on `t0` as today's baseline for next stage experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/workspaces/seed/paper/sanhMultitaskPromptedTraining2022a/evaluation_result/20230112/T0_results.csv'\n",
    "new_csv_path = '/workspaces/seed/paper/sanhMultitaskPromptedTraining2022a/evaluation_result/20230113/T0.csv'\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "new_df = pd.read_csv(new_csv_path)\n",
    "\n",
    "# replace subset_name which is NaN with 'all'\n",
    "df['subset_name'] = df['subset_name'].fillna('all')\n",
    "new_df['subset_name'] = new_df['subset_name'].fillna('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(df):\n",
    "    return df.groupby(['dataset_name', 'subset_name']) \\\n",
    "        .agg({'accuracy': ['max', 'min', 'mean', 'std', 'count']}) \\\n",
    "        .rename(columns={'count': 'num_prompts'}) \\\n",
    "        .sort_values(by=('accuracy', 'std'), ascending=False)\n",
    "\n",
    "def peak(df, dataset_name, subset_name):\n",
    "    return df[(df['dataset_name'] == dataset_name) & (df['subset_name'] == subset_name)].sort_values(by='prompt_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>num_prompts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_name</th>\n",
       "      <th>subset_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">super_glue</th>\n",
       "      <th>cb</th>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.394331</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wic</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.292833</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wsc.fixed</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.521154</td>\n",
       "      <td>0.208841</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anli</th>\n",
       "      <th>all</th>\n",
       "      <td>0.445000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.204300</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">super_glue</th>\n",
       "      <th>copa</th>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.809690</td>\n",
       "      <td>0.191170</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rte</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.047022</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winogrande</th>\n",
       "      <th>winogrande_xl</th>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hellaswag</th>\n",
       "      <th>all</th>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            accuracy                                          \n",
       "                                 max       min      mean       std num_prompts\n",
       "dataset_name subset_name                                                      \n",
       "super_glue   cb             0.803571  0.000000  0.469048  0.394331          15\n",
       "             wic            0.620000  0.000000  0.227000  0.292833          10\n",
       "             wsc.fixed      0.692308  0.009615  0.521154  0.208841          10\n",
       "anli         all            0.445000  0.000000  0.258000  0.204300          15\n",
       "super_glue   copa           0.960000  0.320000  0.809690  0.191170          12\n",
       "             rte            0.875000  0.735000  0.825000  0.047022          10\n",
       "winogrande   winogrande_xl  0.630000  0.550000  0.595000  0.032787           5\n",
       "hellaswag    all            0.035000  0.020000  0.027500  0.006455           4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>num_prompts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_name</th>\n",
       "      <th>subset_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">super_glue</th>\n",
       "      <th>wsc.fixed</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.246306</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>copa</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.750183</td>\n",
       "      <td>0.168953</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wic</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.161933</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.723333</td>\n",
       "      <td>0.141253</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winogrande</th>\n",
       "      <th>winogrande_xl</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.135093</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anli</th>\n",
       "      <th>all</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.115264</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super_glue</th>\n",
       "      <th>rte</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>0.066875</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hellaswag</th>\n",
       "      <th>all</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy                                      \n",
       "                                max   min      mean       std num_prompts\n",
       "dataset_name subset_name                                                 \n",
       "super_glue   wsc.fixed         0.80  0.00  0.520000  0.246306          10\n",
       "             copa              0.95  0.35  0.750183  0.168953          12\n",
       "             wic               0.60  0.00  0.430000  0.161933          10\n",
       "             cb                0.80  0.25  0.723333  0.141253          15\n",
       "winogrande   winogrande_xl     0.80  0.45  0.580000  0.135093           5\n",
       "anli         all               0.55  0.10  0.390000  0.115264          15\n",
       "super_glue   rte               0.85  0.65  0.765000  0.066875          10\n",
       "hellaswag    all               0.05  0.00  0.012500  0.025000           4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(new_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's wrong with `cb`?\n",
    "- `Never != never` That's the problem of comparing in text space. Even in tokenized space, case sensitivity is still there.\n",
    "  - Naively process all t2t generated output and target to lower case.\n",
    "  - Eval in text space is not right. Would have to rebuild later in vector space. \n",
    "- `cb` has no 0 now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>subset_name</th>\n",
       "      <th>test_size</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>GPT-3 style</td>\n",
       "      <td>0.767857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>MNLI crowdsource</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>always/sometimes/never</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>based on the previous passage</td>\n",
       "      <td>0.803571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>can we infer</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>claim true/false/inconclusive</td>\n",
       "      <td>0.017857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>consider always/sometimes/never</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>does it follow that</td>\n",
       "      <td>0.732143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>does this imply</td>\n",
       "      <td>0.803571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>guaranteed true</td>\n",
       "      <td>0.767857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>guaranteed/possible/impossible</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>justified in saying</td>\n",
       "      <td>0.767857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>must be true</td>\n",
       "      <td>0.803571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>should assume</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>56</td>\n",
       "      <td>take the following as truth</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name dataset_name subset_name  test_size  \\\n",
       "16  bigscience/T0   super_glue          cb         56   \n",
       "22  bigscience/T0   super_glue          cb         56   \n",
       "15  bigscience/T0   super_glue          cb         56   \n",
       "11  bigscience/T0   super_glue          cb         56   \n",
       "10  bigscience/T0   super_glue          cb         56   \n",
       "12  bigscience/T0   super_glue          cb         56   \n",
       "17  bigscience/T0   super_glue          cb         56   \n",
       "13  bigscience/T0   super_glue          cb         56   \n",
       "21  bigscience/T0   super_glue          cb         56   \n",
       "18  bigscience/T0   super_glue          cb         56   \n",
       "20  bigscience/T0   super_glue          cb         56   \n",
       "14  bigscience/T0   super_glue          cb         56   \n",
       "19  bigscience/T0   super_glue          cb         56   \n",
       "23  bigscience/T0   super_glue          cb         56   \n",
       "24  bigscience/T0   super_glue          cb         56   \n",
       "\n",
       "                        prompt_name  accuracy  \n",
       "16                      GPT-3 style  0.767857  \n",
       "22                 MNLI crowdsource  0.000000  \n",
       "15           always/sometimes/never  0.000000  \n",
       "11    based on the previous passage  0.803571  \n",
       "10                     can we infer  0.785714  \n",
       "12    claim true/false/inconclusive  0.017857  \n",
       "17  consider always/sometimes/never  0.000000  \n",
       "13              does it follow that  0.732143  \n",
       "21                  does this imply  0.803571  \n",
       "18                  guaranteed true  0.767857  \n",
       "20   guaranteed/possible/impossible  0.000000  \n",
       "14              justified in saying  0.767857  \n",
       "19                     must be true  0.803571  \n",
       "23                    should assume  0.785714  \n",
       "24      take the following as truth  0.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak(df, 'super_glue', 'cb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checkpoint</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>subset_name</th>\n",
       "      <th>test_size</th>\n",
       "      <th>time</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.887944</td>\n",
       "      <td>GPT-3 style</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>1.005579</td>\n",
       "      <td>MNLI crowdsource</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.848107</td>\n",
       "      <td>always/sometimes/never</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.847646</td>\n",
       "      <td>based on the previous passage</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.866905</td>\n",
       "      <td>can we infer</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.908543</td>\n",
       "      <td>claim true/false/inconclusive</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.850977</td>\n",
       "      <td>consider always/sometimes/never</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.833466</td>\n",
       "      <td>does it follow that</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.852169</td>\n",
       "      <td>does this imply</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.851614</td>\n",
       "      <td>guaranteed true</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.847036</td>\n",
       "      <td>guaranteed/possible/impossible</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.835791</td>\n",
       "      <td>justified in saying</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.845323</td>\n",
       "      <td>must be true</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.853411</td>\n",
       "      <td>should assume</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>cb</td>\n",
       "      <td>20</td>\n",
       "      <td>1.008438</td>\n",
       "      <td>take the following as truth</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       checkpoint dataset_name subset_name  test_size      time  \\\n",
       "16  bigscience/T0   super_glue          cb         20  0.887944   \n",
       "22  bigscience/T0   super_glue          cb         20  1.005579   \n",
       "15  bigscience/T0   super_glue          cb         20  0.848107   \n",
       "11  bigscience/T0   super_glue          cb         20  0.847646   \n",
       "10  bigscience/T0   super_glue          cb         20  0.866905   \n",
       "12  bigscience/T0   super_glue          cb         20  0.908543   \n",
       "17  bigscience/T0   super_glue          cb         20  0.850977   \n",
       "13  bigscience/T0   super_glue          cb         20  0.833466   \n",
       "21  bigscience/T0   super_glue          cb         20  0.852169   \n",
       "18  bigscience/T0   super_glue          cb         20  0.851614   \n",
       "20  bigscience/T0   super_glue          cb         20  0.847036   \n",
       "14  bigscience/T0   super_glue          cb         20  0.835791   \n",
       "19  bigscience/T0   super_glue          cb         20  0.845323   \n",
       "23  bigscience/T0   super_glue          cb         20  0.853411   \n",
       "24  bigscience/T0   super_glue          cb         20  1.008438   \n",
       "\n",
       "                        prompt_name  accuracy  \n",
       "16                      GPT-3 style      0.80  \n",
       "22                 MNLI crowdsource      0.75  \n",
       "15           always/sometimes/never      0.70  \n",
       "11    based on the previous passage      0.80  \n",
       "10                     can we infer      0.75  \n",
       "12    claim true/false/inconclusive      0.80  \n",
       "17  consider always/sometimes/never      0.60  \n",
       "13              does it follow that      0.75  \n",
       "21                  does this imply      0.80  \n",
       "18                  guaranteed true      0.75  \n",
       "20   guaranteed/possible/impossible      0.25  \n",
       "14              justified in saying      0.75  \n",
       "19                     must be true      0.80  \n",
       "23                    should assume      0.75  \n",
       "24      take the following as truth      0.80  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak(new_df, 'super_glue', 'cb')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about `wic`\n",
    "- Same case sensitivity problem. Decide to rerun a full eval on `to_3b` on all dataset and reanalysis.\n",
    "- `wic` has no 0, except for prompt: `similar_sense`. Take a look. I think it's just very unclear prompt. \n",
    "```\n",
    "I expect to receive wages.\n",
    "We were expecting a visit from our relatives.\n",
    "Similar sense of expect?\n",
    "yes\n",
    "noun\n",
    "\n",
    "To pick rags.\n",
    "Don't always pick on your little brother.\n",
    "Similar sense of pick?\n",
    "no\n",
    "he picks up his little brother and puts him down.\n",
    "\n",
    "They kept a log of all transmission by the radio station.\n",
    "An email log.\n",
    "Similar sense of log?\n",
    "yes\n",
    "similar sense of log\n",
    "\n",
    "The professionalization of warfare.\n",
    "The professionalization of American sports.\n",
    "Similar sense of professionalization?\n",
    "yes\n",
    "noun\n",
    "\n",
    "He drank a mixture of beer and lemonade.\n",
    "The mixture of sulphuric acid and water produces heat.\n",
    "Similar sense of mixture?\n",
    "no\n",
    "noun\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>subset_name</th>\n",
       "      <th>test_size</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>200</td>\n",
       "      <td>GPT-3-prompt</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>200</td>\n",
       "      <td>GPT-3-prompt-with-label</td>\n",
       "      <td>0.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>200</td>\n",
       "      <td>affirmation_true_or_false</td>\n",
       "      <td>0.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>200</td>\n",
       "      <td>grammar_homework</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>200</td>\n",
       "      <td>polysemous</td>\n",
       "      <td>0.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>200</td>\n",
       "      <td>question-context</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>200</td>\n",
       "      <td>question-context-meaning</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>200</td>\n",
       "      <td>question-context-meaning-with-label</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>200</td>\n",
       "      <td>same_sense</td>\n",
       "      <td>0.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>200</td>\n",
       "      <td>similar-sense</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name dataset_name subset_name  test_size  \\\n",
       "59  bigscience/T0   super_glue         wic        200   \n",
       "62  bigscience/T0   super_glue         wic        200   \n",
       "58  bigscience/T0   super_glue         wic        200   \n",
       "57  bigscience/T0   super_glue         wic        200   \n",
       "63  bigscience/T0   super_glue         wic        200   \n",
       "61  bigscience/T0   super_glue         wic        200   \n",
       "56  bigscience/T0   super_glue         wic        200   \n",
       "55  bigscience/T0   super_glue         wic        200   \n",
       "60  bigscience/T0   super_glue         wic        200   \n",
       "64  bigscience/T0   super_glue         wic        200   \n",
       "\n",
       "                            prompt_name  accuracy  \n",
       "59                         GPT-3-prompt     0.000  \n",
       "62              GPT-3-prompt-with-label     0.515  \n",
       "58            affirmation_true_or_false     0.515  \n",
       "57                     grammar_homework     0.000  \n",
       "63                           polysemous     0.620  \n",
       "61                     question-context     0.000  \n",
       "56             question-context-meaning     0.000  \n",
       "55  question-context-meaning-with-label     0.010  \n",
       "60                           same_sense     0.610  \n",
       "64                        similar-sense     0.000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak(df, 'super_glue', 'wic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checkpoint</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>subset_name</th>\n",
       "      <th>test_size</th>\n",
       "      <th>time</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>20</td>\n",
       "      <td>0.252208</td>\n",
       "      <td>GPT-3-prompt</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>20</td>\n",
       "      <td>0.259192</td>\n",
       "      <td>GPT-3-prompt-with-label</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>20</td>\n",
       "      <td>0.306139</td>\n",
       "      <td>affirmation_true_or_false</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>20</td>\n",
       "      <td>0.260800</td>\n",
       "      <td>grammar_homework</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>20</td>\n",
       "      <td>0.320570</td>\n",
       "      <td>polysemous</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>20</td>\n",
       "      <td>0.252100</td>\n",
       "      <td>question-context</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>20</td>\n",
       "      <td>0.243914</td>\n",
       "      <td>question-context-meaning</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>20</td>\n",
       "      <td>0.268794</td>\n",
       "      <td>question-context-meaning-with-label</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>20</td>\n",
       "      <td>0.315969</td>\n",
       "      <td>same_sense</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>super_glue</td>\n",
       "      <td>wic</td>\n",
       "      <td>20</td>\n",
       "      <td>0.439067</td>\n",
       "      <td>similar-sense</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       checkpoint dataset_name subset_name  test_size      time  \\\n",
       "59  bigscience/T0   super_glue         wic         20  0.252208   \n",
       "62  bigscience/T0   super_glue         wic         20  0.259192   \n",
       "58  bigscience/T0   super_glue         wic         20  0.306139   \n",
       "57  bigscience/T0   super_glue         wic         20  0.260800   \n",
       "63  bigscience/T0   super_glue         wic         20  0.320570   \n",
       "61  bigscience/T0   super_glue         wic         20  0.252100   \n",
       "56  bigscience/T0   super_glue         wic         20  0.243914   \n",
       "55  bigscience/T0   super_glue         wic         20  0.268794   \n",
       "60  bigscience/T0   super_glue         wic         20  0.315969   \n",
       "64  bigscience/T0   super_glue         wic         20  0.439067   \n",
       "\n",
       "                            prompt_name  accuracy  \n",
       "59                         GPT-3-prompt      0.45  \n",
       "62              GPT-3-prompt-with-label      0.40  \n",
       "58            affirmation_true_or_false      0.55  \n",
       "57                     grammar_homework      0.45  \n",
       "63                           polysemous      0.50  \n",
       "61                     question-context      0.45  \n",
       "56             question-context-meaning      0.45  \n",
       "55  question-context-meaning-with-label      0.45  \n",
       "60                           same_sense      0.60  \n",
       "64                        similar-sense      0.00  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak(new_df, 'super_glue', 'wic')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `hellaswag` is elephant in the room. Consistently bad performance across all prompts\n",
    "Preview `hellaswag`\n",
    "```\n",
    "input_text: How does this sentence end?\n",
    "[header] How to recover from an emotional affair [title] Forgive yourself. [step] While forgiving others can be challenging, it's often even harder to forgive yourself. Remember that if you had known the path of your actions and their consequences, you probably would not have done what you did.\n",
    "\n",
    "(a)  Take some time to live in the past and let go of those emotions. For example, if you had experienced a miscarriage, forgiveness would be easy.\n",
    "\n",
    "(b)  Forgiveness in someone else can only serve to make it harder. [substeps] Cheating can be very emotional, and can even be worse.\n",
    "\n",
    "(c)  To begin forgiving yourself, admit that you messed up or made a mistake. Making mistakes is part of being human and no one is exempt from it.\n",
    "\n",
    "(d)  In the moment, forgive yourself for all the things you could have done differently. [substeps] Though you may have felt wronged, forgiving yourself for your actions will take time and effort to live.\n",
    "\n",
    "Hint: the topic of the sentence is Family Life\n",
    "target: to begin forgiving yourself, admit that you messed up or made a mistake. making mistakes is part of being human and no one is exempt from it.\n",
    "LM output: forgiveness\n",
    "\n",
    "input_text: How does this sentence end?\n",
    "A doctor in a lab coat talks about the lenses too, while people are showing how to use them. another news anchor\n",
    "\n",
    "(a)  talks about contacts lenses and how robotic they can be.\n",
    "\n",
    "(b)  also talks about the same lenses and how it has become a dangerous trend among teenagers.\n",
    "\n",
    "(c)  is interviewed about the incident.\n",
    "\n",
    "(d)  talks about the lens and drink is an advertisement that the lens is called printed in a foreign language.\n",
    "\n",
    "Hint: the topic of the sentence is Putting in contact lenses\n",
    "target: also talks about the same lenses and how it has become a dangerous trend among teenagers.\n",
    "LM output: contact, lens, put\n",
    "```\n",
    "- I don't know how to handle multiple choice evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>subset_name</th>\n",
       "      <th>test_size</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>hellaswag</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>Predict ending with hint</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>hellaswag</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>Randomized prompts template</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>hellaswag</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>complete_first_then</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>hellaswag</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>if_begins_how_continues</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name dataset_name subset_name  test_size  \\\n",
       "65  bigscience/T0    hellaswag         all        200   \n",
       "66  bigscience/T0    hellaswag         all        200   \n",
       "67  bigscience/T0    hellaswag         all        200   \n",
       "68  bigscience/T0    hellaswag         all        200   \n",
       "\n",
       "                    prompt_name  accuracy  \n",
       "65     Predict ending with hint     0.020  \n",
       "66  Randomized prompts template     0.030  \n",
       "67          complete_first_then     0.025  \n",
       "68      if_begins_how_continues     0.035  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak(df, 'hellaswag', 'all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `anli`\n",
    "- Common sense eval set. Max=0.44 for now. It is just hard dataset. \n",
    "\n",
    "```\n",
    "input_text: \"Be Right Back\" is the first episode of the second series of British science fiction anthology series \"Black Mirror\". It was written by series creator and showrunner Charlie Brooker, directed by Owen Harris and first aired on Channel 4 on 11 February 2013. Using only the above description and what you know about the world, \"\"Be Right Back\" has existed for over 6 years\" is definitely correct, incorrect, or inconclusive?\n",
    "target: correct\n",
    "LM output: incorrect\n",
    "\n",
    "input_text: Club Atlético Unión de Mar del Plata is an Argentine sports club from Mar del Plata, Buenos Aires Province. The club was founded on December 1, 1926, and its main sports are football and basketball. In football, Unión currently plays in the Torneo Argentino A, which is the regionalised third division of the Argentine football league system. Using only the above description and what you know about the world, \"Club Atlético Unión de Mar del Plata has been around for 100 years\" is definitely correct, incorrect, or inconclusive?\n",
    "target: incorrect\n",
    "LM output: correct\n",
    "\n",
    "input_text: Bantiger TV Tower is a 196 metre tall tower used for FM- and TV-transmission at on the Bantiger mountain, a mountain east of Berne situated in the municipality of Bolligen. The Bantiger TV Tower was built between 1991 and 1996 as replacement of a 100 metres tall radio tower, built in 1954. Using only the above description and what you know about the world, \"Bantiger TV Tower is used for AM-, FM-, and TV-transmissions.  \" is definitely correct, incorrect, or inconclusive?\n",
    "target: incorrect\n",
    "LM output: correct\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `winogrande`\n",
    "- entity resolution\n",
    "\n",
    "```\n",
    "input_text: In the sentence below, does the _ stand for Patricia or Felicia?\n",
    "Patricia decided to buy Felicia dinner because they had been through a lot and _ just inherited some money.\n",
    "target: patricia\n",
    "LM output: felicia\n",
    "\n",
    "input_text: In the sentence below, does the _ stand for south or north?\n",
    "The clothing in the north was warmer than the clothing in the south because there was more snow in the _ .\n",
    "target: north\n",
    "LM output: north\n",
    "\n",
    "input_text: In the sentence below, does the _ stand for transporter or plane?\n",
    "Timmy bought a transporter for his cat so he could take him on the plane but the _ was too small.\n",
    "target: transporter\n",
    "LM output: plane\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>subset_name</th>\n",
       "      <th>test_size</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>MNLI crowdsource</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>always/sometimes/never</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>consider always/sometimes/never</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>guaranteed/possible/impossible</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>claim true/false/inconclusive</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>take the following as truth</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>guaranteed true</td>\n",
       "      <td>0.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>should assume</td>\n",
       "      <td>0.405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>justified in saying</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>based on the previous passage</td>\n",
       "      <td>0.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>can we infer</td>\n",
       "      <td>0.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>does it follow that</td>\n",
       "      <td>0.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>does this imply</td>\n",
       "      <td>0.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>must be true</td>\n",
       "      <td>0.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>anli</td>\n",
       "      <td>all</td>\n",
       "      <td>200</td>\n",
       "      <td>GPT-3 style</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name dataset_name subset_name  test_size  \\\n",
       "25  bigscience/T0         anli         all        200   \n",
       "35  bigscience/T0         anli         all        200   \n",
       "37  bigscience/T0         anli         all        200   \n",
       "34  bigscience/T0         anli         all        200   \n",
       "38  bigscience/T0         anli         all        200   \n",
       "31  bigscience/T0         anli         all        200   \n",
       "39  bigscience/T0         anli         all        200   \n",
       "26  bigscience/T0         anli         all        200   \n",
       "30  bigscience/T0         anli         all        200   \n",
       "29  bigscience/T0         anli         all        200   \n",
       "33  bigscience/T0         anli         all        200   \n",
       "27  bigscience/T0         anli         all        200   \n",
       "36  bigscience/T0         anli         all        200   \n",
       "32  bigscience/T0         anli         all        200   \n",
       "28  bigscience/T0         anli         all        200   \n",
       "\n",
       "                        prompt_name  accuracy  \n",
       "25                 MNLI crowdsource     0.000  \n",
       "35           always/sometimes/never     0.000  \n",
       "37  consider always/sometimes/never     0.000  \n",
       "34   guaranteed/possible/impossible     0.000  \n",
       "38    claim true/false/inconclusive     0.035  \n",
       "31      take the following as truth     0.070  \n",
       "39                  guaranteed true     0.395  \n",
       "26                    should assume     0.405  \n",
       "30              justified in saying     0.410  \n",
       "29    based on the previous passage     0.420  \n",
       "33                     can we infer     0.420  \n",
       "27              does it follow that     0.420  \n",
       "36                  does this imply     0.420  \n",
       "32                     must be true     0.430  \n",
       "28                      GPT-3 style     0.445  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak(df, 'anli', 'all').sort_values(by='accuracy', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>subset_name</th>\n",
       "      <th>test_size</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>winogrande</td>\n",
       "      <td>winogrande_xl</td>\n",
       "      <td>200</td>\n",
       "      <td>Replace</td>\n",
       "      <td>0.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>winogrande</td>\n",
       "      <td>winogrande_xl</td>\n",
       "      <td>200</td>\n",
       "      <td>fill in the blank</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>winogrande</td>\n",
       "      <td>winogrande_xl</td>\n",
       "      <td>200</td>\n",
       "      <td>stand for</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>winogrande</td>\n",
       "      <td>winogrande_xl</td>\n",
       "      <td>200</td>\n",
       "      <td>does underscore refer to</td>\n",
       "      <td>0.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>bigscience/T0</td>\n",
       "      <td>winogrande</td>\n",
       "      <td>winogrande_xl</td>\n",
       "      <td>200</td>\n",
       "      <td>underscore refer to</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name dataset_name    subset_name  test_size  \\\n",
       "54  bigscience/T0   winogrande  winogrande_xl        200   \n",
       "53  bigscience/T0   winogrande  winogrande_xl        200   \n",
       "51  bigscience/T0   winogrande  winogrande_xl        200   \n",
       "50  bigscience/T0   winogrande  winogrande_xl        200   \n",
       "52  bigscience/T0   winogrande  winogrande_xl        200   \n",
       "\n",
       "                 prompt_name  accuracy  \n",
       "54                   Replace     0.550  \n",
       "53         fill in the blank     0.575  \n",
       "51                 stand for     0.600  \n",
       "50  does underscore refer to     0.620  \n",
       "52       underscore refer to     0.630  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak(df, 'winogrande', 'winogrande_xl').sort_values(by='accuracy', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'bigscience/T0'\n",
    "t2t = utils.build_t2t(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-13 03:03:53,182] [datasets.builder] [builder.py:785] Found cached dataset winogrande (/workspaces/seed/cache/hf_dataset/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'winogrande'\n",
    "subset_name = 'winogrande_xl'\n",
    "raw_dataset = utils.load_raw_dataset(dataset_name, subset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-13 03:04:34,403] [datasets.arrow_dataset] [arrow_dataset.py:3930] Loading cached shuffled indices for dataset at /workspaces/seed/cache/hf_dataset/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2/cache-5aca6c830a1dfa33.arrow\n"
     ]
    }
   ],
   "source": [
    "prompt_name = 'stand for'\n",
    "prompt = utils.get_prompt(dataset_name, subset_name, prompt_name)\n",
    "input_text, target_text = utils.preprocess_dataset(raw_dataset, prompt, cutoff=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text: In the sentence below, does the _ stand for Patricia or Felicia?\n",
      "Patricia decided to buy Felicia dinner because they had been through a lot and _ just inherited some money.\n",
      "target: patricia\n",
      "LM output: felicia\n",
      "\n",
      "input_text: In the sentence below, does the _ stand for south or north?\n",
      "The clothing in the north was warmer than the clothing in the south because there was more snow in the _ .\n",
      "target: north\n",
      "LM output: north\n",
      "\n",
      "input_text: In the sentence below, does the _ stand for transporter or plane?\n",
      "Timmy bought a transporter for his cat so he could take him on the plane but the _ was too small.\n",
      "target: transporter\n",
      "LM output: plane\n",
      "\n",
      "input_text: In the sentence below, does the _ stand for diner or food truck?\n",
      "It was easier for the diner to follow their budget than the food truck because the _ had more money to spend.\n",
      "target: diner\n",
      "LM output: food truck\n",
      "\n",
      "input_text: In the sentence below, does the _ stand for headphone or clock?\n",
      "John could not hear his alarm clock when he was sleeping with a headphone on his head because the _ is closer.\n",
      "target: headphone\n",
      "LM output: clock\n",
      "\n",
      "input_text: In the sentence below, does the _ stand for Nick or William?\n",
      "Happiness was most important to Nick but money was most important to William. _ valued a good life.\n",
      "target: nick\n",
      "LM output: william\n",
      "\n",
      "input_text: In the sentence below, does the _ stand for offer or sofa?\n",
      "Carrie posted their sofa for sale on Craigslist, and had received an offer they had to decline because the _ is valuable.\n",
      "target: sofa\n",
      "LM output: sofa\n",
      "\n",
      "input_text: In the sentence below, does the _ stand for brake or wheel?\n",
      "When the car was brought up into the shop, the brake was replaced when the wheel wasn't, since the _ was fixed.\n",
      "target: wheel\n",
      "LM output: brake\n",
      "\n",
      "input_text: In the sentence below, does the _ stand for Tanya or Sarah?\n",
      "Tanya was the most powerful person in the world, and Sarah admired them, because _ was something to aspire to.\n",
      "target: tanya\n",
      "LM output: sarah\n",
      "\n",
      "input_text: In the sentence below, does the _ stand for Sarah or Kayla?\n",
      "Sarah was quickly promoted to manager over Kayla, because _ excelled at angering other people.\n",
      "target: kayla\n",
      "LM output: kayla\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, t in zip(input_text, target_text):\n",
    "    print('input_text:', i)\n",
    "    print('target:', t)\n",
    "    print('LM output:', t2t(i)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
