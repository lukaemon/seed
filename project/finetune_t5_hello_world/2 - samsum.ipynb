{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.philschmid.de/fine-tune-flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 10:35:08.138368: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-10 10:35:08.663469: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/torch/lib:/usr/local/lib/python3.8/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-10 10:35:08.663524: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/torch/lib:/usr/local/lib/python3.8/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-10 10:35:08.663529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google/flan-t5-base\"\n",
    "dataset_name = \"samsum\"\n",
    "\n",
    "ft_output_dir = os.getenv(\"HF_FINETUNE_OUTPUT_DIR\")\n",
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "hub_model_id = f\"{model_name}-{dataset_name}\"\n",
    "model_output_dir = os.path.join(ft_output_dir, hub_model_id)\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = hub_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a341a851d4842488f87b373679a9daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(dataset_name)\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model.parallelize()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13818513',\n",
       " 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
       " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ds[\"train\"][0]\n",
    "example\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max_length analysis\n",
    "Given last miserable OOM experience from `xsum`, investigate [truncation and padding](https://huggingface.co/docs/transformers/main/en/pad_truncation#padding-and-truncation) this time.  \n",
    "Get statistics on dialogue and summary token length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dialogue  summary\n",
      "count   14732.0  14732.0\n",
      "mean      149.0     28.9\n",
      "std       110.7     15.1\n",
      "min         1.0      2.0\n",
      "25%        66.0     17.0\n",
      "50%       120.0     26.0\n",
      "75%       202.0     37.0\n",
      "max      1153.0     94.0\n"
     ]
    }
   ],
   "source": [
    "tk_dialogue = tokenizer(ds[\"train\"][\"dialogue\"])[\"input_ids\"]\n",
    "tk_summary = tokenizer(ds[\"train\"][\"summary\"])[\"input_ids\"]\n",
    "pd.set_option('display.float_format', lambda x: '%.1f' % x)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\"dialogue\": [len(d) for d in tk_dialogue], \"summary\": [len(s) for s in tk_summary]}\n",
    ")\n",
    "print(df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My hunch is I shouldn't truncate the input. Just need to pad to the longest of the batch. \n",
    "The setting would be `tokenizer(batch_sentences, padding=True)`.  \n",
    "\n",
    "However, it seems that [truncation is inevitable in production](https://twitter.com/RamaswmySridhar/status/1621870502766858241). How to truncate wisely?\n",
    "\n",
    "### Padding experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dialogue  summary\n",
      "count   14732.0  14732.0\n",
      "mean     1153.0     94.0\n",
      "std         0.0      0.0\n",
      "min      1153.0     94.0\n",
      "25%      1153.0     94.0\n",
      "50%      1153.0     94.0\n",
      "75%      1153.0     94.0\n",
      "max      1153.0     94.0\n"
     ]
    }
   ],
   "source": [
    "tk_dialogue = tokenizer(ds[\"train\"][\"dialogue\"], padding=True)[\"input_ids\"]\n",
    "tk_summary = tokenizer(ds[\"train\"][\"summary\"], padding=True)[\"input_ids\"]\n",
    "pd.set_option('display.float_format', lambda x: '%.1f' % x)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\"dialogue\": [len(d) for d in tk_dialogue], \"summary\": [len(s) for s in tk_summary]}\n",
    ")\n",
    "print(df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected, since this is full batch, all sequences are pad to the max length of the whole corpus.  \n",
    "Let's try this idea with `batch_size = 8`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842 1842\n",
      "482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1153, 389.02904564315355, 92)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, padding=True)\n",
    "dl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'])), batch_size=8, collate_fn=collator)\n",
    "\n",
    "\n",
    "tk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n",
    "\n",
    "print(len(tk_batched), len(dl))\n",
    "print(len(np.unique(tk_batched)))\n",
    "\n",
    "np.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()\n",
    "\n",
    "# 1842 batches, with 482 unique lengths, would be brutal for jax jit LoL\n",
    "# try pad_to_multiple_of=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842 1842\n",
      "91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1160, 485.27472527472526, 96)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator = DataCollatorForSeq2Seq(tokenizer, padding=True, pad_to_multiple_of=8)\n",
    "dl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'])), batch_size=8, collate_fn=collator)\n",
    "\n",
    "\n",
    "tk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n",
    "\n",
    "print(len(tk_batched), len(dl))\n",
    "print(len(np.unique(tk_batched)))\n",
    "\n",
    "np.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()\n",
    "\n",
    "# 1842 batches with 91 unique lengths, much better. \n",
    "# does truncation=True change anything here?\n",
    "# according to doc: tokenizer(batch_sentences, padding=True, truncation=True)\n",
    "# has the same effect as tokenizer(batch_sentences, padding=True)\n",
    "# both padding to max sequence in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842 1842\n",
      "51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(512, 311.52941176470586, 96)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator = DataCollatorForSeq2Seq(tokenizer, padding=True, pad_to_multiple_of=8)\n",
    "dl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'], truncation=True)), batch_size=8, collate_fn=collator)\n",
    "\n",
    "\n",
    "tk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n",
    "\n",
    "print(len(tk_batched), len(dl))\n",
    "print(len(np.unique(tk_batched)))\n",
    "\n",
    "np.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `truncation=True` truncates the dialogue to 512 tokens, which is the max length of the T5. \n",
    "- but as discussed in README, by default T5 should not have a set maximum length.\n",
    "- this is imposed, artificial limitation by transformers library. \n",
    "- input loss here. use with caution.\n",
    "- in xsum ipynb, I did `truncation=true` in tokenizer, which cut the input to 512 if not other `max_length` is set. That's why it solved OOM problem for me, at the cost of losing info\n",
    "- should experiment with truncation settings to observer cuda memory vs performance.\n",
    "\n",
    "### Compare to source ipynb\n",
    "In [source ipynb](https://www.philschmid.de/fine-tune-flan-t5): \n",
    "```python\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "\n",
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "    pass\n",
    "```\n",
    "1. It pads every input to absolute corpus max length. Would waste tons of memory and computation.  \n",
    "Would definite experiment with `pad_with_multiple_of` on JAX jit to find a better balance. \n",
    "2. I use `flan-t5` which is the heir of LM adopted T5, which means prepend `summarize:` to the input in not useful, and not necessary. \n",
    "\n",
    "## Move on to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no truncation, since the max_length in the training set is only 1153. Should be fine.\n",
    "def preprocess(examples):\n",
    "    output = tokenizer(examples[\"dialogue\"])\n",
    "    output[\"labels\"] = tokenizer(examples[\"summary\"])[\"input_ids\"]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_ds = ds.map(preprocess, batched=True).remove_columns(ds['train'].column_names)\n",
    "tk_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15bfc6418474459ba92605ff970fc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(tokenizer, padding=True, pad_to_multiple_of=8)\n",
    "\n",
    "# truncation could only be done in tokenizer, the padding settings are back to collator. \n",
    "# make sense since collator is where batching happened. \n",
    "# in xsum I tried to pushed all these settings to tokenizer, this is better balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    predict_with_generate=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    hub_model_id=hub_model_id,\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tk_ds[\"train\"],\n",
    "    eval_dataset=tk_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14732\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 460\n",
      "  Number of trainable parameters = 247577856\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/seed/project/finetune_t5_hello_world/wandb/run-20230210_103551-yvd6b2f3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lukaemon/flan-t5-base-samsum/runs/yvd6b2f3' target=\"_blank\">grateful-terrain-11</a></strong> to <a href='https://wandb.ai/lukaemon/flan-t5-base-samsum' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lukaemon/flan-t5-base-samsum' target=\"_blank\">https://wandb.ai/lukaemon/flan-t5-base-samsum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lukaemon/flan-t5-base-samsum/runs/yvd6b2f3' target=\"_blank\">https://wandb.ai/lukaemon/flan-t5-base-samsum/runs/yvd6b2f3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rouge1</td><td>▁</td></tr><tr><td>eval/rouge2</td><td>▁</td></tr><tr><td>eval/rougeL</td><td>▁</td></tr><tr><td>eval/rougeLsum</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.39589</td></tr><tr><td>eval/rouge1</td><td>0.47813</td></tr><tr><td>eval/rouge2</td><td>0.24603</td></tr><tr><td>eval/rougeL</td><td>0.40496</td></tr><tr><td>eval/rougeLsum</td><td>0.44394</td></tr><tr><td>eval/runtime</td><td>30.6889</td></tr><tr><td>eval/samples_per_second</td><td>26.655</td></tr><tr><td>eval/steps_per_second</td><td>3.356</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>6601082276265984.0</td></tr><tr><td>train/train_loss</td><td>1.46147</td></tr><tr><td>train/train_runtime</td><td>323.8242</td></tr><tr><td>train/train_samples_per_second</td><td>45.494</td></tr><tr><td>train/train_steps_per_second</td><td>1.421</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pleasant-glade-8</strong> at: <a href='https://wandb.ai/lukaemon/flan-t5-base-samsum/runs/xfcey9dg' target=\"_blank\">https://wandb.ai/lukaemon/flan-t5-base-samsum/runs/xfcey9dg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230210_032308-xfcey9dg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU utilization: 20.38 TFLOPS\n"
     ]
    }
   ],
   "source": [
    "total_flos = trainer.state.total_flos\n",
    "runtime = trainer.state.log_history[1]['train_runtime']\n",
    "\n",
    "print(f\"GPU utilization: {total_flos / 1e12 / runtime:.2f} TFLOPS\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "- `rouge-1` matches the source ipynb.\n",
    "- No truncation seems to work on this dataset.  \n",
    "- Maybe `xsum` has such outlier long sequences that make truncation is necessary. Otherwise, outlier batch would cause OOM, padding or not. \n",
    "\n",
    "### About TFLOPS\n",
    "- `m.parallelize()`\n",
    "  - `20.43` tflops.\n",
    "    - My current profile max on the workstation is ~35 tflops for each, ~70 tflops total.\n",
    "    - Achieved with `megatron` + `nvlink`. No `nvlink` number is around 62. \n",
    "  - GPU1: 16.6G, GPU2: 14.9G\n",
    "- No `m.parallelize()`\n",
    "  - `16.66` tflops.\n",
    "  - GPU1: 22.27, GPU2: 21.93G\n",
    "  - Why...?\n",
    "- `pad_to_multiple_of=64` -> `19.72` tflops\n",
    "  - I' not ready to innovate on [dark magic](https://twitter.com/karpathy/status/1621578354024677377) yet LoL. \n",
    "- No `pad_to_multiple_of=8` -> `20.38` tflops\n",
    "  - I don't need to do this religiously. Make no difference in this case.\n",
    "\n",
    "ps: previous GPU profile: \n",
    "![](asset/tflop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
